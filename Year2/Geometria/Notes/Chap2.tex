\chapter{Geometría afín y proyectiva lineal}
\section{Espacios proyectivos y afines}
\begin{definition}[Espacio afín]
	Sea $\displaystyle \K $ un cuerpo. Un $\displaystyle \K $\textbf{-espacio afín} de dimensión $\displaystyle n < \infty $ es una terna $\displaystyle \left(\mathbb{A}, \vec{\mathbb{A}}, \vec{ \cdot}\right) $ donde $\displaystyle \mathbb{A} $ es un conjunto no vacío, $\displaystyle \vec{\mathbb{A}} $ es un $\displaystyle \K $-espacio vectorial de dimensión $\displaystyle n $ y 
	\[
	\begin{split}
		\vec{ \cdot} : \mathbb{A} \times \mathbb{A} & \to \vec{\mathbb{A}} \\
		\left(A,B\right) & \to \overrightarrow{AB},
	\end{split}
	\]
	que cumple 
	\begin{enumerate}
	\item $\displaystyle \forall A \in \mathbb{A} $, $\displaystyle \forall v \in \vec{\mathbb{A}} $, $\displaystyle \exists ! B \in \mathbb{A} $ tal que $\displaystyle \overrightarrow{AB} = v $. 
	\item $\displaystyle \forall A,B,C \in \mathbb{A} $, $\displaystyle \overrightarrow{AB} + \overrightarrow{BC} = \overrightarrow{AC} $.
	\end{enumerate}
\end{definition}
\begin{eg}
Dado un espacio vectorial $\displaystyle \K^{n} $, siempre podemos dotar a $\displaystyle \K^{n} $ de una estructura afín. En efecto, tomamos $\displaystyle \mathbb{A} : = \K^{n} $, $\displaystyle \vec{\mathbb{A}} := \K^{n}	$ y 
\[ \vec{ \cdot } : \K^{n} \times \K^{n} \to \K^{n} : \left(A,B\right) \to B - A .\]
Si tenemos una base podemos expresar la aplicación anterior de la forma
\[\overrightarrow{\left(a_{1}, \ldots, a_{n}\right)\left(b_{1}, \ldots, b_{n}\right)} = \left(b_{1}-a_{1}, \ldots, b_{n}-a_{n}\right) .\]
\end{eg}
\begin{notation}
Si $\displaystyle \overrightarrow{AB} = v $ escribimos $\displaystyle A + v = B $.
\end{notation}
\begin{observation}
	\begin{itemize}
	\item $\displaystyle \forall A \in \mathbb{A} $ la función $\displaystyle \overrightarrow{ \cdot A} : \mathbb{A} \to \vec{\mathbb{A}} : B \to \overrightarrow{AB} $ es una biyección. Esto se deduce directamente de \textbf{(1)}. De forma similar, si $\displaystyle v \in \vec{\mathbb{A}} $, la aplicación $\displaystyle + v : \mathbb{A} \to \mathbb{A}: A \to A + v $ también es biyectiva.
	\item $\displaystyle \overrightarrow{AB} = 0 \iff A = B $. En efecto, por \textbf{(2)} se tiene que 
		\[\overrightarrow{A A} + \overrightarrow{ A A} = \overrightarrow{A A} \iff \overrightarrow{A A } = 0 .\]
		Como la aplicación $\displaystyle \overrightarrow{ \cdot A} $ es biyectiva, si $\displaystyle \overrightarrow{AB} = 0 $ debe ser que $\displaystyle A = B $. 
	\item $\displaystyle \overrightarrow{AB} = - \overrightarrow{BA} $. En efecto, tenemos que
		\[0 = \overrightarrow{A A} = \overrightarrow{AB} + \overrightarrow{BA} \iff \overrightarrow{AB} = - \overrightarrow{BA} .\]
	\item Se cumple la \textbf{ley del paralelogramo}. Es decir, tenemos que $\displaystyle \overrightarrow{AB} = \overrightarrow{CD} \Rightarrow \overrightarrow{AC} = \overrightarrow{BD} $. En efecto,
		\[\overrightarrow{AC} = \overrightarrow{AB} + \overrightarrow{BD} + \overrightarrow{DC} = \overrightarrow{AB} + \overrightarrow{BC}-\overrightarrow{CD} = \overrightarrow{AB} + \overrightarrow{BD} - \overrightarrow{AB} = \overrightarrow{BD} .\]
	\end{itemize} 
\end{observation}
\begin{definition}[Proyectivizado de un espacio vectorial]
Sea $\displaystyle V $ un $\displaystyle \K $-espacio vectoria de $\displaystyle \dim _{\K}V = n $. El \textbf{proyectivizado} de $\displaystyle V $, denotado $\displaystyle \mathbb{P}\left(V\right) $, es el conjunto de los subespacios vectoriales de $\displaystyle V $ de dimensión 1. La dimensión de $\displaystyle \mathbb{P}\left(V\right) $, denotada $\displaystyle \dim \mathbb{P}\left(V\right) $, es igual a $\displaystyle \dim _{\K}\left(V\right) -1 $.
\end{definition}
\begin{observation}
	$\displaystyle \mathbb{P}\left(V\right) = \left(V- \left\{ 0\right\} \right)/_{\sim} $, donde $\displaystyle \sim $ denota la relación
	\[u \sim v \iff \exists \lambda \in \K ^{*}, \; u = \lambda v .\]
	Si $\displaystyle v = \left(a_{1}, \ldots, a_{n}\right) \in \K^{n} $, usamos $\displaystyle [v] $, $\displaystyle [v]_{n} $ o $\displaystyle [a_{1} : a_{2} : \cdots : a_{n}] $ para denotar al punto $\displaystyle L\left(v\right) $ de $\displaystyle \mathbb{P}\left(V\right) $.
\end{observation}
\begin{eg}
\begin{enumerate}
	\item Sea $\displaystyle V = \left\{ 0\right\}  $ el espacio vectorial trivial. Tenemos que $\displaystyle \mathbb{P}\left(V\right) = \emptyset $. Así, tenemos que el conjunto vacío es un espacio proyectivo con $\displaystyle \dim\mathbb{P}\left(V\right) = -1 $.
	\item Si $\displaystyle V = \K $, tenemos que $\displaystyle \mathbb{P}\left(V\right) = \left\{ *\right\}  $ es un punto, por lo que $\displaystyle \dim\left(\mathbb{P}\left(\K\right)\right) = 0 $.
	\item Si $\displaystyle V = \R^{2} $, tenemos que $\displaystyle \dim\mathbb{P}\left(\R^{2}\right) = 1 $. Hay una biyección $\displaystyle [0,\pi) \to \mathbb{P}\left(\R^{2}\right) : \theta \to [\left(\cos\theta, \sin \theta\right)] $. Tenemos que $\displaystyle \mathbb{P}\left(\R^{2}\right) \cong \mathbb{S}^{1}$, que es una circunferencia.
\end{enumerate}
\end{eg}
\begin{prop}
	Sea $\displaystyle V $ un $\displaystyle \K $-espacio vectorial de $\displaystyle \dim _{\K}V\geq 1 $. Sea $\displaystyle f : V \to \K $ una aplicación lineal sobreyectiva. Tenemos que $\displaystyle \mathcal{U} = \Ker\left(f\right) \subset V $ es un subespacio vectorial de $\displaystyle V $. Entonces, $\displaystyle \left(\mathbb{P}\left(V\right)/\mathbb{P}\left(\mathcal{U}\right), \mathcal{U}, \vec{ \cdot }\right) $ es un espacio afín donde $\displaystyle \overrightarrow{[u][v]}  = \frac{v}{f\left(v\right)}-\frac{u}{f\left(u\right)}$.
\end{prop}
\begin{proof}
Primero comprobamos que la definición de $\displaystyle \vec{ \cdot} $ no depende de los representantes. Sea $\displaystyle u' = \lambda u $ y $\displaystyle v' = \mu v $ con $\displaystyle \lambda, \mu \neq 0 $. Tenemos que
\[\frac{v'}{f\left(v'\right)} - \frac{u'}{f\left(u'\right)} = \frac{\lambda v}{f\left(\lambda v\right)} - \frac{\mu u}{f\left(\mu u\right)} = \frac{\lambda v}{\lambda f\left(v\right)} - \frac{\mu u}{\mu f\left(u\right)} = \frac{v}{f\left(v\right)} - \frac{u}{f\left(u\right)} .\]
Comprobamos que $\displaystyle \forall [v_{1}], [v_{2}] \in \mathbb{P}\left(V\right)/\mathbb{P}\left(\mathcal{U}\right) $, $\displaystyle \overrightarrow{[v_{1}][v_{2}]} \in \mathcal{U} $. Tenemos que
\[\overrightarrow{[v_{1}][v_{2}]} = \frac{v_{2}}{f\left(v_{2}\right)} - \frac{v_{1}}{f\left(v_{1}\right)} \Rightarrow f\left(\frac{v_{2}}{f\left(v_{2}\right)} - \frac{v_{1}}{f\left(v_{1}\right)}\right) = \frac{f\left(v_{2}\right)}{f\left(v_{2}\right)} - \frac{f\left(v_{1}\right)}{f\left(v_{1}\right)} = 0.\]
Así, tenemos que $\displaystyle \overrightarrow{[v_{1}][v_{2}]} \in \mathcal{U} $. \\
Demostremos que cumple los axiomas. 
\begin{enumerate}
	\item Demostremos primero la existencia. Sea $\displaystyle A \in \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) $, por lo que $\displaystyle A = [w] $ con $\displaystyle f\left(w\right) \neq 0 $. Sea $\displaystyle v \in \mathcal{U} $. Tomamos $\displaystyle B = \left[\frac{w}{f\left(w\right)} + v\right] $.Comprobemos que $\displaystyle B \in \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) $:
	\[f\left(\frac{w}{f\left(w\right)} + v\right) = f\left(\frac{w}{f\left(w\right)}\right) + f\left(v\right) = \frac{f\left(w\right)}{f\left(w\right)} + f\left(v\right) = 1 \neq 0 \Rightarrow B \in \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) .\]
Así, tenemos que 
\[
\begin{split}
	\overrightarrow{AB} = \overrightarrow{[w]\left[\frac{w}{f\left(w\right)} + v\right] } = \frac{\frac{w}{f\left(w\right)} + v}{f\left(\frac{w}{f\left(w\right)}+v\right)}-\frac{w}{f\left(w\right)} = v .
\end{split}
\]
Demostramos ahora la unicidad. Sea $\displaystyle B' \in \mathbb{P}\left(V\right)/\mathbb{P}\left(\mathcal{U}\right) $ tal que $\displaystyle \overrightarrow{AB'} = v = \overrightarrow{AB} $. Tenemos que $\displaystyle B' = [z] $ con $\displaystyle f\left(z\right) \neq 0 $. Así, 
\[\overrightarrow{AB'} = \overrightarrow{[w][z]} = \frac{z}{f\left(z\right)} - \frac{w}{f\left(w\right)} = v \Rightarrow z = \left(v + \frac{w}{f\left(w\right)}\right)f\left(z\right) \Rightarrow z = \lambda\left(\frac{w}{f\left(w\right)}+v\right), \; \lambda \in \K^{*} .\]
Por tanto, tenemos que $\displaystyle [z] = \left[\frac{w}{f\left(w\right)}+v\right]  $, por lo que $\displaystyle B = B' $ y queda demostrada la unicidad.
\item Sean $\displaystyle A,B,C \in \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) $ tales que $\displaystyle A = [a] $, $\displaystyle B = [b] $ y $\displaystyle C = [c] $ con $\displaystyle f\left(a\right), f\left(b\right), f\left(c\right) \neq 0 $. Tenemos que
	\[\overrightarrow{AB}+\overrightarrow{BC} = \left(\frac{b}{f\left(b\right)}-\frac{a}{f\left(a\right)}\right) + \left(\frac{c}{f\left(c\right)} - \frac{b}{f\left(b\right)}\right) = \frac{c}{f\left(c\right)} - \frac{a}{f\left(a\right)} = \overrightarrow{AC} .\]
\end{enumerate}
\end{proof}
\begin{eg}
	Sean $\displaystyle V = \K^{3} $ y $\displaystyle f: \K^{3} \to \K : \left(x_{0}, x_{1}, x_{2}\right) \to x_{0} $. Entonces, $\displaystyle \mathcal{U} = \Ker\left(f\right) = \left\{ x_{0} = 0\right\}  $. Tenemos que
	\[\mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) = \left\{ [x_{0} : x_{1} : x_{2}] \in \mathbb{P}\left(V\right) \; : \; x_{0} \neq 0\right\} = \left\{ [1:x_{1}:x_{2}] \in \mathbb{P}\left(V\right)\right\}  .\]
	Tenemos que $\displaystyle \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) $ es un plano afín con espacio vectorial asociado $\displaystyle \mathcal{U} $ \footnote{Esto se parece mucho a nuestro intento de constuir un plano afín desde el espacio proyectivo $\displaystyle \K^{3} $.}. En este caso podemos observar que
\[
\begin{split}
	\overrightarrow{[x_{0}:x_{1}:x_{2}][y_{0}:y_{1}:y_{2}]}  = & \frac{\left(1,y_{1}, y_{2}\right)}{f\left(1,y_{1}, y_{2}\right)} - \frac{\left(1, x_{1}, x_{2}\right)}{f\left(1,x_{1}, x_{2}\right)} \\
	= &  \left(1, y_{1}, y_{2}\right) - \left(1, x_{1}, x_{2}\right) = \left(0, y_{1}-x_{1}, y_{2}-x_{2}\right) .
\end{split}
\]
Consideremos ahora $\displaystyle \mathbb{P}\left(\mathcal{U}\right) = \left\{ [0:x_{1}:x_{2}] \in \mathbb{P}\left(V\right)\right\}  $. Podemos considerar la aplicación $\displaystyle g : \mathcal{U} \to \K : \left(0, x_{1}, x_{2}\right) \to x_{1} $. Sea $\displaystyle W = \Ker\left(g\right) $, entonces $\displaystyle \mathbb{P}\left(\mathcal{U}\right) / \mathbb{P}\left(W\right) $ es un espacio afín asociado a $\displaystyle W $ con $\displaystyle \dim_{\K}\left(W\right) = 1 $. 
Así, tenemos que 
\[\mathbb{P}\left(\mathcal{U}\right) / \mathbb{P}\left(W\right) = \left\{ [0:x_{1}:x_{2}] \in \mathbb{P}\left(V\right) \; : \; x_{1} \neq 0\right\} = \left\{ [0:1:x_{2}] \in \mathbb{P}\left(V\right)\right\}  .\]
Si realizamos el cálculo anterior
\[
\begin{split}
	\overrightarrow{[0:1:x_{2}][0:1:y_{2}]} = & \frac{\left(0,1,y_{2}\right)}{g\left(0,1,y_{2}\right)} - \frac{\left(0,1,x_{2}\right)}{g\left(0,1,x_{2}\right)} \\
	= &  \left(0,1,y_{2}\right) - \left(0,1,x_{2}\right) = \left(0,0,y_{2}-x_{2}\right) .
\end{split}
\]
Tenemmos que $\displaystyle \mathbb{P}\left(W\right) = \left\{ [0:0:x_{2}] \in \mathbb{P}\left(V\right)\right\} = \left\{ [0:0:1]\right\}  $. Podríamos seguir hasta obtener el conjunto vacío. 
\end{eg}
\begin{observation}
Tenemos que
\[
\begin{split}
	\mathbb{P}\left(\K^{3}\right) = & \mathbb{P}\left(V\right) = \mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right) \sqcup \mathbb{P}\left(\mathcal{U}\right)\\
	= & \underbrace{\mathbb{P}\left(V\right) / \mathbb{P}\left(\mathcal{U}\right)}_{\text{plano afín}} \sqcup \underbrace{\mathbb{P}\left(\mathcal{U}\right)/\mathbb{P}\left(W\right)}_{\text{recta afín}} \sqcup \underbrace{\mathbb{P}\left(W\right)}_{\text{punto}}.
\end{split}
\]
\end{observation}
\subsection{Sistemas de referencia}
\begin{definition}[Referencia cartesiana]
Sea $\displaystyle \mathbb{A} $ un espacio afín. Una \textbf{referencia cartesiana} es un par $\displaystyle \mathcal{R}_{C}= \left(O,\mathcal{B}\right) $ donde $\displaystyle O \in \mathbb{A} $ y $\displaystyle \mathcal{B} $ es una base de $\displaystyle \vec{\mathbb{A}} $. Las coordenadas de $\displaystyle A \in \mathbb{A} $ en $\displaystyle \mathcal{R}_{C} $ son las coordenadas de $\displaystyle \overrightarrow{OA} $ en la base $\displaystyle \mathcal{B} $.
\end{definition}
\begin{eg}
Consideremos $\displaystyle \mathbb{A} = \R^{2} $ y la siguiente referencia cartesiana:
\[ \mathcal{R}_{C} = \left( O = \left(1,0\right), \mathcal{B} = \left\{ \left(1,1\right), \left(1,-1\right)\right\} \right) .\]
Consideremos $\displaystyle A = \left(3,2\right) \in \mathbb{A} $ y calculemos sus coordenadas en $\displaystyle \mathcal{R}_{C} $:
\[\overrightarrow{OA} = \left(3,2\right)-\left(1,0\right) = \left(2,2\right) = 2e_{1} .\]
Por tanto, $\displaystyle \overrightarrow{OA} = \left(2,0\right)\mathcal{B} $ y $\displaystyle A = \left(2,0\right)_{\mathcal{R}_{C}} $.
\end{eg}
A continuación introduciremos las coordenadas baricéntricas. Para ello, necesitamos primero:
\begin{prop}
Consideremos $\displaystyle P_{0}, \ldots, P_{n} \in \mathbb{A} $ y $\displaystyle \lambda_{0}, \ldots, \lambda_{n} \in \K $ tales que $\displaystyle \sum^{n}_{i = 0}\lambda_{i} = 1 $. Entonces, $\displaystyle \forall s,t = 0, \ldots, n $ se tiene que 
\[P_{s} + \sum^{n}_{i = 0, i \neq s}\lambda_{i}\overrightarrow{P_{s}P_{i}} = P_{t} + \sum^{n}_{i = 0, i \neq t}\lambda_{i}\overrightarrow{P_{t}P_{i}} .\]
\end{prop}
\begin{proof}
Está claro que
\[
\begin{split}
	P_{s} + \sum^{n}_{i = 0}\lambda_{i}\overrightarrow{P_{s}P_{i}} = & P_{t} + \overrightarrow{P_{t}P_{s}} + \sum^{n}_{i = 0}\lambda_{i}\left(\overrightarrow{ P_{s}P_{t}} + \overrightarrow{P_{t}P_{i}}\right) 
	=  P_{t} + \overrightarrow{P_{t}P_{s}} + \sum^{n}_{i = 0 }\lambda_{i}\overrightarrow{P_{s}P_{t}} + \sum^{n}_{i = 0}\overrightarrow{P_{t}P_{i}} \\
	= & P_{t} + \overrightarrow{P_{t}P_{s}} + \overrightarrow{P_{s}P_{t}} + \sum^{n}_{i = 0}\overrightarrow{P_{t}P_{i}} = P_{t} + \sum^{n}_{i = 0}\lambda_{i}\overrightarrow{P_{t}P_{i}} .
\end{split}
\]
\end{proof}
\begin{definition}[Combinación afín]
Una \textbf{combinación afín} de $\displaystyle P_{0}, \ldots, P_{n} \in \mathbb{A} $ es un punto de la forma $\displaystyle P_{0} + \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} $ con $\displaystyle \sum^{n}_{i = 0}\lambda_{i} = 1 $. Usamos $\displaystyle \sum^{n}_{i = 0}\lambda_{i}P_{i} $ para denotar a $\displaystyle P_{t}+\sum^{n}_{i = 0}\lambda_{i}\overrightarrow{P_{t}P_{i}} $ con $\displaystyle \sum^{n}_{i = 0}\lambda_{i} = 1 $.
\end{definition}
\begin{observation}
La proposición anterior nos permite ver que la notación que hemos empleado en la definición anterior tiene sentido. 
\end{observation}
\begin{definition}
	Una colección $\displaystyle \left\{ P_{0}, \ldots, P_{n}\right\} \subset \mathbb{A} $ es  
	\begin{itemize}
	\item \textbf{afinmente generadora} si $\displaystyle \forall P \in \mathbb{A} $ existen $\displaystyle \lambda_{0}, \ldots, \lambda_{n} \in \K $ tales que $\displaystyle \sum\lambda_{i} = 1 $ y $\displaystyle P = \sum\lambda_{i}P_{i} $ (todo punto es combinación afín de $\displaystyle P_{0}, \ldots, P_{n} $).
	\item  \textbf{afinmente dependiente} si existe $\displaystyle i \in \left\{ 0, \ldots, n\right\}  $ tal que $\displaystyle P_{i} $ es combinación afín de los demás. 
	\item  \textbf{afinmente independiente} si no es afinmente dependiente.
	\end{itemize}
\end{definition}
\begin{definition}[Referencia afín]
	Una \textbf{referencia afín} de $\displaystyle \mathbb{A} $ es una colección ordenada de puntos $\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, \ldots, P_{n}\right\}  $ que es afinmente generadora y afinmente independiente. Las \textbf{coordenadas baricéntricas} de $\displaystyle A \in \mathbb{A} $ son $\displaystyle \left(\lambda_{0}, \ldots, \lambda_{n}\right) $ si $\displaystyle \sum\lambda_{i} = 1 $ y $\displaystyle \sum\lambda_{i}P_{i} = A $.
\end{definition}
\begin{prop}
Las coordenadas baricéntricas de $\displaystyle A $ en $\displaystyle \mathcal{R}_{A} $ existen y son únicas. 
\end{prop}
\begin{proof}
Como $\displaystyle \mathcal{R}_{A} $ es afinmente generador, tenemos que existen $\displaystyle \lambda_{0}, \ldots, \lambda_{n} \in \K $ tales que $\displaystyle \sum\lambda_{i} = 1 $ y $\displaystyle A = \sum\lambda_{i}P_{i} $. Demostremos ahora la unicidad. Supongamos que 
\[A = \sum\lambda_{i}P_{i} = \sum\mu_{i}P_{i}, \; \sum\mu_{i} = 1 .\]
Tenemos que
\[
\begin{split}
& P_{0} + \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} = P_{0} + \sum^{n}_{i = 1}\mu_{i} \overrightarrow{P_{0}P_{i}} \\
	\Rightarrow & \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} = \sum^{n}_{i = 1}\mu_{i}\overrightarrow{P_{0}P_{i}} = \sum^{n}_{i = 1}\left(\lambda_{i}-\mu_{i}\right)\overrightarrow{P_{0}P_{i}} = 0.
\end{split}
\]
Hay dos posibles casos:
\begin{itemize}
\item Si $\displaystyle \lambda_{i}-\mu_{i} = 0 $, $\displaystyle \forall i = 1, \ldots, n $, tenemos que 
	\[\lambda_{0} = 1 - \sum^{n}_{i = 1}\lambda_{i} = 1 - \sum^{n}_{i = 1}\mu_{i} = \mu_{0} .\]
	Así, nos queda que $\displaystyle \lambda_{i} = \mu_{i} $ para $\displaystyle i = 0, \ldots, n $.
\item Supongamos que existe algún $\displaystyle i \in \left\{ 0, \ldots, n\right\}  $ tal que $\displaystyle \lambda_{i}-\mu_{i} \neq 0 $. Entonces, tendríamos que 
	\[\left(\lambda_{i}-\mu_{i}\right)\overrightarrow{P_{0}P_{i}} = \sum^{n}_{j = 0, j \neq i}-\left(\lambda_{j}-\mu_{j}\right)\overrightarrow{P_{0}P_{j}} \Rightarrow \overrightarrow{P_{0}P_{i}} = \sum^{n}_{j = 0, j \neq i}\alpha_{j}\overrightarrow{P_{0}P_{j}} ,\]
donde $\displaystyle \alpha_{j} = -\frac{\lambda_{j}-\mu_{j}}{\lambda_{i}-\mu_{i}} $. Así, nos queda que
\[P_{i} = P_{0} + \overrightarrow{P_{0}P_{i}} = P_{0}+\sum^{n}_{j = 0, j \neq i}\alpha_{j}\overrightarrow{P_{0}P_{j}} .\]
Por tanto, $\displaystyle P_{i} $ es una combinación afín de $\displaystyle P_{0}, \ldots, P_{i-1}, P_{i+1}, \ldots, P_{n} $ \footnote{Es fácil comprobar que $\displaystyle \sum^{n}_{j=0, j \neq i}\alpha_{j} = 1$.}  que contradice que $\displaystyle \mathcal{R}_{A} $ sea afinmente independiente. 
\end{itemize}
\end{proof}
\begin{lema}
	$\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, \ldots, P_{n}\right\}  $ es una referencia afín si y solo si $\displaystyle \mathcal{B} = \left\{ \overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{n}}\right\}  $ es una base de $\displaystyle \vec{\mathbb{A}} $. En particular, $\displaystyle \left|\mathcal{R}_{A}\right| = \dim\mathbb{A} + 1$. 
\end{lema}
\begin{proof}
\begin{description}
	\item[(i)] Vamos a ver que $\displaystyle \mathcal{B}= \left\{ \overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{n}}\right\}  $ genera $\displaystyle \vec{\mathbb{A}} $. Sea $\displaystyle v \in \vec{\mathbb{A}} $. Tenemos que $\displaystyle P_{0} + v \in \mathbb{A} $ y $\displaystyle P_{0}+v = \left(\lambda_{0}, \ldots, \lambda_{n}\right)_{\mathcal{R}_{A}} $. Así, tenemos que
		\[P_{0} + v = P_{0} + \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} .\]
		Por tanto, debe ser que $\displaystyle v = \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} $, por lo que $\displaystyle \mathcal{B} $ genera a $\displaystyle \vec{\mathbb{A}} $. Veamos que son linealmente independientes:
		\[ \alpha_{1}\overrightarrow{P_{0}P_{1}} + \cdots + \alpha_{n}\overrightarrow{P_{0}P_{n}} = 0 ,\]
con $\displaystyle \alpha_{0} = 1 - \alpha_{1} - \cdots -\alpha_{n} $. Así, nos queda que
\[\sum^{n}_{i = 0}\alpha_{i}P_{i} = P_{0} + \alpha_{1}\overrightarrow{P_{0}P_{1}} + \cdots + \alpha_{n}\overrightarrow{P_{0}P_{n}} = P_{0} + 0 = P_{0} .\]
Así, tenemos que $\displaystyle P_{0} = \left(1, 0, \ldots, 0\right)_{\mathcal{R}_{A}} $ y $\displaystyle P_{0} = \left(\alpha_{0}, \alpha_{2}, \ldots, \alpha_{n}\right)_{\mathcal{R}_{A}} $, por lo que $\displaystyle \alpha_{1} = \cdots= \alpha_{n} = 0 $. Así, hemos visto que $\displaystyle \mathcal{B} $ son linealmente independientes.
\item[(ii)] Supongamos que $\displaystyle \mathcal{B} $ es una base de $\displaystyle \vec{\mathbb{A}} $. Veamos que $\displaystyle \mathcal{R}_{A} $ es afinmente generadora. Sea $\displaystyle P \in \mathbb{A} $, está claro que $\displaystyle P = P_{0} + \overrightarrow{P_{0}P} $. Como $\displaystyle \overrightarrow{P_{0}P} \in \vec{\mathbb{A}}$, tenemos que existen $\displaystyle \lambda_{1}, \ldots, \lambda_{n} \in \K $ tales que
	\[\overrightarrow{P_{0}P} = \lambda_{1}\overrightarrow{P_{0}P_{1}} + \cdots + \lambda_{n}\overrightarrow{P_{0}P_{n}} .\]
	Si tomamos $\displaystyle \lambda_{0} = 1 - \sum^{n}_{i = 1}\lambda_{i} $, tenemos que $\displaystyle P = P_{0} + \sum^{n}_{i = 1}\lambda_{i}\overrightarrow{P_{0}P_{i}} $, por lo que $\displaystyle P $ es una combinación afín de $\displaystyle \mathcal{R}_{A} $ y $\displaystyle \mathcal{R}_{A} $ es afinmente generadora. Veamos que $\displaystyle \mathcal{R}_{A} $ es afimente independiente. 
Supongamos que $\displaystyle P_{i} = \sum_{j \neq i}\alpha_{j}P_{j} $ con $\displaystyle \sum_{j \neq i}\alpha_{j} = 1 $ para $\displaystyle i \neq 0 $ (si $\displaystyle i = 0 $ para lo que continua tomamos otro punto). Tenemos que $\displaystyle P_{i} = P_{0} + \overrightarrow{P_{0}P_{i}} $ y además
\[P_{i} = P_{0} + \sum^{n}_{j = 0, j \neq i} \alpha_{j}\overrightarrow{P_{0}P_{j}} \Rightarrow \overrightarrow{P_{0}P_{i}} = \sum^{n}_{j = 0, j\neq i}\alpha_{j}\overrightarrow{P_{0}P_{j}}.\]
Esto contradice que $\displaystyle \mathcal{B} $ sea linealmente independiente.
\end{description}
\end{proof}
\begin{eg}
	Consideremos $\displaystyle \mathbb{A} = \mathbb{P}\left(\R^{2}\right) / \left\{ x_{0}+2x_{1} = 0\right\} = \mathbb{P}\left(\R^{2}\right) / \mathbb{P}\left(U\right) $ donde $\displaystyle U = \Ker\left(f\right) $ y $\displaystyle f\left(x_{0},x_{1}\right) = x_{0}+2x_{1} $. 
	\begin{enumerate}
		\item Probemos que $\displaystyle P_{0} = [1:1] $ y $\displaystyle P_{1} = [1:0] $ forman una referencia afín de $\displaystyle \mathbb{A} $. Por lo visto anteriormente, $\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, P_{1}\right\}  $ es una referencia afín si y solo si $\displaystyle \mathcal{B} = \left\{ \overrightarrow{P_{0}P_{1}}\right\}  $ es una base de $\displaystyle \vec{\mathbb{A}} $. En este caso, tenemos que $\displaystyle \vec{A} = U $ y $\displaystyle \dim U = 1 $. Tenemos que 
			\[\overrightarrow{P_{0}P_{1}} = \frac{\left(1,0\right)}{f\left(1,0\right)}-\frac{\left(1,1\right)}{f\left(1,1\right)} = \left(1,0\right)-\frac{1}{3}\left(1,1\right) = \left(\frac{2}{3}, -\frac{1}{3}\right) .\]
		Como $\displaystyle \overrightarrow{P_{0}P_{1}} \neq 0 $, tenemos que $\displaystyle \mathcal{B} $ es una base de $\displaystyle \vec{\mathbb{A}} $. 	
		\item Calculemos las coordenadas baricéntricas de $\displaystyle [5:-2] $ en la referencia afín. Queremos que existan $\displaystyle \lambda_{0}, \lambda_{1} \in \R $ tales que $\displaystyle \lambda_{0}+\lambda_{1} = 1 $ y
			\[ [5:-2] = \left(\lambda_{0}, \lambda_{1}\right)_{\mathcal{R}_{A}} .\]
			Además,
			\[[5:-2] = [1:1] + \lambda_{1}\overrightarrow{P_{0}P_{1}} = [1:1] + \lambda_{1}\left(\frac{2}{3}, -\frac{1}{3}\right) \iff \overrightarrow{[1:1][5:-2]} = \lambda_{1}\left(\frac{2}{3}, -\frac{1}{3}\right) .\]
		Así, nos queda que 
		\[\left(\frac{14}{3},-\frac{7}{3}\right) = \lambda_{1}\left(\frac{2}{3}, -\frac{1}{3}\right) .\]
		Nos queda que $\displaystyle \lambda_{1} = 7 $ y $\displaystyle \lambda_{0}=-6 $. Así, las coordenadas baricéntricas de $\displaystyle [5:-2] $ son $\displaystyle \left(-6,7\right)_{\mathcal{R}_{A}} $.	
	\end{enumerate}
\end{eg}
Ahora vamos a intriducir referencias en el espacio proyectivo. 
\begin{definition}
	Una familia de puntos $\displaystyle [v_{0}], \ldots, [v_{n}] \in \mathbb{P}\left(V\right) $ es \textbf{independiente} si $\displaystyle v_{0}, \ldots, v_{n} $ es linealmente independiente.
\end{definition}
\begin{lema}
Ser independiente no depende de los representantes.
\end{lema}
\begin{proof}
	Sean $\displaystyle [v_{0}], \ldots, [v_{n}] \in \mathbb{P}\left(V\right)$ y supongamos que $\displaystyle v_{0}, \ldots, v_{n} $ son linealmente independientes. Sean $\displaystyle [v'_{0}] = [v_{0}], \ldots, [v'_{n}] = [v_{n}] $. Así, para $\displaystyle i = 1, \ldots, n $ existe $\displaystyle \lambda_{i} \in \K^{*} $ tal que $\displaystyle v_{i}' = \lambda_{i}v_{i} $. 
	Tenemos que demostrar que $\displaystyle v_{0}', \ldots, v_{n}' $ son linealmente independientes. Si $\displaystyle \mu_{0}, \ldots, \mu_{n} \in \K $,
	\[0 = \mu_{0}v_{0}' + \cdots + \mu_{n}v_{n}' = \mu_{0}\lambda_{0}v_{0} + \cdots + \mu_{n}\lambda_{n}v_{n} .\]
	Como $\displaystyle v_{0}, \ldots, v_{n} $ son linealmente independientes, debe ser que $\displaystyle \mu_{i}\lambda_{i} = 0 $, $\displaystyle \forall i = 0, \ldots, n $. Como $\displaystyle \lambda_{i} \neq 0 $ debe ser que $\displaystyle \mu_{i} = 0 $ y $\displaystyle v_{0}', \ldots, v_{n}' $ son linealmente independientes.
\end{proof}
\begin{observation}
Observamos que si $\displaystyle \dim\left(V\right) = n +1$, entonces toda familia independiente de $\displaystyle \mathbb{P}\left(V\right) $ tiene a lo sumo $\displaystyle n +1 $ elementos.
\end{observation}
\begin{definition}
	$\displaystyle P_{0}, \ldots, P_{r} \in \mathbb{P}\left(V\right) $ están en \textbf{posición general} si cualquier subconjunto de tamaño $\displaystyle \dim\left(V\right) $ contiene elementos independientes. \footnote{En el caso de que $\displaystyle r+1 < \dim\left(V\right) $ basta con que los elementos de $\displaystyle \left\{ P_{0}, \ldots, P_{r}\right\}  $ sean independientes.} 
\end{definition}
\begin{eg}
$\displaystyle P_{0}, \ldots, P_{n} \in \mathbb{P}\left(\R^{3}\right) $ están en posición general si ninguna terna está alineada. 
\end{eg}
\begin{definition}[Referencia proyectiva]
Sea $\displaystyle n = \dim \mathbb{P}\left(V\right) $ ($\displaystyle \dim_{\K}V = n +1 $). Una \textbf{referencia proyectiva} de $\displaystyle \mathbb{P}\left(V\right) $ es una colección ordenada de $\displaystyle n + 2 $ puntos en posición general
\[\mathcal{R} = \left\{ P_{0}, P_{1}, \ldots, P_{n}; P_{n+1}\right\}  .\]
A $\displaystyle P_{n+1} $ se le llama \textbf{punto de medida} o \textbf{punto de unidad}. Diremos que una base de $\displaystyle V $, $\displaystyle \mathcal{B}= \left\{ v_{0}, \ldots, v_{n}\right\}  $, es una \textbf{base asociada} a $\displaystyle \mathcal{R} $ si $\displaystyle P_{i} = [v_{i}] $ para $\displaystyle  i = 0, \ldots, n $ y $\displaystyle P_{n+1} = \left[v_{0}+v_{1} + \cdots + v_{n}\right]  $. Las \textbf{coordenadas homogéneas} de $\displaystyle P \in \mathbb{P}\left(V\right) $ son $\displaystyle \left[\lambda_{0} : \lambda_{1} : \cdots : \lambda_{n}\right]  $ si $\displaystyle P = \left[\lambda_{0}v_{0}+\cdots + \lambda_{n}v_{n}\right] = [\left(\lambda_{0}, \ldots, \lambda_{n}\right)_{\mathcal{B}}]  $. 
\end{definition}
\begin{lema}
Las bases asociadas a una referencia proyectiva son proporcionales entre ellas. En particular, las coordenadas homogéneas respecto a una referencia proyectiva son únicas, salvo proporcionalidad. 
\end{lema}
\begin{proof}
	Sea $\displaystyle \mathcal{R} = \left\{ P_{0}, \ldots, P_{n}; P_{n+1}\right\}  $ una referencia proyectiva de $\displaystyle \mathbb{P}\left(V\right) $ y sean $\displaystyle \mathcal{B} = \left\{ v_{0}, \ldots, v_{n}\right\}  $ y $\displaystyle \mathcal{B}' = \left\{ v_{0}', \ldots, v_{n}'\right\}  $ bases asociadas tales que $\displaystyle P_{i} = \left[v_{i}\right]  = \left[v_{i}\right] ' $ para $\displaystyle i = 0, \ldots, n $ y $\displaystyle P_{n+1} = \left[v_{0}+\cdots + v_{n}\right] = \left[v_{0}' + \cdots + v_{n}'\right]  $. 
	Así, existe $\displaystyle \lambda_{i} \in \K $ para $\displaystyle i = 0, \ldots, n $ tal que $\displaystyle v_{i}' = \lambda_{i}v_{i} $ con $\displaystyle \lambda_{i} \neq 0 $. Similarmente, existe $\displaystyle \lambda\neq 0 $ tal que 
	\[v_{0}' + \cdots + v_{n}' = \lambda\left(v_{0} + \cdots + v_{n}\right) .\]
Así, tenemos que
\[v_{0}' + \cdots + v'_{n} = \lambda_{0}v_{0} + \cdots + \lambda_{n}v_{n} = \lambda v_{0} + \cdots + \lambda v_{n}.\]
Por ser $\displaystyle \mathcal{B} $ base de $\displaystyle V $, tenemos que $\displaystyle \lambda_{i} = \lambda  $, $\displaystyle \forall i = 0, \ldots, n $ y nos queda que $\displaystyle \mathcal{B}' = \left\{ \lambda v_{0}, \ldots, \lambda v_{n}\right\}  $. Explicamos el 'en particular': si $\displaystyle P = [a_{0}v_{0} + \cdots + a_{n}v_{n}] $, entonces  
\[
\begin{split}
	P = [a_{0} : \cdots : a_{n}] = \left[\frac{a_{0}}{\lambda }\left(\lambda v_{0}\right) + \cdots + \left(\frac{a_{n}}{\lambda }\right)\left(\lambda v_{n}\right)\right] \Rightarrow P = \left[\frac{a_{0}}{\lambda } : \cdots : \frac{a_{n}}{\lambda }\right] .
\end{split}
\]
\end{proof}
\begin{eg}
	La referencia proyectiva estándar es  
	\[ \mathcal{R} = \left\{ \left[1:0:\cdots:0\right] , [0:1:\cdots:0], \cdots , [0:0:\cdots :1]; [1:1: \cdots :1]\right\}.\]
Así, tenemos que la base asociada es la base estándar
\[\mathcal{B} = \left\{ \left(1,0, \ldots, 0\right), \left(0,1, \ldots, 0\right), \ldots, \left(0,0, \ldots, 1\right)\right\}  .\]
\end{eg}
\begin{eg}
Consideremos los puntos 
\[P_{0} = [1:2:3], \quad P_{1} = [2:-3:4], \quad P_{2} = [4:5:-6], \quad P_{3} = [11:9:-5] .\]
Veamos que $\displaystyle \mathcal{R} = \left\{ P_{0}, P_{1}, P_{2} ; P_{3}\right\}  $ es una referencia proyectiva y buscamos la base asociada. \\
Consideremos 
\[v_{0} = \left(1,2,3\right), \quad v_{1} = \left(2, -3, 4\right), \quad v_{2} = \left(4,5-6\right), \quad v_{3} = \left(11,9,-5\right) .\]
Tenemos que $\displaystyle \mathcal{R} $ es una referencia proyectiva si y solo si están en posición general, es decir, si toda colección de tres vectores de $\displaystyle \left\{ v_{0}, v_{1}, v_{2}, v_{3}\right\}  $ son linealmente independientes. Esto es equivalente a que $\displaystyle \left\{ v_{0}, v_{1}, v_{2}\right\}  $ sean linealmente independientes y que $\displaystyle \alpha v_{0} + \beta v_{1} + \gamma v_{2} = v_{3} $ implica que $\displaystyle \alpha, \beta, \gamma \neq 0 $. Tenemos que
\[\begin{vmatrix} 1 & 2 & 3 \\ 2 & -3 & 4 \\ 4 & 5 & -6 \end{vmatrix}  = 120 \neq 0 .\]
Por tanto, $\displaystyle \left\{ v_{0}, v_{1}, v_{2}\right\}  $ son linealmente independientes. Veamos la segunda parte:
\[\alpha\left(1,2,3\right) + \beta\left(2,-3,4\right) + \gamma\left(4,5,-6\right) = \left(11,9,-5\right) .\]
Así, nos queda el sistema 
\[
\begin{cases}
\alpha + 2\beta + 4\gamma = 11 \\
2\alpha - 3\beta + 5\gamma = 9 \\
3\alpha + 4\beta -6\gamma = -5
\end{cases}
\Rightarrow \alpha = \beta = 1, \; \gamma = 2.\]
Por tanto, $\displaystyle P_{0}, P_{1}, P_{2}, P_{3} $ están en posición general y $\displaystyle \mathcal{R} $ es una referencia proyectiva. Busquemos la base asociada $\displaystyle \mathcal{B}= \left\{ u_{0}, u_{1}, u_{2}\right\}  $ tal que $\displaystyle P_{i} = [u_{i}] $ con $\displaystyle i = 0, 1, 2 $ y $\displaystyle P_{3} = [u_{0} + u_{1} + u_{2}] $. Tendremos que 
\[\mathcal{B} = \left\{ u_{0}, u_{1}, u_{2}\right\} = \left\{ \alpha v_{0}, \beta v_{1}, \gamma v_{2}\right\} = \left\{ v_{0}, v_{1}, 2v_{2}\right\}  .\]
\end{eg}
\subsection{Cambio de coordenadas cartesianas}
Estudiemos primero el caso de las coordenadas cartesianas en $\displaystyle \mathbb{A} $. \\ \\
Sean $\displaystyle \mathcal{R}_{C} = \left\{ O, \mathcal{B}\right\} $ y $\displaystyle \mathcal{R}_{C}' = \left\{ O', \mathcal{B}'\right\}  $ referencias cartesianas de $\displaystyle \mathbb{A} $. Si $\displaystyle A \in \mathbb{A} $ tenemos que 
\[A = O + \sum^{n}_{i = 1}x_{i}v_{i} = O'+\sum^{n}_{i = 1}y_{i}v_{i}' .\]
De aquí deducimos que 
\[\overrightarrow{O'A} = \sum^{n}_{i = 1}y_{i}v'_{i} = \overrightarrow{O'O} + \overrightarrow{OA} = \overrightarrow{O'O} + \sum^{n}_{ i = 1}x_{i}v_{i}  .\]
Tenemos que $\displaystyle \overrightarrow{O'O} = \left(a_{0}, \ldots, a_{n}\right)_{\mathcal{B}'} $ con $\displaystyle O = \left(a_{0}, \ldots, a_{n}\right)_{\mathcal{R}'_{C}} $. Así, nos queda que 
\[\sum^{n}_{i = 1}y_{i}v'_{i}= \sum^{n}_{i = 1}a_{i}v'_{i} = \sum^{n}_{i = 1}x_{i}v_{i} \Rightarrow \sum^{n}_{i = 1}\left(y_{i}-a_{i}\right)v_{i}' = \sum^{n}_{i = 1}x_{i}v_{i} .\]
Sea $\displaystyle C_{\mathcal{B}\mathcal{B}'} $ la matriz de cambio de base de la base $\displaystyle \mathcal{B} $ a la base $\displaystyle \mathcal{B}' $. 
Así, tenemos que
\[\begin{pmatrix} y_{1}-a_{1} \\ \vdots \\ y_{n}-a_{n} \end{pmatrix}= C_{\mathcal{B}\mathcal{B}'}\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} \Rightarrow \begin{pmatrix} y_{1} \\ \vdots \\ y_{n} \end{pmatrix} = \begin{pmatrix} a_{1} \\ \vdots \\ a_{n} \end{pmatrix} + C_{\mathcal{B}\mathcal{B}'}\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}.\]
Para que quede elegante ponemos 
\[\begin{pmatrix} 1 \\ y_{1} \\ \vdots \\ y_{n} \end{pmatrix} = \begin{pmatrix} 1 & 0 & \cdots & 0 \\
a_{1} & & & \\
\vdots & & C_{\mathcal{B}\mathcal{B}'} & \\
a_{n} & & & & \end{pmatrix}\begin{pmatrix} 1 \\ x_{1} \\ \vdots \\ x_{n} \end{pmatrix} .\]
La matriz de cambio de $\displaystyle \mathcal{R}_{C} $ a $\displaystyle \mathcal{R}'_{C} $ es 
\[C_{\mathcal{R}_{C}\mathcal{R}'_{C}} = \begin{pmatrix} 1 & 0 & \cdots & 0 \\
a_{1} & & & \\
\vdots & & C_{\mathcal{B}\mathcal{B}'} & \\
a_{n} & & & & \end{pmatrix} .\]
\subsection{Cambio de coordenadas baricéntricas}
Sean $\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, \ldots, P_{n}\right\}  $ y $\displaystyle \mathcal{R}_{A}' = \left\{ Q_{0}, \ldots, Q_{n}\right\}  $ referencias afines de $\displaystyle \mathbb{A} $. Sea $\displaystyle A \in \mathbb{A} $ con $\displaystyle A = \left(\lambda_{0}, \ldots, \lambda_{n}\right)_{\mathcal{R}_{A}} = \left(\mu_{0}, \ldots, \mu_{n}\right)_{\mathcal{R}_{A}'} $. Supongamos que $\displaystyle P_{j} = \left(a_{0j}, \ldots, a_{nj}\right)_{\mathcal{R}_{A}'} $.
Tenemos que
\[A = \sum^{n}_{j = 0}\lambda_{j}P_{j} = \sum^{n}_{j = 0}\lambda_{j}\sum^{n}_{i = 0}a_{ij}Q_{i} = \sum^{n}_{i = 0}\left(\sum^{n}_{j = 0}\lambda_{j}a_{ij}\right)Q_{i}= \sum^{n}_{j = 0}\mu_{j}Q_{j} .\]
Así, tenemos que $\displaystyle \mu_{i} = \sum^{n}_{j = 0}a_{ij}\lambda_{j} $.
Matricialmente obtenemos la expresión
\[\begin{pmatrix} \mu_{0} \\ \mu_{1} \\ \vdots \\ \mu_{n} \end{pmatrix} = \underbrace{\begin{pmatrix} a_{00} & a_{01} & \cdots & a_{0n} \\
a_{10} & a_{11} & \cdots & a_{1n} \\
\vdots & \vdots & \vdots & \vdots \\
a_{n0} & a_{n1} & \cdots & a_{nn}\end{pmatrix}}_{C_{\mathcal{R}_{A}\mathcal{R}_{A}'}}\begin{pmatrix} \lambda_{0} \\\lambda_{1} \\ \vdots \\ \lambda_{n} \end{pmatrix} .\]
Tenemos que $\displaystyle C_{\mathcal{R}_{A}\mathcal{R}_{A}'} $ es la matriz de cambio de $\displaystyle \mathcal{R}_{A} $ a $\displaystyle \mathcal{R}_{A}' $. Podemos observar que las columnas de $\displaystyle C_{\mathcal{R}_{A}\mathcal{R}_{A}'} $ son las coordenadas baricéntricas de $\displaystyle P_{i} $ en la referencia $\displaystyle \mathcal{R}_{A}' $.
\subsection{Cambios de coordenadas homogéneas en $\displaystyle \mathbb{P} $}
Sean $\displaystyle \mathcal{R} $ y $\displaystyle \mathcal{R}' $ referencias proyectivas y sean $\displaystyle \mathcal{B} = \left\{ v_{0}, \ldots, v_{n}\right\}  $ y $\displaystyle \mathcal{B}'= \left\{ v_{0}', \ldots, v_{n}'\right\}  $ sus bases asociadas, respectivamente. Sea $\displaystyle P \in \mathbb{P} $ con $\displaystyle P = [a_{0}: \cdots : a_{n}]_{\mathcal{R}}= [a_{0}v_{0}+ \cdots + a_{n}v_{n}] = [a'_{0}v_{0}' + \cdots + a'_{n}v_{n}'] $. 
Supongamos que $\displaystyle v_{i} = b_{i0}v_{0}' + \cdots + b_{in}v_{n}' $. Así, nos queda que
\[
\begin{split}
	\left(a_{0}, \ldots, a_{n}\right)_{\mathcal{B}} = & a_{0}\left(b_{00}v_{0}' + \cdots + b_{0n}v_{n}'\right) + \cdots + a_{n}\left(b_{n0}v_{0}' + \cdots + b_{nn}v'_{n}\right) \\
	= & \left(a_{0}b_{00} + \cdots + a_{n}b_{n0}\right)v_{0}' + \cdots + \left(a_{0}b_{0n} + \cdots + a_{n}b_{nn}\right)v'_{n} \\
	= & \left(a_{0}b_{00} + \cdots + a_{n}b_{n0}, \ldots, a_{0}b_{0n} + \cdots + a_{n}b_{nn}\right)_{\mathcal{B}'} .
\end{split}
\]
Matricialmente nos queda que
\[\begin{pmatrix} a_{0}' \\ a_{1}' \\ \vdots \\ a_{n}' \end{pmatrix} = \underbrace{\begin{pmatrix} b_{00} & b_{10} & \cdots & b_{n0} \\ 
b_{01} & b_{11} & \cdots & b_{n1} \\ 
\vdots & \vdots & \vdots & \vdots \\
b_{0n} & b_{1n} & \cdots & b_{nn}\end{pmatrix}}_{C_{\mathcal{B}\mathcal{B}'}}\begin{pmatrix} a_{0} \\ a_{1} \\ \vdots \\ a_{n} \end{pmatrix} .\]
Cada una de las columnas de $\displaystyle C_{\mathcal{B}\mathcal{B}'} $ es $\displaystyle v_{i} $ en la base $\displaystyle \mathcal{B}' $. Podemos observar que $\displaystyle P = [a_{0} : \cdots : a_{n}] = [\lambda a_{0}: \cdots : \lambda a_{n}]$ con $\displaystyle \lambda \in \K^{*} $. 
En particular
\[ \begin{pmatrix} a_{0}' \\ a_{1}' \\ \vdots \\ a_{n}' \end{pmatrix} =\lambda C_{\mathcal{B}\mathcal{B}'}\begin{pmatrix} a_{0} \\ a_{1} \\ \vdots \\ a_{n} \end{pmatrix}\]
también nos sirve para cambiar de coordenadas homogéneas de $\displaystyle \mathcal{R} $ en $\displaystyle \mathcal{R}' $. Por tanto, no buscamos una matriz para cambiar de una referencia a otra, sino una clase de equivalencia de matrices de cambio de referencias:
\[ [C_{\mathcal{R}\mathcal{R}'}] = \left\{ \lambda C_{\mathcal{B}\mathcal{B}'} \; : \; \lambda \in \K^{*}\right\}  .\]
\begin{eg}
Consideremos las refernecias 
\[\mathcal{R} = \left\{ [1:0:0], [0:1:0], [0:0:1]; [1:1:1]\right\}  .\]
\[\mathcal{R}' = \left\{[1:1:0], [-1:1:0], [1:0:1]; [1: -2:1] \right\}  .\]
Calculemos las posibles matrices de cambio de referencia. Tenemos que $\displaystyle \mathcal{B} = \left\{ \left(1,0,0\right), \left(0,1,0\right), \left(0,0,1\right)\right\}  $ es la base asociada a $\displaystyle \mathcal{R} $. Calculemos la base asociada a $\displaystyle \mathcal{R}' $. Cogemos $\displaystyle v_{0}= \left(1,1,0\right), v_{1} = \left(-1,1,0\right) $ y $\displaystyle v_{2} = \left(1,0,1\right) $. Tenemos que
\[ \begin{vmatrix}  1 & 1 & 0 \\ - 1 & 1 & 0 \\ 1 & 0 & 1\end{vmatrix}  = 2 .\]
Así, $\displaystyle \left\{ v_{0}, v_{1}, v_{2}\right\}  $ son linealmente independientes. Encontremos $\displaystyle \alpha, \beta, \gamma \in \K $ tales que 
\[\alpha v_{0} + \beta v_{1} + \gamma v_{2} = \left(1,-2,1\right) .\]
Nos queda el sistema 
\[
\begin{cases}
\alpha-\beta+\gamma = 1 \\
\alpha + \beta = 2 \\
\gamma = 1
\end{cases}
\Rightarrow \alpha = \beta = -1, \; \gamma = 1.\]
Así, tenemos que $\displaystyle \mathcal{B}' = \left\{ \left(-1,-1,0\right), \left(1, - 1, 0\right), \left(1, 0, 1\right)\right\}  $. Para encontrar $\displaystyle [C_{\mathcal{R}\mathcal{R}'}] $ buscamos  
\[ C_{\mathcal{B}\mathcal{B}'} = \left(C_{\mathcal{B}'\mathcal{B}}\right)^{-1} = \begin{pmatrix} -1 & 1 & 1 \\ - 1 & - 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}^{-1}= \begin{pmatrix} -\frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\
\frac{1}{2} & -\frac{1}{2} & -\frac{1}{2} \\
0 & 0 & 1\end{pmatrix}.\]
Así, nos queda que 
\[[C_{\mathcal{R}\mathcal{R}'}] = \left[\begin{pmatrix} -\lambda & - \lambda & \lambda \\ \lambda & - \lambda & - \lambda \\ 0 & 0 & 2\lambda  \end{pmatrix} \; : \; \lambda \in \K^{*}\right]  .\]
\end{eg}
\begin{observation}
En los tres tipos de referencia tenemos que $\displaystyle C^{-1}_{\mathcal{R}\mathcal{R}'} = C_{\mathcal{R}'\mathcal{R}} $. También es cierto que $\displaystyle C_{\mathcal{R}'\mathcal{R}''} C_{\mathcal{R}\mathcal{R}'} = C_{\mathcal{R}\mathcal{R}''} $. Esto último es útil porque, en general, es más sencillo calcular $\displaystyle C_{\mathcal{R}\mathcal{E}} $, donde $\displaystyle \mathcal{E} $ es la base canónica. Así, para cambiar de $\displaystyle \mathcal{R} $ a $\displaystyle \mathcal{R}' $ podemos hacer
\[C_{\mathcal{R}\mathcal{R}'} = C_{\mathcal{R}'\mathcal{E}}^{-1}C_{\mathcal{R}\mathcal{E}} .\]
\end{observation}
\section{Aplicaciones afines}
\begin{definition}[Aplicación afín]
Sean $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $ espacios afines. Una \textbf{aplicación afín} es una función $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ tal que $\displaystyle \forall O \in \mathbb{A} $,
\[\vec{f}_{O} : \vec{\mathbb{A}} \to \vec{\mathbb{A}'} : \overrightarrow{OA} \to \overrightarrow{f\left(O\right)f\left(A\right)} ,\]
es lineal. Si $\displaystyle f $ es biyectiva, diremos que $\displaystyle f $ es una \textbf{afinidad}.
\end{definition}
\begin{prop}
Sea $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ una función. Son equivalentes:
\begin{description}
\item[(i)] $\displaystyle \forall O \in \mathbb{A} $, $\displaystyle \vec{f}_{O} : \vec{\mathbb{A}} \to \vec{\mathbb{A}} : A \to \vec{f}_{O}\left(\overrightarrow{OA}\right) = \overrightarrow{f\left(O\right)f\left(A\right)} $ es lineal.
\item[(ii)] Si $\displaystyle \sum \lambda_{i} = 1 $, $\displaystyle f\left(\sum^{r}_{i= 0}\lambda_{i}P_{i}\right) = \sum^{r}_{i= 0}\lambda_{i}f\left(P_{i}\right) $.
\end{description}
\end{prop}
\begin{proof}
\begin{description}
\item[(i)] Sean $\displaystyle P_{0}, \ldots, P_{r} \in \mathbb{A} $ y $\displaystyle \lambda_{0}, \ldots, \lambda_{r} \in \K $ con $\displaystyle \sum \lambda_{i} = 1 $. Por definición de $\displaystyle \vec{f}_{O} $ tenemos que $\displaystyle \vec{f}_{O}\left(\overrightarrow{O\sum\lambda_{i}P_{i}}\right) = \overrightarrow{f\left(O\right)f\left(\sum\lambda_{i}P_{i}\right)} $. Es decir, 
	\[f\left(O\right) + \vec{f}_{O}\left(\overrightarrow{O\sum\lambda_{i}P_{i}}\right) = f\left(\sum\lambda_{i}P_{i}\right) .\]
Así, tenemos que
\[
\begin{split}
	f\left(\sum^{r}_{i = 0}\lambda_{i}P_{i}\right) = & f\left(O\right) + \vec{f}_{O}\left(\overrightarrow{O\sum^{r}_{i = 0}\lambda_{i}P_{i}}\right) = f\left(O\right) + \vec{f}_{O}\left(\sum^{r}_{i = 0}\lambda_{i}\overrightarrow{OP_{0}}\right)  .
\end{split}
\]
Esta última igualdad se debe a que
\[
\begin{split}
	\overrightarrow{O\sum^{r}_{i = 0}\lambda_{i}P_{i}} = & \overrightarrow{O\left(P_{0}+\sum^{r}_{i = 0}\lambda_{i}\overrightarrow{P_{0}P_{i}}\right)} = \overrightarrow{OP_{0}} + \sum^{r}_{ i= 0}\lambda_{i}\overrightarrow{P_{0}P_{i}} \\
	= & \sum^{r}_{i = 0}\overrightarrow{OP_{0}} + \sum^{r}_{i = 0}\lambda_{i}\overrightarrow{P_{0}P_{i}} = \sum^{r}_{i = 0}\lambda_{i}\left(\overrightarrow{OP_{0}}+\overrightarrow{P_{0}P_{i}}\right) .
\end{split}
\]
Aplicando que $\displaystyle \vec{f}_{O} $ es lineal, si volvemos a nuestro cálculo inicial tomando $\displaystyle \lambda_{-1} = 0 $ y $\displaystyle f\left(P_{-1}\right) = O $,
\[
\begin{split}
	=  f\left(O\right) + \sum^{r}_{i = 0}\lambda_{i}\vec{f}_{O}\left(\overrightarrow{OP_{i}}\right) = f\left(O\right) + \sum^{r}_{i = 0}\lambda_{i}\overrightarrow{f\left(O\right)f\left(P_{i}\right)} =\sum^{r}_{i = -1}\lambda_{i}f\left(P_{i}\right) 
=  \sum^{r}_{i = 0}\lambda_{i}f\left(P_{i}\right) .
\end{split}
\]
\item[(ii)] Sean $\displaystyle v_{1}, v_{2} \in \vec{\mathbb{A}} $ y $\displaystyle \lambda_{1},\lambda_{2} \in \K $. Supongamos que $\displaystyle v_{1} = \overrightarrow{OA} $ y $\displaystyle v_{2} = \overrightarrow{OB} $. Tenemos que
	\[
	\begin{split}
		\vec{f}_{O}\left(\lambda_{1}v_{1} +\lambda_{2}v_{2}\right) = & \vec{f}_{O}\left(\lambda_{1}\overrightarrow{OA} + \lambda_{2}\overrightarrow{OB}\right) =  \overrightarrow{f\left(O\right)f\left(O + \lambda_{1}\overrightarrow{OA}+\lambda_{2}\overrightarrow{OB}\right)} \\
		= &  \overrightarrow{f\left(O\right)f\left(\left(1-\lambda_{1}-\lambda_{2}\right)O + \lambda_{1}A + \lambda_{2}B\right)} \\
		= & \overrightarrow{f\left(O\right)\left(\left(1-\lambda_{1}-\lambda_{2}\right)f\left(O\right) + \lambda_{1}f\left(A\right) + \lambda_{2}f\left(B\right)\right)} \\
		= & \overrightarrow{f\left(O\right)\left(f\left(O\right) + \lambda_{1}\overrightarrow{f\left(O\right)f\left(A\right)}+\lambda_{2}\overrightarrow{f\left(O\right)f\left(A\right)}\right)} \\
		= & \lambda_{1}\overrightarrow{f\left(O\right)f\left(A\right)} + \lambda_{2}\overrightarrow{f\left(O\right)f\left(B\right)} = \lambda_{1}\vec{f}_{O}\left(\overrightarrow{OA}\right) + \lambda_{2}\vec{f}_{O}\left(\overrightarrow{OB}\right) \\
		= & \lambda_{1}\vec{f}_{O}\left(v_{1}\right) + \lambda_{2}\vec{f}_{O}\left(v_{2}\right).
	\end{split}
	\]
	Por tanto, $\displaystyle \vec{f}_{O} $ es lineal.
\end{description}
\end{proof}
\begin{prop}
	Sean $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $ dos espacios afines de dimensión $\displaystyle n $. Sean $\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, \ldots, P_{n}\right\}  $ y $\displaystyle \mathcal{R}_{A}' = \left\{ Q_{0}, \ldots, Q_{n}\right\}  $ referencias afines de $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $, respectivamente. Entonces existe una única afinidad
	\[\phi : \mathbb{A} \to \mathbb{A}', \; \phi\left(P_{i}\right) = Q_{i}, \; i = 0, \ldots, n .\]
\end{prop}
\begin{proof}
\begin{description}
\item[Existencia.] Definimos $\displaystyle \phi $ de las siguiente forma: 
	\[\phi\left(\sum^{n}_{i=0}\lambda_{i}P_{i}\right) = \sum^{n}_{i = 0}\lambda_{i}\phi\left(P_{i}\right) = \sum^{n}_{i = 0}\lambda_{i}Q_{i}, \; \sum \lambda_{i} = 1 .\]
	Vamos a ver que esto define una aplicación lineal $\displaystyle \vec{\phi}_{P_{0}} $. Por ser $\displaystyle \mathcal{R}_{A} $ y $\displaystyle \mathcal{R}_{A}' $ referencias afines, tenemos que $\displaystyle \left\{ \overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{n}}\right\}  $ y $\displaystyle \left\{ \overrightarrow{Q_{0}Q_{1}}, \ldots, \overrightarrow{Q_{0}Q_{n}}\right\}  $ son bases de $\displaystyle \vec{\mathbb{A}}$ y $\displaystyle \vec{\mathbb{A}}' $, respectivamente.
Sea $\displaystyle A = \left(a_{0}, \ldots, a_{n}\right)_{\mathcal{R}_{A}} \in \mathbb{A} $. Así, tenemos que
\[\overrightarrow{P_{0}A} = a_{1}\overrightarrow{P_{0}P_{1}} + \cdots + a_{n}\overrightarrow{P_{0}P_{n}} .\]
Ahora podemos calcular
\[\vec{\phi}_{P_{0}}\left(\overrightarrow{P_{0}A}\right) = \vec{\phi}_{P_{0}}\left(a_{1}\overrightarrow{P_{0}P_{1}} + \cdots + a_{n}\overrightarrow{P_{0}P_{n}}\right) .\]
Por otro lado tenemos que esta expresión es igual a
\[
\begin{split}
	\overrightarrow{\phi\left(P_{0}\right)\phi\left(A\right)} = & \overrightarrow{\phi\left(P_{0}\right)\phi\left(\sum a_{i}P_{i}\right)} = \overrightarrow{Q_{0}\sum a_{i}Q_{i}} = a_{1}\overrightarrow{Q_{0}Q_{1}} + \cdots + a_{n}\overrightarrow{Q_{0}Q_{n}} \\
	= & a_{1}\vec{\phi}_{P_{0}}\left(\overrightarrow{P_{0}P_{1}}\right) + \cdots + a_{n}\vec{\phi}_{P_{0}}\left(\overrightarrow{P_{0}P_{n}}\right).
\end{split}
\]
Así, tenemos que $\displaystyle \vec{\phi}_{P_{0}} $ es lineal y $\displaystyle \phi $ es una afinidad.
\item[Unicidad.] Sea $\displaystyle \psi : \mathbb{A} \to \mathbb{A}' $ una afinidad tal que $\displaystyle \psi\left(P_{i}\right) = Q_{i} $, $\displaystyle \forall i = 0, \ldots, n $. Entonces, tenemos que
	\[\psi\left(\sum^{n}_{i = 0}\lambda_{i}P_{i}\right) = \sum^{n}_{i = 0}\lambda_{i}\psi\left(P_{i}\right) = \sum^{n}_{i = 0}\lambda_{i}Q_{i} = \sum^{n}_{i = 0}\lambda_{i}\phi\left(P_{i}\right) = \phi\left(\sum^{n}_{i = 0}\lambda_{i}P_{i}\right) .\]
	Como $\displaystyle \phi $ y $\displaystyle \psi $ coinciden en todo punto tenemos que $\displaystyle \phi = \psi $. 
\end{description}
\end{proof}
\begin{notation}
Dados $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A} $ espacios afines con referencias afines $\displaystyle \mathcal{R}_{A} $ y $\displaystyle \mathcal{R}_{A}' $, respectivamente, y $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ afín, denotamos por $\displaystyle M_{\mathcal{R}_{A}\mathcal{R}_{A}'}\left(f\right) $ a la matriz de la función $\displaystyle f $ que cumple que
\[f \begin{pmatrix} a_{0} \\ \vdots \\ a_{n} \end{pmatrix}_{\mathcal{R}_{A}'} = M_{\mathcal{R}_{A}\mathcal{R}_{A}'}\left(f\right) \begin{pmatrix} a_{0} \\ \vdots \\ a_{n} \end{pmatrix}_{\mathcal{R}_{A}} .\]
\end{notation}
\begin{eg}
Sea $\displaystyle \phi : \mathbb{A} \to \mathbb{A} $ de dimensión 2 tal que 
\[\phi\left(1,1\right) = \left(2,2\right), \; \phi\left(0,1\right) = \left(-1,1\right), \; \phi\left(2,-1\right) = \left(0,1\right) .\]
Tenemos que $\displaystyle \mathcal{R}_{A} = \left\{ \left(1,1\right), \left(0,1\right), \left(2,-1\right)\right\}  $ y $\displaystyle \mathcal{R}_{A}' = \left\{ \left(2,2\right), \left(-1,1\right), \left(0,1\right)\right\} $ son referencias afines de $\displaystyle \mathbb{A} $. Tenemos que en estas referencias 
\[M_{\mathcal{R}_{A}\mathcal{R}_{A}'}\left(\phi\right) = I .\]
Calculemos la matriz de $\displaystyle \phi $ esta vez con la referencia $\displaystyle \mathcal{E} = \left\{ \left(0,0\right), \left(1,0\right), \left(0,1\right)\right\}  $, es decir, buscamos $\displaystyle M_{\mathcal{E}\mathcal{E}}\left(\phi\right) $. Veremos que
\[M_{\mathcal{E}\mathcal{E}}\left(\phi\right) = C_{\mathcal{R}_{A}'\mathcal{E}} M _{\mathcal{R}_{A}\mathcal{R}_{A}'}\left(\phi\right) C_{\mathcal{E}\mathcal{R}_{A}} .\]
Tenemos que 
\[C_{\mathcal{R}_{A}\mathcal{E}} = \begin{pmatrix} -1 & 0 & 0 \\ 1 & 0 & 2 \\ 1 & 1 & -1 \end{pmatrix}, \; C_{\mathcal{R}'_{A}\mathcal{E}} = \begin{pmatrix} -3 & 1 & 0 \\ 2 & - 1 & 0 \\ 2 & 1 & 1 \end{pmatrix} .\]
Así, nos queda que
\[M_{\mathcal{E}\mathcal{E}}\left(\phi\right) = \begin{pmatrix} \frac{9}{2} & \frac{1}{2} & 1 \\ -\frac{7}{2} & -\frac{1}{2} & -1 \\ 0 & 1 & 1\end{pmatrix} .\]
\end{eg}
\begin{prop}
	Sean $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $ espacios afines de dimensión $\displaystyle n $. Sea $\displaystyle \mathcal{R}_{C} = \left\{ O, \mathcal{B} = \left\{ v_{1}, \ldots, v_{n}\right\} \right\}  $ referencia de $\displaystyle \mathbb{A} $ y $\displaystyle \mathcal{R}_{C}' = \left\{ O', \mathcal{B}' = \left\{ v_{1}', \ldots, v_{n}'\right\} \right\}  $ referencia de $\displaystyle \mathbb{A}' $. Entonces existe una única afinidad $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ tal que $\displaystyle f\left(O\right) = O' $ y $\displaystyle \vec{f}\left(v_{i}\right) = v_{i}' $.
\end{prop}
\begin{proof}
	Sean $\displaystyle \mathcal{R}_{A} = \left\{ O, O + v_{1}, \ldots, O + v_{n}\right\}  $ y $\displaystyle \mathcal{R}_{A}' = \left\{ O', O' + v_{1}' , \cdots , O' + v'_{n}\right\}  $ referencias afines de $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $, respectivamente. Podemos poner $\displaystyle v_{0} = v'_{0} = 0 $. Por la proposición anterior, existe una única $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ afinidad tal que $\displaystyle f\left(O + v_{i}\right) = O' + v_{i}' $, $\displaystyle i = 0, \ldots, n $. 
	De esta forma, tenemos que 
	\[\vec{f}\left(v_{i}\right) = \vec{f}\left(\overrightarrow{O \left(O + v_{i}\right)}\right) = \overrightarrow{f\left(O\right)f\left(O + v_{i}\right)} = \overrightarrow{O'\left(O'+v_{i}'\right)} = v_{i}'.\]
\end{proof}
\begin{observation}
Sabemos que 
\[ f : \mathbb{A} \to \mathbb{A}' : O + \overrightarrow{OA} \to f\left(O\right) + \vec{f}\left(\overrightarrow{OA}\right) .\]
La matriz de $\displaystyle f $ en las referencias cartesianas $\displaystyle \mathcal{R}_{C} $ y $\displaystyle \mathcal{R}_{C}' $ cumple que
\[\begin{pmatrix} 1 \\ f\left(\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}\right)_{\mathcal{R}_{C}'} \end{pmatrix} = M_{\mathcal{R}_{C}\mathcal{R}_{C}'}\left(f\right)\begin{pmatrix} 1 \\ \begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} _{\mathcal{R}_{C}} \end{pmatrix}.\]
Nos queda que 
\[M_{\mathcal{R}_{C}\mathcal{R}_{C}'} = \begin{pmatrix} 1 & 0  \\ f\left(O\right)_{\mathcal{R}_{C}} & M_{\mathcal{B}\mathcal{B}'}\left(\vec{f}\right) \end{pmatrix} .\]
\end{observation}
\begin{eg}
Sea $\displaystyle f : \R^{2} \to \R^{2} $ tal que 
\[f\left(1,1\right) = \left(2,2\right), \quad f\left(0,1\right) = \left(-1,1\right), \quad f\left(2,-1\right) = \left(0,1\right) .\]
Habíamos calculado $\displaystyle M_{\mathcal{R}_{A}}\left(f\right) $ donde $\displaystyle \mathcal{R}_{A} = \left\{ \left(0,0\right), \left(1,0\right), \left(0,1\right)\right\}  $. Ahora, buscamos $\displaystyle M_{\mathcal{R}_{C}}\left(f\right) $ en la referencia $\displaystyle \mathcal{R}_{C} = \left\{ \left(0,0\right), \mathcal{B} = \left\{ \left(1,0\right), \left(0,1\right)\right\} \right\}  $. 
\begin{description}
	\item[Opción 1.] Sea 
		\[  \mathcal{R}_{1} = \left\{ \left(1,1\right), \mathcal{B}_{1} = \left\{ \overrightarrow{\left(1,1\right)\left(0,1\right)}, \overrightarrow{\left(1,1\right)\left(2,-1\right)}\right\} \right\} = \left\{ \left(1,1\right), \mathcal{B}_{1} = \left\{ \left(-1,0\right),\left(1,-2\right)\right\} \right\} .\]
	Podemos tomar otra referencia 
	\[\mathcal{R}_{2} = \left\{ \left(2,2\right), \mathcal{B}_{2} = \left\{ \overrightarrow{\left(2,2\right)\left(-1,1\right)},\overrightarrow{\left(2,2\right)\left(0,1\right)}\right\} \right\} = \left\{ \left(2,2\right), \mathcal{B}_{2} = \left\{ \left(-3,-1\right),\left(-2,-1\right)\right\} \right\}  .\]
	Así, nos queda que
	\[M_{\mathcal{R}_{1}\mathcal{R}_{2}}\left(f\right) = \begin{pmatrix} 1 & 0 & 0 \\
	0 & 1&0 \\
0 &0 &1 \end{pmatrix} .\]
Así, tenemos que 
\[
\begin{split}
	M_{\mathcal{R}_{C}}\left(f\right) = & C_{\mathcal{R}_{2}\mathcal{R}_{C}}M_{\mathcal{R}_{1}\mathcal{R}_{2}}C_{\mathcal{R}_{C}\mathcal{R}_{1}}\\
	= & \begin{pmatrix} 1 & 0 & 0 \\ 2 & -3 & - 2 \\ 2 & - 1 & - 1 \end{pmatrix} I \begin{pmatrix} 1 &  0 & 0 \\ 1 & - 1 & 1 \\ 1 & 0 & -2 \end{pmatrix}^{-1} = \begin{pmatrix} 1 & 0 & 0 \\ -\frac{7}{2} & 3 & \frac{5}{2} \\ 0 & 1 & 1 \end{pmatrix}.
\end{split}
\]
\item[Opción 2.] Buscamos $\displaystyle \vec{f}\left(1,0\right) $ y $\displaystyle \vec{f}\left(0,1\right) $. Tenemos que
	\[
	\begin{split}
		\vec{f}\left(1,0\right) = & \vec{f}\left(-\left(-1,0\right)\right) = -\vec{f}\left(\overrightarrow{\left(1,1\right)\left(0,1\right)}\right) = - \overrightarrow{f\left(1,1\right)f\left(0,1\right)} \\
		= &  -\overrightarrow{\left(2,2\right)\left(-1,1\right)} = \left(3,1\right) .
	\end{split}
	\]
	De forma análoga, tenemos que 
	\[
	\begin{split}
		\vec{f}\left(0,1\right) = & \vec{f}\left(-\frac{1}{2}\left(-1,0\right) -\frac{1}{2}\left(1,-2\right)\right) = \vec{f}\left(-\frac{1}{2}\overrightarrow{\left(1,1\right)\left(0,1\right)} - \frac{1}{2}\overrightarrow{\left(1,1\right)\left(2,-1\right)}\right) \\
		= & \frac{1}{2}\overrightarrow{f\left(1,1\right)f\left(0,1\right)} - \frac{1}{2}\overrightarrow{f\left(1,1\right)f\left(2,-1\right)} = -\frac{1}{2}\left(-3,-1\right)-\frac{1}{2}\left(-2,-1\right) \\
		= & \left(\frac{5}{2},1\right) .
	\end{split}
	\]
\end{description}
Finalmente, calculamos $\displaystyle f\left(0,0\right) $:
\[
\begin{split}
	f\left(0,0\right) = & f\left(\left(1,1\right)-\left(1,1\right)\right) = f\left(1,1\right) - \vec{f}\left(1,1\right) \\
	= & \left(2,2\right)-\left(3,1\right)-\left(\frac{5}{2}, 1\right) = \left(-\frac{7}{2}, 0\right) .
\end{split}
\]
Así, hemos obtenido la misma matriz de dos formas distintas. 
\end{eg}
\begin{prop}
Sean $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ y $\displaystyle g : \mathbb{A}' \to \mathbb{A}'' $ aplicaciones afines. Entonces
\begin{enumerate}
\item $\displaystyle g\circ f : \mathbb{A} \to \mathbb{A}'' $ es afín y $\displaystyle \overrightarrow{g\circ f} = \vec{g}\circ \vec{f} $.
\item $\displaystyle f $ inyectiva $\displaystyle \iff  $ $\displaystyle \vec{f} $ inyectiva.
\item $\displaystyle f $ sobreyectiva $\displaystyle \iff  $ $\displaystyle \vec{f} $ sobreyectiva.
\item Si $\displaystyle f $ es biyectiva, entonces $\displaystyle f^{-1} $ es afín y $\displaystyle \overrightarrow{f^{-1}} = \vec{f}^{-1} $.
\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}
	\item Por ser $\displaystyle g $ y $\displaystyle f $ aplicaciones afines, sabemos que conserban las combinaciones afines, por lo que $\displaystyle g \circ f $ también conservará las combinaciones afines y será una aplicación afín. Ahora, sean $\displaystyle P,Q \in \mathbb{A} $ y $\displaystyle A = f\left(P\right), B = f\left(Q\right) \in \mathbb{A}' $. 
		Tenemos que
		\[
		\begin{split}
			\overrightarrow{g\circ f}\left(\overrightarrow{PQ}\right) = & \overrightarrow{\left(g\circ f\right)\left(P\right)\left(g\circ f\right)\left(Q\right)} = \overrightarrow{g\left(A\right)g\left(B\right)} = \vec{g}\left(\overrightarrow{AB}\right) = \vec{g} \left(\overrightarrow{f\left(P\right)f\left(Q\right)}\right) \\
			= & \vec{g}\left(\vec{f}\left(\overrightarrow{PQ}\right)\right) = \vec{g}\circ \vec{f}\left(\overrightarrow{PQ}\right)  .
		\end{split}
		\]
	\item Tenemos que $\displaystyle f $ es inyectiva si y solo si $\displaystyle f\left(P\right) \neq f\left(Q\right) $ cuando $\displaystyle P \neq Q $, es decir, si $\displaystyle \vec{f}\left(\overrightarrow{PQ}\right)=\overrightarrow{f\left(P\right)f\left(Q\right)} \neq 0 $ cuando $\displaystyle \overrightarrow{PQ} \neq 0 $, es decir, cuando $\displaystyle \vec{f} $ sea inyectiva. 
	\item Supongamos que $\displaystyle f $ es sobreyectiva. Dado $\displaystyle v \in \vec{\mathbb{A}'} $, sean $\displaystyle P,Q \in \mathbb{A}' $ tales que $\displaystyle v = \overrightarrow{PQ} $. Sean $\displaystyle A,B \in \mathbb{A} $ tales que $\displaystyle f\left(A\right) = P $ y $\displaystyle f\left(B\right) = Q $. Así, tenemos que 
		\[\vec{f}\left(\overrightarrow{AB}\right) = \overrightarrow{f\left(A\right)f\left(B\right)} = \overrightarrow{PQ} = v .\]
		Por tanto, $\displaystyle \vec{f} $ es sobreyectiva. Ahora, supongamos que $\displaystyle \vec{f} $ es sobreyectiva y sea $\displaystyle P \in \mathbb{A}' $. Fijamos $\displaystyle O \in \mathbb{A} $ y como $\displaystyle \overrightarrow{f\left(O\right)P} \in \vec{\mathbb{A}'} $, existe $\displaystyle u \in \vec{\mathbb{A}} $ tal que $\displaystyle \vec{f}\left(u\right) = \overrightarrow{f\left(O\right)P} $.
Entonces, tenemos que si cogemos $\displaystyle B = O + u $, entonces $\displaystyle f\left(B\right) = P $. En efecto,
\[f\left(B\right) = f\left(O + u\right) = f\left(O \right)+ \vec{f}\left(u\right) = f\left(O\right) + \overrightarrow{f\left(O\right)P} = P .\]
Así, hemos visto que $\displaystyle f $ es sobreyectiva. 
\item Veamos que $\displaystyle f^{-1} $ conserva las combinaciones afines. Dados $\displaystyle Q_{1}, \ldots, Q_{r} \in \mathbb{A}' $ y $\displaystyle \lambda_{0}, \ldots, \lambda_{r} \in \K $ con $\displaystyle \sum^{r}_{i = 0}\lambda_{i} = 1 $, sea $\displaystyle P_{i} = f^{-1}\left(Q_{i}\right) $. Como $\displaystyle f $ es afín tenemos que
	\[
	\begin{split}
		\sum^{r}_{i = 0}\lambda_{i}f^{-1}\left(Q_{i}\right) = & \sum^{r}_{i = 0}\lambda_{i}P_{i} = f^{-1}\left(f\left(\sum^{r}_{i = 0}\lambda_{i}P_{i}\right)\right) \\
		= & f^{-1}\left(\sum^{r}_{i = 0}\lambda_{i}f\left(P_{i}\right)\right) = f^{-1}\left(\sum^{r}_{i = 0}\lambda_{i}Q_{i}\right) .
	\end{split}
	\]
	Por tanto, tenemos que $\displaystyle f^{-1} $ también es afín. Usando \textbf{(1)} deducimos que
	\[id _{\vec{\mathbb{A}'}} = \overrightarrow{f \circ f^{-1}} = \vec{f}\circ \overrightarrow{f^{-1}} \quad \text{y}\quad id _{\vec{\mathbb{A}'}} = \overrightarrow{f^{-1}\circ f} = \overrightarrow{f^{-1}}\circ \vec{f} .\]
Por tanto, tenemos que $\displaystyle \overrightarrow{f^{-1}} = \vec{f}^{-1} $. 	
	\end{enumerate}
\end{proof}
\begin{prop}
	Sea $\displaystyle \mathcal{R} = \left\{ P_{0}, \ldots, P_{n}; P_{n+1}\right\}  $ una referencia proyectiva de $\displaystyle \mathbb{P}\left(V\right) $. Sea $\displaystyle \mathcal{B} = \left\{ v_{0}, \ldots, v_{n}\right\}  $ asociada y sea $\displaystyle f : V \to \K $ lineal tal que $\displaystyle f\left(v_{i}\right) \neq 0 $, $\displaystyle \forall i = 0, \ldots, n $. Entonces $\displaystyle \mathcal{R}_{A} = \left\{ P_{0}, \ldots, P_{n}\right\}  $ es una referencia afín de $\displaystyle \mathbb{P}\left(V\right) / \mathbb{P}\left(\Ker\left(f\right)\right) $. 
\end{prop}
\begin{proof}
	Como $\displaystyle P_{i} = [v_{i}] $ para $\displaystyle i = 0, \ldots,n $ y $\displaystyle f\left(v_{i}\right) \neq 0 $, tenemos que $\displaystyle P_{i} \in \mathbb{P}\left(V\right) / \mathbb{P}\left(\Ker\left(f\right)\right) $. Veamos que $\displaystyle \left\{ \overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0} P_{n}}\right\}  $ es una base de $\displaystyle \Ker\left(f\right) $. 
	Basta ver que son linealmente independientes puesto que $\displaystyle \dim \Ker\left(f\right) = n $. Recordamos que
	\[\overrightarrow{P_{0}P_{i}} = \frac{v_{i}}{f\left(v_{i}\right)} - \frac{v_{0}}{f\left(v_{0}\right)}, \; \forall i = 1, \ldots, n .\]
	Así, tenemos que 
	\[0 = \alpha_{1}\overrightarrow{P_{0}P_{1}} + \cdots + \alpha_{n}\overrightarrow{P_{0}P_{n}} = \alpha_{1}\left(\frac{v_{1}}{f\left(v_{1}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right) + \cdots + \alpha_{n}\left(\frac{v_{n}}{f\left(v_{n}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right) .\]
	De donde se deduce que
	\[\sum\alpha_{i}\frac{v_{0}}{f\left(v_{0}\right)} = \alpha_{1}\frac{v_{1}}{f\left(v_{1}\right)}+ \cdots + \alpha_{n}\frac{v_{n}}{f\left(v_{n}\right)} .\]
	Como $\displaystyle \left\{ v_{0}, \ldots, v_{n}\right\}  $ son linealmente independientes, tenemos que $\displaystyle \alpha_{i} = 0 $, $\displaystyle \forall i = 0, \ldots, n $. Por tanto, $\displaystyle \left\{ \overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{n}}\right\}  $ son linealmente independientes.
\end{proof}
\begin{prop}
Sea $\displaystyle \mathbb{A} $ un espacio afín sobre $\displaystyle \K $ de dimensión $\displaystyle n $. Sea $\displaystyle \mathcal{R}_{A} = \left\{ Q_{0}, \ldots, Q_{n}\right\}  $ una referencia afín y sea $\displaystyle \mathbb{P}\left(V\right) $ un espacio proyectivo de $\displaystyle \K $ de dimensión $\displaystyle n $ con $\displaystyle \mathcal{R} = \left\{ P_{0}, \ldots, P_{n}; P_{n+1}\right\}  $ una referencia proyectiva con base asociada $\displaystyle \mathcal{B} = \{v_{0}, \ldots, v_{n}\} $ . Si $\displaystyle f : V \to \K $ es lineal tal que $\displaystyle f\left(v_{i}\right) \neq 0 $, $\displaystyle \forall i = 0, \ldots, n $ tal que $\displaystyle P_{i} = [v_{i}] $, entonces 
	\[
	\begin{split}
		\phi : \mathbb{A} & \to \mathbb{P}\left(V\right) / \mathbb{P}\left(\Ker\left(f\right)\right) \\ 
		\left(a_{0}, \ldots, a_{n}\right)_{\mathcal{R}_{A}} & \to \left[\frac{a_{0}}{f\left(v_{0}\right)} : \cdots : \frac{a_{n}}{f\left(v_{n}\right)}\right] ,
	\end{split}
	\]
	es una afinidad que cumple $\displaystyle \phi\left(Q_{i}\right) = P_{i} $, $\displaystyle \forall i = 0, \ldots, n $.
\end{prop}
\begin{proof}
	En la situación de la proposición tenemos que $\displaystyle \mathcal{R}_{A}' = \left\{ P_{0}, \ldots, P_{n}\right\}  $ es referencia afín de $\displaystyle \mathbb{A}'=\mathbb{P}\left(V\right) / \mathbb{P}\left(\Ker\left(f\right)\right) $. Por tanto, existe una única afinidad $\displaystyle \phi : \mathbb{A} \to \mathbb{A}':Q_{i} \to P_{i} $.  
	Sea $\displaystyle P = \left(a_{0}, \ldots, a_{n}\right)_{\mathcal{R}_{A}'} $, entonces $\displaystyle P = P_{0} + \sum^{n}_{ i=1}a_{i}\overrightarrow{P_{0}P_{i}} $.
	 Así, tenemos que $\displaystyle P = [a_{0} : \cdots : a_{n}] $ si y solo si $\displaystyle P = [a_{0}v_{0}+\cdots + a_{n}v_{n}] $. Si $\displaystyle P \in \mathbb{P}\left(V\right)/\mathbb{P}\left(\Ker\left(f\right)\right) $, entonces $\displaystyle f\left(a_{0}v_{0} + \cdots + a_{n}v_{n}\right) \neq 0 $. Así, nos queda que
	\[
	\begin{split}
		P = & [v_{0}] + a_{1}\left(\frac{v_{1}}{f\left(v_{1}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right) + \cdots + a_{n}\left(\frac{v_{n}}{f\left(v_{n}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right) \\
		= & \left[\frac{v_{0}}{f\left(v_{0}\right)}+a_{1}\left(\frac{v_{1}}{f\left(v_{1}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right) + \cdots + a_{n}\left(\frac{v_{n}}{f\left(v_{n}\right)}-\frac{v_{0}}{f\left(v_{0}\right)}\right)\right] \\
		= & \left[\left(1-\sum^{n}_{i = 1}a_{i}\right)\frac{v_{0}}{f\left(v_{0}\right)} + a_{1}\frac{v_{1}}{f\left(v_{1}\right)} + \cdots +a_{n}\frac{v_{n}}{f\left(v_{n}\right)}\right] \\
		= & \left[a_{0}\frac{v_{0}}{f\left(v_{0}\right)} + \cdots + a_{n}\frac{v_{n}}{f\left(v_{n}\right)}\right] = \left[\frac{a_{0}}{f\left(v_{0}\right)}: \frac{a_{1}}{f\left(v_{1}\right)}: \cdots : \frac{a_{n}}{f\left(v_{n}\right)}\right] _{\mathcal{R}}  .
	\end{split}
	\]
\end{proof}
\section{Aplicaciones proyectivas}
Escribimos $\displaystyle f : A \to B $ para denotar a una función definida sobre un subconjunto de $\displaystyle A $. 
\begin{definition}[Aplicación proyectiva]
	Una \textbf{aplicación proyectiva} $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ es una función asociada a una función lineal $\displaystyle \hat{f}: V \to V $ de tal forma que $\displaystyle [\hat{f}(v)] = f\left([v]\right) $. La aplicación $\displaystyle f $ no está definida sobre $\displaystyle \mathbb{P}\left(\Ker\left(\hat{f}\right)\right) $. A este conjunto lo llamamos el \textbf{centro} de $\displaystyle f $ y lo denotamos $\displaystyle Z\left(f\right) $. 
\end{definition}
\begin{observation}
Podemos ver que la definición de aplicación proyectiva está bien definida puesto que 
\[f\left([\lambda v]\right) = \left[\hat{f}\left(\lambda v\right)\right] = \left[\lambda \hat{f}\left(v\right)\right] = \left[\hat{f}\left(v\right)\right]  .\]
\end{observation}
\begin{observation}
	Tenemos que realmente $\displaystyle f $ es de la forma $\displaystyle f : \mathbb{P}\left(V\right)/Z\left(f\right) \to \mathbb{P}\left(V'\right) $. Si $\displaystyle Z\left(f\right) = \emptyset $, es decir, $\displaystyle \Ker\left(\hat{f}\right) = \left\{ 0\right\}  $, entonces escribimos $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $. 
	Si además $\displaystyle \hat{f} $ es un isomorfismo, decimos que $\displaystyle f $ es una \textbf{homografía}. 
\end{observation}
\begin{prop}
	Sea $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ una aplicación proyectiva. Si $\displaystyle \hat{f},\hat{g} : V \to V' $ cumplen que $\displaystyle \left[\hat{f}\left(v\right)\right]  = \left[\hat{g}\left(v\right)\right] = f\left([v]\right)$, $\displaystyle \forall [v] \in \mathbb{P}\left(V\right)/Z\left(f\right) $, entonces existe $\displaystyle \lambda \in \K^{*} $ tal que $\displaystyle \hat{f} = \lambda \hat{g} $.
\end{prop}
\begin{proof}
	Tenemos que $\displaystyle f $ está bien definida en $\displaystyle \mathbb{P}\left(V\right)/\mathbb{P}\left(\Ker\left(\hat{f}\right)\right) = \mathbb{P}\left(V\right)/\mathbb{P}\left(\Ker\left(\hat{g}\right)\right) $. Por tanto, $\displaystyle \mathbb{P}\left(\Ker\left(\hat{f}\right)\right) = \mathbb{P}\left(\Ker\left(\hat{g}\right)\right) $ y en consecuencia $\displaystyle \Ker\left(\hat{f}\right) = \Ker\left(\hat{g}\right) $. Ponemos $\displaystyle \hat{Z} = \Ker \hat{f}$. 
	Buscamos un complemento de $\displaystyle \hat{Z} $ tal que $\displaystyle V = \hat{Z} \oplus W $. Si $\displaystyle v \in V $, $\displaystyle \exists ! z \in \hat{Z}, w \in W $ tales que $\displaystyle v = z + w $. Así, tenemos que 
	\[\hat{f}\left(v\right) = \hat{f}\left(z + w\right) = \hat{f}\left(z\right) + \hat{f}\left(w\right) = \hat{f}\left(w\right) .\]
	Análogamente, $\displaystyle \hat{g}\left(v\right) = \hat{g}\left(w\right) $. Así, tenemos que 
	\[f\left([v]\right) = [\hat{f}\left(v\right)] = [\hat{f}\left(w\right)] = [\hat{g}\left(v\right)] = [\hat{g}\left(w\right)] ,\]
	por lo que existe $\displaystyle \lambda_{w} \in \K^{*} $ tal que $\displaystyle \hat{f}\left(w\right) = \lambda_{w}\hat{g}\left(w\right) $. Necesitamos probar que $\displaystyle \forall w_{1}, w_{2} \in W $ se tiene que $\displaystyle \lambda_{w_{1}} = \lambda_{w_{2}} $. Consideremos 
	\[\hat{f}\left(w_{1} + w_{2}\right) = \lambda_{w_{1} + w_{2}}\hat{g}\left(w_{1} + w_{2}\right) = \lambda_{w_{1} + w_{2}}\hat{g}\left(w_{1}\right) + \lambda_{w_{1}+w_{2}}\hat{g}\left(w_{2}\right) = \lambda_{w_{1}}\hat{g}\left(w_{1}\right) + \lambda_{w_{2}}\hat{g}\left(w_{2}\right) .\]
	Si $\displaystyle \left\{ \hat{g}\left(w_{1}\right), \hat{g}\left(w_{2}\right)\right\}  $ son linealmente independientes, está claro que $\displaystyle \lambda_{w_{1}+w_{2}} = \lambda_{w_{1}} = \lambda_{w_{2}} $. Supongamos ahora que son linealmente dependientes, es decir, $\displaystyle \hat{g}\left(w_{1}\right)= \mu\hat{g}\left(w_{2}\right) $ para $\displaystyle \mu \in \K^{*} $ (para que $\displaystyle \mu \neq 0 $ debemos tomar $\displaystyle w_{1}, w_{2} \neq 0 $). Sabemos que $\displaystyle \hat{g}\left(w_{1}\right), \hat{g}\left(w_{2}\right) \neq 0 $. Así, nos queda que
	\[\hat{g}\left(w_{1}-\mu w_{2}\right) = 0 \iff w_{1} - \mu w_{2} \in \hat{Z} \cap W \iff w_{1} = \mu w_{2} .\]
Así, está claro que 
\[\hat{f}\left(w_{1} + w_{2}\right) = \left(1+\mu\right)\hat{f}\left(w_{2}\right) = \left(1+\mu\right)\lambda_{w_{2}}\hat{g}\left(w_{2}\right) = \mu\lambda_{w_{1}}\hat{g}\left(w_{2}\right) + \lambda_{w_{2}}\hat{g}\left(w_{2}\right) .\]
De aquí obtenemos que
\[ \mu \lambda_{w_{2}}\hat{g}\left(w_{2}\right) = \mu\lambda_{w_{1}}\hat{g}\left(w_{2}\right).\]
		Como $\displaystyle \mu \neq 0 $, tenemos que $\displaystyle \lambda_{w_{1}} = \lambda_{w_{2}} $. Por tanto, existe $\displaystyle \lambda \in \K^{*} $ tal que $\displaystyle \forall w \in W $ se tiene que $\displaystyle \hat{f} \left(w\right) = \lambda \hat{g}\left(w\right) $. Así, si $\displaystyle v \in V $, tenemos que 
	\[\hat{f}\left(v\right) = \hat{f}\left(w\right) = \lambda \hat{g}\left(w\right) = \lambda \hat{g}\left(v\right) .\]
\end{proof}
\begin{prop}
	Sean $\displaystyle \mathbb{P}\left(V\right) $ y $\displaystyle \mathbb{P}\left(V'\right) $ espacios proyectivos de dimensión $\displaystyle n $ sobre $\displaystyle \K $. Sea $\displaystyle \mathcal{R} = \left\{ P_{0}, \ldots, P_{n}; P_{n+1}\right\}  $ y $\displaystyle \mathcal{R}' = \left\{ P_{0}', \ldots, P_{n}'; P_{n+1}'\right\}  $ referencias proyectivas de $\displaystyle \mathbb{P}\left(V\right) $ y $\displaystyle \mathbb{P}\left(V'\right) $ respectivamente. 
Entonces, existe una única homografía $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ tal que $\displaystyle f\left(P_{i}\right) = P_{i}' $. 
\end{prop}
\begin{proof}
	Sea $\displaystyle \mathcal{B} = \left\{ v_{0}, \ldots, v_{n}\right\}  $ una base asociada a $\displaystyle \mathcal{R} $ y $\displaystyle \mathcal{B}' = \left\{ v_{0}', \ldots, v_{n}'\right\} $ una base asociada a $\displaystyle \mathcal{R}' $.
	\begin{description}
	\item[Existencia.] Tomamos $\displaystyle \hat{f} : V \to V' $ tal que $\displaystyle \hat{f}\left(v_{i}\right) = v_{i}' $, por lo que $\displaystyle \hat{f} $ es un isomorfismo. 
	En particular, $\displaystyle \Ker\hat{f} = \left\{ 0\right\}  $. Como $\displaystyle \dim\mathbb{P}\left(V\right) = \dim\mathbb{P}\left(V'\right) $ tenemos que $\displaystyle \hat{f} $ induce una homografía. Así, podemos tomar
	\[f\left(P_{i}\right) = f\left([v_{i}]\right) = [\hat{f}\left(v_{i}\right)] = [v_{i}'] = P_{i}' .\]
Además, tenemos que 
\[f\left(P_{n+1}\right) = f\left([v_{0} + \cdots + v_{n}]\right) = [\hat{f}\left(v_{0}+ \cdots + v_{n}\right)] = [v_{0}'+\cdots +v_{n}'] = P_{n+1}' .\]
\item[Unicidad.] Sea $\displaystyle h : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ una homografía tal que $\displaystyle h\left(P_{i}\right) = P_{i}' $ con $\displaystyle i = 0, \ldots, n+ 1 $. Sea $\displaystyle \hat{h} : V \to V'$ la aplicación lineal que induce $\displaystyle h $. 
Tenemos que 
\[h\left(P_{i}\right) = f\left(P_{i}\right) \Rightarrow [\hat{h}\left(v_{i}\right)] = [\hat{f}\left(v_{i}\right)] .\]
Así, tenemos que $\displaystyle \hat{h}\left(v_{i}\right) = \lambda_{i}v_{i}' $ y $\displaystyle \hat{h} \left(v_{0} + \cdots + v_{n}\right) = \lambda\left(v'_{0} + \cdots + v'_{n}\right) $. De esta manera, obtenemos que
\[\lambda\left(v_{0}' + \cdots + v_{n}'\right) = \lambda_{0}v_{0}' + \cdots + \lambda_{n}v_{n}' .\]
Como $\displaystyle \mathcal{B}' $ es una base tenemos que $\displaystyle \lambda_{0} = \cdots = \lambda_{n} = \lambda  $, así tenemos que $\displaystyle \hat{h}\left(v_{i}\right) = \lambda v_{i}' = \lambda \hat{f}\left(v_{i}\right) $, por lo que $\displaystyle \hat{h} = \lambda \hat{f} $ y $\displaystyle h = f $.
	\end{description}
\end{proof}
\begin{eg}
En $\displaystyle \R\mathbb{P}^{1} := \mathbb{P}\left(\R^{2}\right) $ consideremos 
\[
\begin{split}
	f : \R\mathbb{P}^{1} & \to \R\mathbb{P}^{1} : [x_{0}:x_{1}]  \to [x_{0}:2x_{1}] \\
	g : \R\mathbb{P}^{1} & \to \R\mathbb{P}^{1} : [x_{0}:x_{1}] \to [2x_{0}:x_{1}].
\end{split}
\]
Tenemos que $\displaystyle f\left([1:0]\right)=[1:0] $ y $\displaystyle g\left([1:0]\right)= [2:0] $, por lo que $\displaystyle  f\left([1:0]\right) =  g\left([1:0]\right) $. Análogamente, podemos ver que $\displaystyle f\left([0,1]\right)=[0:2] = [0:1] = g\left([0:1]\right) $. Sin embargo, como $\displaystyle f\left([1:1]\right) = [1:2] \neq [2:1] = g\left([1:1]\right)$ no puede tratarse de una homografía. Es decir, para poder decir que $\displaystyle f $ y $\displaystyle g $ son iguales debemos encontrar tres puntos que formen una referencia proyectiva y cuyas imágenes cuadren. Sin embargo una afinidad de $\displaystyle \R \to \R $ está determinada por dos puntos. 
\end{eg}
\begin{observation}
	Si $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ es una aplicación proyectiva, $\displaystyle \mathcal{R} $ y $\displaystyle \mathcal{R}' $ son referencias proyectivas de $\displaystyle \mathbb{P}\left(V\right) $ y $\displaystyle \mathbb{P}\left(V'\right) $, respectivamente, con bases asociadas $\displaystyle \mathcal{B} $ y $\displaystyle \mathcal{B}' $, existe una aplicación lineal $\displaystyle \hat{f} : V \to V' $ tal que $\displaystyle f\left([v]\right) = [\hat{f}\left(v\right)] $.
	Entonces, tenemos que la matriz que representa a la aplicación $\displaystyle f  $ será la clase de equivalencia
	\[ \left[M_{\mathcal{R}\mathcal{R}'}\left(f\right)\right] = \left\{ \lambda M_{\mathcal{B}\mathcal{B}}\left(\hat{f}\right) \; :\; \lambda \in \K^{*}\right\}  .\]
	Tenemos que 
	\[f\left(\left(x_{1}, \ldots, x_{n}\right)_{\mathcal{R}}\right) = M_{\mathcal{B}\mathcal{B}'}\left(\hat{f}\right)\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} = M_{\mathcal{R}\mathcal{R}'}\left(f\right)\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} .\]
\end{observation}
\begin{eg}
Consideremos los siguientes puntos en $\displaystyle \mathbb{P}\left(\R^{3}\right) $:
\[P_{0}= [1:0:0], \quad P_{1} = [1:-2:1], \quad P_{2} = [0:2:1], \quad P_{3} = [1:-2:0] .\]
\[Q_{0} = [1:-1:0], \quad Q_{1} = [1 : 0: 1], \quad Q_{2} = [1:-1:1], \quad Q_{3} = [1:0:0] .\]
Nos preguntamos si existe una aplicación proyectiva $\displaystyle f : \mathbb{P}\left(\R^{3}\right) \to \mathbb{P}\left(\R^{3}\right) $ tal que $\displaystyle f\left(P_{i}\right) = Q_{i} $ y calcular $\displaystyle f $ en la referencia estandar
\[\mathcal{E} = \left\{ [1:0:0], [0:1:0], [0:0:1]; [1:1:1]\right\}  .\]
Comprobemos que se $\displaystyle \mathcal{R} = \left\{ P_{i} \; :\; 0 \leq i \leq 3\right\}  $ es una referencia proyectiva. Tenemos que 
\[\begin{vmatrix} 1 & 1 & 0 \\ 0 & - 2 & 2 \\ 0 & 1 & 1 \end{vmatrix}  = - 4 \neq 0 .\]
Así, son independientes. Por otro lado, tenemos que 
\[\left(1,-2,0\right) = \alpha\left(1,0,0\right)+\beta\left(1,-2,1\right)+\gamma\left(0,2,1\right) .\]
Obtenemos que $\displaystyle \alpha = \frac{1}{2} $, $\displaystyle \beta = \frac{1}{2} $ y $\displaystyle \gamma = -\frac{1}{2} $. Como ninguno es nulo, tenemos que $\displaystyle \mathcal{R} $ es una referencia proyectiva que tiene de base asociada 
\[\mathcal{B} = \left\{ \left(1,0,0\right), \left(1, -2,1\right), \left(0,-2,-1\right)\right\}  .\]
Hacemos lo mismo para ver que $\displaystyle \mathcal{R}' = \left\{ Q_{i} \; : \; 0 \leq i \leq 3\right\}  $ es una referencia proyectiva y calculamos que su base asociada es
\[\mathcal{B}' = \left\{ \left(1,-1,0\right), \left(1,0,1\right), \left(-1,1,-1\right)\right\}  .\]
Por la última proposición, existe la $\displaystyle f $ que buscamos y es única por mandar de una referencia a otra. Así, podemos definir la aplicación lineal asociada como 
\[\hat{f}\left(1,0,0\right) = \left(1,-1,0\right), \quad \hat{f}\left(1,-2,1\right) = \left(0,-2,-1\right), \quad \hat{f}\left(0,-2,-1\right) = \left(-1,1,-1\right) .\]
De aquí es fácil deducir el valor de $\displaystyle \hat{f} $ en la referencia canónica calculando como actúa $\displaystyle \hat{f} $ sobre la base canónica o con matrices. Lo hacemos por matrices. Tenemos que $\displaystyle M_{\mathcal{B}\mathcal{B}'}\left(\hat{f}\right) = I $. Así, tenemos que 
\[M_{\mathcal{E}\mathcal{E}}\left(\hat{f}\right) = C_{\mathcal{B}'\mathcal{E}} M_{\mathcal{B}\mathcal{B}'}\left(\hat{f}\right)C_{\mathcal{E}\mathcal{B}}.\]
Donde tenemos que 
\[C_{\mathcal{E}\mathcal{B}} = C_{\mathcal{B}\mathcal{E}}^{-1} = \begin{pmatrix} 1 & 1 & 0 \\ 0 & - 2 & - 2 \\ 0 & 1 & -1 \end{pmatrix}^{-1} = \begin{pmatrix} 1 & \frac{1}{4} & -\frac{1}{2} \\ 0 & -\frac{1}{4} & \frac{1}{2} \\ 0 & -\frac{1}{4} & -\frac{1}{2} \end{pmatrix}, \; C_{\mathcal{B}'\mathcal{E}} = \begin{pmatrix} 1 & 1 & 1 \\ -1 & 0 & 1 \\ 0 & 1 & -1 \end{pmatrix} .\]
Como estamos en el caso proyectivo, tenemos que lo que nos importa son las clases de equivalencia de las matrices, no los representantes, por lo que podemos tomar
\[ C_{\mathcal{E}\mathcal{B}} = \begin{pmatrix} 4 & 1 & - 2 \\ 0 & - 1 & 2 \\ 0 & - 1 & - 2 \end{pmatrix} .\]
Así, nos queda
\[M_{\mathcal{E}\mathcal{E}}\left(f\right) = \begin{pmatrix} 1 & 1 & -1 \\ -1 & 0 & 1 \\ 0 & 1 & - 1 \end{pmatrix}\begin{pmatrix} 4 & 1 & -2\\ 0 & - 1 & 2 \\ 0 & - 1 & -2 \end{pmatrix} = \begin{pmatrix} 4 & 1 & 1 \\ -4 & -2 & 0 \\ 0 & 0 & 4 \end{pmatrix} .\]
En la igualdad anterior estamos igualando clases de equivalencia, no ponemos los corchetes por estética. Así, tendríamos que la expresión analítica de $\displaystyle f $ es
\[f\left([x_{0}:x_{1}:x_{2}]\right) = [4x_{0}+x_{1}+x_{2}:-4x_{0}-2x_{1}:4x_{2}] .\]
\end{eg}
\begin{prop}
Sea $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ y $\displaystyle g : \mathbb{P}\left(V'\right) \to \mathbb{P}\left(V''\right) $ aplicaciones proyectivas con aplicaciones lineales asociadas $\displaystyle \hat{f} : V \to V' $ y $\displaystyle \hat{g} : V' \to V'' $. Entonces
\begin{enumerate}
\item La composición $\displaystyle g\circ f $ es una aplicación proyectiva y $\displaystyle \widehat{g \circ f} =\hat{g}\circ\hat{f} $.
\item $\displaystyle f $ es inyectiva si y solo si $\displaystyle \hat{f} $ es inyectiva.
\item $\displaystyle f $ es sobreyectiva si y solo si $\displaystyle \hat{f} $ es sobreyectiva.
\end{enumerate}
\end{prop}
\begin{proof}
	\begin{enumerate}
	\item Tenemos que $\displaystyle g\circ f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V''\right) $ cumple que $\displaystyle \forall v \in V $,
		\[g\left(f\left([v]\right)\right) = g([\hat{f}\left(v\right)]) = [\hat{g}(\hat{f}\left(v\right))] .\]
	Así, está claro que $\displaystyle g\circ f $ es una aplicación proyectiva y que $\displaystyle \widehat{g\circ f} = \hat{g}\circ\hat{f} $.
\item Supongamos que $\displaystyle f $ es inyectiva y que $\displaystyle \hat{f}\left(v\right) = \hat{f}\left(w\right) $. Así, tenemos que
	\[[\hat{f}\left(v\right)] = [\hat{f}\left(w\right)] \Rightarrow f\left([v]\right)=f\left([w]\right) \Rightarrow [v]=[w] .\]
	Así, tenemos que $\displaystyle v = \lambda w $ para $\displaystyle \lambda \in \K^{*} $. Así, tenemos que $\displaystyle \hat{f}\left(v\right) = \lambda\hat{f}\left(w\right) = \hat{f}\left(w\right) $, por lo que debe ser que $\displaystyle \lambda = 1 $ (tenemos que $\displaystyle \hat{f}\left(w\right)\neq 0 $). Recíprocamente, si $\displaystyle \hat{f} $ es inyectiva y $\displaystyle f\left([v]\right) = f\left([w]\right) $, tenemos que 
	\[[\hat{f}\left(v\right)] = [\hat{f}\left(w\right)] \Rightarrow \hat{f}\left(v\right) = \lambda \hat{f}\left(w\right), \lambda \in \K^{*}.\]
	Así, obtenemos que $\displaystyle v-\lambda w \in \Ker\hat{f} $, por lo que $\displaystyle v - \lambda w = 0 $ y $\displaystyle v = \lambda w $, de forma que $\displaystyle [v] = [w] $.
\item Supongamos que $\displaystyle f $ es sobreyectiva. Si $\displaystyle v \in V' $, tenemos que existe $\displaystyle u \in V $ tal que $\displaystyle f\left([u]\right) = [v] $, es decir, 
	\[[\hat{f}\left(u\right)] = [v] \Rightarrow \hat{f}\left(u\right) = \lambda v, \; \lambda \in \K^{*} .\]
Así, si tomamos $\displaystyle u' = \frac{u}{\lambda } \not\in \Ker\left(\hat{f}\right) $ tenemos que 
\[\hat{f}\left(u'\right) = \hat{f}\left(\frac{u}{\lambda }\right) = \frac{1}{\lambda }\lambda v = v .\]
Recíprocamente, supongamos que $\displaystyle \hat{f} $ es sobreyectiva. Si $\displaystyle [v] \in \mathbb{P}\left(V'\right) $, podemos coger a su representante $\displaystyle v \in V' $ y sabemos que existe $\displaystyle u \in V $ tal que $\displaystyle \hat{f}\left(u\right) = v $, es decir,
\[f\left([u]\right) = [\hat{f}\left(u\right)] = [v] .\]
	\end{enumerate}
\end{proof}
\section{Variedades}
\subsection{Variedades afines}
\begin{definition}[Variedad afín]
Si $\displaystyle \mathbb{A} $ es un espacio afín, una \textbf{variedad} es un conjunto $\displaystyle \emptyset \neq X \subset \mathbb{A} $ tal que existe $\displaystyle P \in X $ y existe $\displaystyle W \in \mathcal{L}\left(\vec{\mathbb{A}}\right) $ tal que 
\[X = P + W = \left\{ P + w \; : \; w \in W\right\}  .\]
\end{definition}
\begin{lema}
Sean $\displaystyle X $ una variedad de $\displaystyle \mathbb{A} $; $\displaystyle P,Q \in X $; $\displaystyle W,U \in \mathcal{L}\left(\vec{\mathbb{A}}\right) $. Si $\displaystyle P + W = Q + U = X $, entonces $\displaystyle W = U $ y $\displaystyle \overrightarrow{PQ} \in W $.
\end{lema}
\begin{proof}
Supongamos que $\displaystyle P + W = Q + U $, tenemos que $\displaystyle Q \in P + W $, es decir, existe $\displaystyle w \in W $ tal que $\displaystyle P + w = Q $, por lo que $\displaystyle \overrightarrow{PQ} = w \in W $. Por otro lado, si $\displaystyle u \in U $, tenemos que $\displaystyle Q + u \in P + W $, por lo que existe $\displaystyle w \in W $ tal que $\displaystyle Q + u = P + w $, así nos queda que $\displaystyle u = \overrightarrow{QP} +w \in W $ por lo que $\displaystyle u \in W $. El otro contenido se hace de forma análoga por lo que $\displaystyle W = U $.
\end{proof}
\begin{notation}
Si $\displaystyle X $ es una variedad de $\displaystyle \mathbb{A} $ y $\displaystyle X = P + W $, ponemos que $\displaystyle \vec{X} = W $ y lo llamamos la \textbf{dirección} de $\displaystyle X $.
\end{notation}
\begin{lema}
Sea $\displaystyle X $ una variedad de $\displaystyle \mathbb{A} $. Entonces, $\displaystyle \forall P \in X $, $\displaystyle X = P + \vec{X} $.
\end{lema}
\begin{proof}
Por definición, sabemos que existe $\displaystyle O \in X $ tal que $\displaystyle X = O + \vec{X} $. Sea $\displaystyle P \in X $, tenemos que ver que $\displaystyle P + \vec{X} = O + \vec{X} $. Como $\displaystyle P \in O + \vec{X} $, existe $\displaystyle v \in \vec{X} $ tal que $\displaystyle P = O + v $, es decir, $\displaystyle \overrightarrow{OP} = v $. Así, si $\displaystyle Q \in P + \vec{X} $, existe $\displaystyle w \in \vec{X} $ tal que 
\[Q = P + w = O + \overrightarrow{OP} + w = O + \left(v + w\right) \in O + \vec{X} .\]
Análogamente, si $\displaystyle Q \in O + \vec{X} $, existe $\displaystyle w \in \vec{X} $ tal que 
\[Q = O + w = P + \overrightarrow{PO} + w = P + \left(w -v\right) \in P + \vec{X} .\]
Así, tenemos que $\displaystyle O+\vec{X} = P + \vec{X} $.
\end{proof}
\begin{definition}[Dimensión de una variedad]
Sea $\displaystyle X \subset \mathbb{A} $ una variedad. Decimos que la \textbf{dimensión} de $\displaystyle X $ es $\displaystyle \dim X := \dim_{\K}\vec{X} $. 
\begin{itemize}
\item Si $\displaystyle \dim X = 0 $, $\displaystyle X $ es un \textbf{punto}.
\item Si $\displaystyle \dim X = 1 $, $\displaystyle X $ es una \textbf{recta}.
\item Si $\displaystyle \dim X = 2 $, $\displaystyle X $ es un \textbf{plano}.
\item Si $\displaystyle \dim X = \dim \mathbb{A} -1 $, $\displaystyle X $ es un \textbf{hiperplano}.
\end{itemize}
\end{definition}
\begin{lema}
Sea $\displaystyle X,Y \subset \mathbb{A} $ variedades. Si $\displaystyle X \cap Y \neq \emptyset $, entonces $\displaystyle X \cap Y  $ es una variedad y $\displaystyle \overrightarrow{X \cap Y} = \vec{X} \cap \vec{Y} $. 
\end{lema}
\begin{proof}
Tenemos que ver que $\displaystyle X \cap Y = P + \vec{X} \cap \vec{Y} $ con $\displaystyle P \in \mathbb{A} $. Como $\displaystyle X \cap Y \neq \emptyset $, existe $\displaystyle P \in X \cap Y $. Es trivial que $\displaystyle P + \vec{X} \cap \vec{Y} \subset X \cap Y $. Ahora, si $\displaystyle Q \in X \cap Y $, tenemos que $\displaystyle \overrightarrow{PQ} \in \vec{X} \cap \vec{Y} $, por lo que $\displaystyle Q = P + \overrightarrow{PQ} \in P + \vec{X} \cap \vec{Y} $. 
\end{proof}
\begin{definition}[Variedad generada por un conjunto]
La variedad de $\displaystyle \mathbb{A} $ \textbf{generada} por $\displaystyle S \subset \mathbb{A} $ la denotamos por $\displaystyle V\left(S\right) $ y consiste en la menor variedad que contiene a $\displaystyle S $. Así, tenemos que 
\[V\left(S\right) = \bigcap_{ \substack{X \; \text{variedad} \\ S \subset X}}X .\]
\end{definition}
\begin{lema}
	Dados $\displaystyle \left\{ P_{0}, \ldots, P_{r}\right\} \subset \mathbb{A} $, entonces 
	\[ V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right) = \left\{ \sum^{r}_{i = 0}\lambda_{i}P_{i}\; : \; \sum^{r}_{i = 0}\lambda_{i} = 1\right\}.\]
	\[  \overrightarrow{V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right)} = L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right).\]
\end{lema}
\begin{proof}
	Sea $\displaystyle Y = V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right) $. Como $\displaystyle P_{0}, \ldots, P_{r} \in Y $, tenemos que $\displaystyle \overrightarrow{P_{0}P_{i}} \in \vec{Y} $. Así, necesariamente debe ser que $\displaystyle L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right) \subset \vec{Y} $, por lo que
	\[P_{0} + L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right) \subset Y .\]
	Por ser $\displaystyle Y $ la menor variedad que contiene a $\displaystyle \left\{ P_{0}, \ldots, P_{r}\right\}  $ tenemos que $\displaystyle Y \subset P_{0} + L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right) $, por lo que $\displaystyle Y = P_{0} + L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right) $. Así, nos queda que
	\[\vec{Y} = L\left(\overrightarrow{P_{0}P_{1}}, \ldots, \overrightarrow{P_{0}P_{r}}\right) .\]	
\end{proof}
\begin{observation}
	Tenemos que $\displaystyle \left\{ P_{0}, \ldots, P_{r}\right\} \subset \mathbb{A} $ es afinmente independiente si y solo si $\displaystyle V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right) \neq V\left( \left\{ P_{0}, \ldots, P_{i-1}, P_{i+1}, \ldots, P_{r}\right\} \right) $, $\displaystyle \forall i \in \left\{ 0, \ldots, r\right\}  $. Esto es equivalente a que $\displaystyle \dim V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right) =r $.	
\end{observation}
\begin{prop}
Un conjunto $\displaystyle X \subset \mathbb{A} $ es una variedad si y solo si $\displaystyle X $ es cerrado por combinaciones afines. 
\end{prop}
\begin{proof}
\begin{description}
	\item[(i)] Si $\displaystyle X $ es una variedad y $\displaystyle P_{0}, \ldots, P_{r} \in X $, entonces $\displaystyle V\left( \left\{ P_{0}, \ldots, P_{r}\right\} \right)\subset X $, así el conjunto de combinaciones afines de $\displaystyle \left\{ P_{0}, \ldots, P_{r}\right\}  $ están contenidas en $\displaystyle X $. 
	\item[(ii)] Sabemos que $\displaystyle X $ es cerrado por combinaciones afines. Sea $\displaystyle W = \left\{ \overrightarrow{AB} \; : \; A,B \in X\right\} \subset \vec{\mathbb{A}} $. Vamos a ver que $\displaystyle W $ es un subespacio vectorial. Consideremos $\displaystyle \overrightarrow{AB}, \overrightarrow{CD} \in W $ y $\displaystyle \lambda, \mu \in \K $, tenemos que
		\[\lambda \overrightarrow{AB} + \mu \overrightarrow{CD} = \lambda \overrightarrow{AB} + \mu\left(\overrightarrow{CA} + \overrightarrow{AD}\right) = \lambda \overrightarrow{AB} -\mu\overrightarrow{AC} + \mu\overrightarrow{AD} .\]
		Así, tenemos que 
		\[A + \left(\lambda \overrightarrow{AB} + \mu\overrightarrow{CD}\right) = \left(1-\lambda \right)A + \lambda B -\mu C + \mu D \in X .\]
	Por tanto, tenemos que $\displaystyle A + \left(\lambda \overrightarrow{AB} + \mu\overrightarrow{CD}\right) \in X $ y en consecuencia $\displaystyle \lambda \overrightarrow{AB} + \mu \overrightarrow{CD} \in W $, por lo que $\displaystyle W $ es un subespacio vectorial. Falta probar que $\displaystyle X = A + W $. Si $\displaystyle B \in X $, tenemos que $\displaystyle \overrightarrow{AB} \in W $ por lo que $\displaystyle B = A + \overrightarrow{AB} \in A + W $. Recíprocamente, si tomamos $\displaystyle w \in W $, tenemos que existen $\displaystyle B,C \in X $ tal que $\displaystyle w = \overrightarrow{BC} $. Veamos que $\displaystyle A + w \in X $. 
	\[A + w = A + \overrightarrow{BC} = A + \overrightarrow{BA} + \overrightarrow{AC}= A - \overrightarrow{AB} + \overrightarrow{AC} = 1 \cdot A - 1 \cdot B + 1 \cdot C \in X ,\]
	por estar $\displaystyle X $ cerrado por combinaciones afines.
\end{description}
\end{proof}
\begin{notation}
Dadas dos variedades $\displaystyle X_{1} $ y $\displaystyle X_{2} $, denotamos $\displaystyle X_{1} + X_{2} := V\left(X_{1} \cup X_{2}\right) $, es decir, $\displaystyle X_{1} + X_{2} $ es la menor variedad que contiene a $\displaystyle X_{1} \cup X_{2} $.
\end{notation}
\begin{lema}
Sean $\displaystyle X_{1} = P_{1} + \overrightarrow{X}_{1} $ y $\displaystyle X_{2} = P_{2} + \overrightarrow{X}_{2} $ variedades. Entonces, 
\[X_{1} + X_{2} = P_{1} + \left(L\left(\overrightarrow{P_{1}P_{2}}\right)+\overrightarrow{X}_{1} + \overrightarrow{X}_{2}\right) .\]
En particular, $\displaystyle \overrightarrow{X_{1}+X_{2}} = L\left(\overrightarrow{P_{1}P_{2}}+\overrightarrow{X}_{1}+\overrightarrow{X}_{2}\right)$.
\end{lema}
\begin{proof}
Consideremos 
\[ Y = P_{1} + \left(L\left(\overrightarrow{P_{1}P_{2}}\right)+\overrightarrow{X}_{1} + \overrightarrow{X}_{2}\right).\]
Está claro que $\displaystyle X_{1} = P_{1} + \overrightarrow{X}_{1} \subset Y $. Análogamente, $\displaystyle X_{2} = P_{2} + \overrightarrow{X}_{2} = P_{1}+\overrightarrow{P_{1}P_{2}} + \overrightarrow{X}_{2} \subset Y $. De esta manera, tenemos que $\displaystyle X_{1} + X_{2} \subset Y $, puesto que $\displaystyle X_{1} + X_{2} $ es la menor variedad que contienen a $\displaystyle X_{1} $ y $\displaystyle X_{2} $.
Por otro lado, supongamos que $\displaystyle P \in Y $, entonces tenemos que existen $\displaystyle \lambda \in \K $, $\displaystyle w_{1} \in W_{1} $ y $\displaystyle w_{2} \in W_{2}$ tales que
\[P = P_{1} + \lambda \overrightarrow{P_{1}P_{2}} + w_{1} + w_{2} .\]
Podemos encontrar $\displaystyle Q_{1} \in X_{1} $ y $\displaystyle Q_{2} \in X_{2} $ tales que
\[
\begin{split}
	P = & P_{1} +\lambda\overrightarrow{P_{1}P_{2}} + \overrightarrow{P_{1}Q_{1}} + \overrightarrow{P_{2}Q_{2}} = P_{1} + \lambda \overrightarrow{P_{1}P_{2}} + \overrightarrow{P_{1}Q_{1}} + \overrightarrow{P_{2}P_{1}} + \overrightarrow{P_{1}Q_{2}} \\
	= & P_{1} + \lambda\overrightarrow{P_{1}P_{2}} + \overrightarrow{P_{1}Q_{1}} - \overrightarrow{P_{1}P_{2}} + \overrightarrow{P_{1}Q_{2}} = -\lambda P_{1} + \left(\lambda - 1\right)P_{2} + Q_{1}+Q_{2}.
\end{split}
\]
Así, tenemos que $\displaystyle P $ es una combinación afín de $\displaystyle P_{1}, P_{2}, Q_{1}, Q_{2} \in X_{1} \cup X_{2} \subset X_{1} + X_{2} $, que está cerrado por combinaciones afines, por lo que $\displaystyle P \in X_{1} + X_{2} $ e $\displaystyle Y \subset X_{1} + X_{2} $.
\end{proof}
\begin{theorem}[Fórmula de Grassmann]
Sean $\displaystyle X_{1}, X_{2} \subset \mathbb{A} $ variedades con $\displaystyle \dim\mathbb{A}\leq \infty $. Entonces
\[\dim X_{1} + X_{2} = \dim X_{1} + \dim X_{2} - \dim \left(\vec{X}_{1} \cap \vec{X}_{2}\right) + \epsilon ,\]
donde 
\[\epsilon = 
\begin{cases}
0, \; X_{1} \cap X_{2} \neq \emptyset \\ 
1, \; X_{1} \cap X_{2} = \emptyset
\end{cases}
.\]
\end{theorem}
\begin{proof}
Tenemos que
\[\dim X_{1} + X_{2} = \dim_{\K}\overrightarrow{X_{1} + X_{2}} .\]
\begin{itemize}
\item Si $\displaystyle X_{1} \cap X_{2} \neq \emptyset $ existe $\displaystyle P \in X_{1} \cap X_{2} $ tal que $\displaystyle X_{1} = P + \overrightarrow{X}_{1} $ y $\displaystyle X_{2} = P + \overrightarrow{X}_{2} $. Así, tenemos que
\[\overrightarrow{X_{1} + X_{2}} = L\left(\overrightarrow{P P}\right) + \overrightarrow{X}_{1} + \overrightarrow{X}_{2} = \overrightarrow{X}_{1} + \overrightarrow{X}_{2} .\]
Entonces,
\[\dim X_{1} + X_{2} = \dim_{\K}\overrightarrow{X}_{1} + \overrightarrow{X}_{2} = \dim_{\K}\overrightarrow{X}_{1} + \dim_{\K}\overrightarrow{X}_{2} - \dim_{\K}\overrightarrow{X}_{1} \cap \overrightarrow{X}_{2} .\]
\item Ahora, si $\displaystyle X_{1} \cap X_{2} = \emptyset $, tenemos que $\displaystyle X_{1} = P_{1} + \overrightarrow{X}_{1} $ y $\displaystyle X_{2} = P_{2} + \overrightarrow{X}_{2} $. Veamos que $\displaystyle \overrightarrow{P_{1}P_{2}} \not\in \overrightarrow{X}_{1} + \overrightarrow{X}_{2} $. En efecto, si $\displaystyle \overrightarrow{P_{1}P_{2}} \in \overrightarrow{X}_{1} + \overrightarrow{X}_{2} $ tenemos que existen $\displaystyle v_{1} \in \overrightarrow{X}_{1} $ y $\displaystyle v_{2} \in \overrightarrow{X}_{2} $ tales que
\[\overrightarrow{P_{1}P_{2}}=v_{1} +v_{2} .\]
Existe $\displaystyle B \in X_{1} $ tal que $\displaystyle v_{1} = \overrightarrow{P_{1}B} $. Así, nos queda que 
\[v_{2} = \overrightarrow{P_{1}P_{2}} -v_{1} = \overrightarrow{P_{1}P_{2}}-\overrightarrow{P_{1}B} = \overrightarrow{BP_{1}} + \overrightarrow{P_{1}P_{2}} = \overrightarrow{BP_{2}} .\]
Así, tendríamos que $\displaystyle P_{2}-v_{2} = P_{2}-\overrightarrow{BP_{2}} = P_{2}+\overrightarrow{P_{2}B } = B \in X_{1} \cap X_{2} $, que es una contradicción. Por tanto, tendremos que
\[
\begin{split}
	\dim X_{1} + X_{2} = & \dim_{\K}\left(\overrightarrow{X_{1}+X_{2}}\right) \\
	= & 1 + \dim_{\K}\left(\overrightarrow{X}_{1} + \overrightarrow{X}_{2}\right) = 1 + \dim_{\K}\overrightarrow{X}_{1} + \dim_{\K}\overrightarrow{X}_{2} - \dim_{\K}\left(\overrightarrow{X}_{1} \cap \overrightarrow{X}_{2}\right).
\end{split}
\]
\end{itemize}
\end{proof}
\begin{eg}
En $\displaystyle \R^{3} $ si tomamos $\displaystyle X_{1} $ como una recta y $\displaystyle X_{2} $ como un plano tal que $\displaystyle X_{1} \cap X_{2} = \emptyset $ y $\displaystyle \overrightarrow{X}_{1} \subset \overrightarrow{X}_{2} $, tenemos que
\[\dim X_{1} + X_{2} = \dim X_{1} + \dim X_{2} - \dim\left(\overrightarrow{X}_{1} \cap \overrightarrow{X}_{2}\right) +\epsilon = 3 .\]
Sin embargo, tenemos que
\[\dim X_{1} + X_{2} \neq \dim X_{1} + \dim X_{2} - \dim\left(X_{1} \cap X_{2}\right) + \epsilon ,\]
esta última ecuación no tiene sentido puesto que no hemos descrito ninguna forma de asociar una dimensión al conjunto vacío.
\end{eg}
\begin{definition}
Sean $\displaystyle X_{1} $ y $\displaystyle X_{2} $ variedades de $\displaystyle \mathbb{A} $. 
\begin{enumerate}
\item Se dice que son \textbf{paralelas} si $\displaystyle X_{1} \cap X_{2} = \emptyset $ y $\displaystyle \overrightarrow{X}_{1} \subset \overrightarrow{X}_{2} $ o $\displaystyle \overrightarrow{X}_{2} \subset \overrightarrow{X}_{1} $.
\item Se dice que se \textbf{cortan} si $\displaystyle X_{1} \cap X_{2} \neq \emptyset $, $\displaystyle \overrightarrow{X}_{1} \not\subset \overrightarrow{X}_{2} $ y $\displaystyle \overrightarrow{X}_{2} \not\subset \overrightarrow{X}_{1} $. 
\item Se dice que se \textbf{cruzan} si $\displaystyle X_{1} \cap X_{2} = \emptyset $, $\displaystyle \overrightarrow{X}_{1} \not\subset \overrightarrow{X}_{2} $ y $\displaystyle \overrightarrow{X}_{2} \not\subset \overrightarrow{X}_{1} $. 
\end{enumerate}
\end{definition}
\begin{prop}
Sean $\displaystyle \mathbb{A} $ y $\displaystyle \mathbb{A}' $ dos espacios afines, $\displaystyle X \subset \mathbb{A} $, $\displaystyle Y \subset \mathbb{A}' $ variedades afines y $\displaystyle f : \mathbb{A} \to \mathbb{A}' $ una aplicación afín. Entonces,
\begin{enumerate}
\item $\displaystyle f\left(X\right) $ es una variedad afín y 
	\[\overrightarrow{f\left(X\right)} = \vec{f}\left(\overrightarrow{X}\right) \quad \text{y} \quad f\left(X\right) = f\left(A\right) + \vec{f}\left(\overrightarrow{X}\right), \; A \in X .\]
\item Si $\displaystyle f^{-1}\left(Y\right) \neq \emptyset $, entonces $\displaystyle f^{-1}\left(Y\right) $ es una variedad afín y
	\[\overrightarrow{f^{-1}\left(Y\right)} = \vec{f}^{-1}\left(\overrightarrow{Y}\right) \quad \text{y} \quad f^{-1}\left(Y\right)= A + \vec{f}^{-1}\left(\overrightarrow{Y}\right), \; A \in f^{-1}\left(Y\right) .\]
\end{enumerate}
\end{prop}
\begin{proof}
\begin{enumerate}
\item Sea $\displaystyle X = A + \vec{X} $, veamos que $\displaystyle f\left(X\right)= f\left(A\right) + \vec{f}\left(\vec{X}\right) $. Si $\displaystyle B \in f\left(X\right) $ entonces existe $\displaystyle C \in X $ tal que $\displaystyle f\left(C\right) = B $. Como $\displaystyle C \in X $ tenemos que $\displaystyle C = A + v $ con $\displaystyle v \in \vec{X} $. Por ser $\displaystyle f $ una aplicación afín tenemos que
	\[B = f\left(C\right) = f\left(A + v\right)=f\left(A\right) + \vec{f}\left(v\right) \in f\left(A\right) + \vec{f}\left(\vec{X}\right) .\]
	Recíprocamente, si $\displaystyle B \in f\left(A\right) + \vec{f}\left(\vec{X}\right) $, tenemos que existe $\displaystyle v \in \vec{f}\left(\vec{X}\right) $ y en consecuencia $\displaystyle u \in \vec{X} $ con $\displaystyle v = \vec{f}\left(u\right) $, tal que
	\[B = f\left(A\right) + v = f\left(A\right) + \vec{f}\left(u\right) = f\left(A + u\right) \in f\left(X\right).\]
\item Supongamos que $\displaystyle f^{-1}\left(Y\right) \neq \emptyset $, entonces existe $\displaystyle A \in f^{-1}\left(Y\right) $, es decir, $\displaystyle f\left(A\right) \in Y $. Si $\displaystyle B \in f^{-1}\left(Y\right) $, tenemos que $\displaystyle f\left(B\right) \in Y $. Por ser $\displaystyle f $ una aplicación afín tenemos que
	\[f\left(B\right) = f\left(A+\overrightarrow{AB}\right) = f\left(A\right) + \vec{f}\left(\overrightarrow{AB}\right) \iff \vec{f}\left(\overrightarrow{AB}\right) = \overrightarrow{f\left(A\right)f\left(B\right)} \in \vec{Y} .\]
	Así, tenemos que $\displaystyle \overrightarrow{AB} \in \vec{f}^{-1}\left(\vec{Y}\right) $, por lo que $\displaystyle B \in A + \vec{f}^{-1}\left(\vec{Y}\right) $. Recíprocamente, si $\displaystyle B \in A + \vec{f}^{-1}\left(\vec{Y}\right) $, tenemos que existe $\displaystyle v \in \vec{f}^{-1}\left(\vec{Y}\right) $ tal que $\displaystyle B = A + v $. Así, tenemos que 
	\[f\left(B\right) = f\left(A\right) + \vec{f}\left(v\right) \in Y .\]
\end{enumerate}
\end{proof}
\subsection{Ecuaciones de variedades afines}
Sea $\displaystyle \mathbb{A} $ un espacio afín de dimensión $\displaystyle n $ y $\displaystyle X \subset \mathbb{A} $ una variedad de dimensión $\displaystyle d $. En una referencia cartesiana $\displaystyle \mathcal{R}_{C} = \left\{ O, \mathcal{B}\right\}  $ podemos describir $\displaystyle X $ con ecuaciones implícitas con $\displaystyle C \in \mathcal{M}_{\left(n-d\right) \times n}\left(\K\right) $ y $\displaystyle a_{1}, \ldots, a_{n-d} \in \K $ tales que 
\[\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} \in X \iff C\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}=\begin{pmatrix} a_{1} \\ \vdots \\ a_{n-d} \end{pmatrix} .\]
\begin{eg}
Consideremos la variedad dada por las ecuaciones
\[
\begin{cases}
2x_{1} + x_{2} = 2 \\ 
3x_{1} + x_{3} = 1
\end{cases}
.\]
En este caso tenemos que $\displaystyle n = 3 $ y $\displaystyle d = 1 $, donde $\displaystyle n-d $ es el número de ecuaciones. Tenemos que $\displaystyle X $ es una recta. 
\end{eg}
Tenemos que $\displaystyle \overrightarrow{X}= \left\{ \overrightarrow{OA} \; : \; A \in X\right\}  $. Así, si $\displaystyle O = \left(y_{1}, \ldots, y_{n}\right) $ y $\displaystyle \overrightarrow{OA} = \left(x_{1}-y_{1}, \ldots, x_{n}-y_{n}\right) $,
\[C\begin{pmatrix} x_{1}-y_{1} \\ \vdots \\ x_{n}-y_{n} \end{pmatrix} = C\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}-C\begin{pmatrix} y_{1} \\ \vdots \\ y_{n} \end{pmatrix}=\begin{pmatrix} a_{1} \\ \vdots \\ a_{n-d} \end{pmatrix} - \begin{pmatrix} a_{1} \\ \vdots \\ a_{n-d} \end{pmatrix}= 0 .\]
Por tanto, tenemos que
\[ \left(x_{1}, \ldots, x_{n}\right)_{\mathcal{B}} \in \overrightarrow{X} \iff C\begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}= \begin{pmatrix} 0 \\ \vdots \\ 0 \end{pmatrix}.\]
\begin{eg}
En el ejemplo anterior, tenemos que 
\[\overrightarrow{X}= \left\{ \left(x_{1}, x_{2}, x_{3}\right) \; : \; 
\begin{cases}
2x_{1} + x_{2} = 0 \\
3x_{1} + x_{3} = 0
\end{cases}
\right\}  .\]
\end{eg}
También podemos describir $\displaystyle X $ con \textbf{ecuaciones paramétricas}. Es decir, existen $\displaystyle C \in \mathcal{M}_{n \times d}\left(\K\right) $ y $\displaystyle P = \left(a_{1}, \ldots, a_{n}\right)_{\mathcal{R}_{C}} $ entonces
\[\begin{pmatrix} x_{1}\\ \vdots \\ x_{n} \end{pmatrix} \in X \iff \exists \begin{pmatrix} \lambda_{1} \\ \vdots \\ \lambda_{d} \end{pmatrix} \in \K^{d}, \; \begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix} = \begin{pmatrix} a_{1} \\ \vdots\\ a_{n} \end{pmatrix} + C\begin{pmatrix} \lambda_{1} \\ \vdots \\ \lambda_{d} \end{pmatrix}.\]
\begin{eg}
Consideremos la variedad
\[X = \left\{ \left(x_{1}, x_{2}, x_{3}\right) \; : \; 
\begin{cases}
2x_{1} + x_{2} = 2 \\ 
3x_{1} +x_{3} = 1
\end{cases}
\right\}  .\]
Tenemos que $\displaystyle P = \left(0,2,1\right) \in X $ y
\[\overrightarrow{X}= \left\{ \left(x_{1}, x_{2}, x_{3}\right) \; : \; 
\begin{cases}
2x_{1} + x_{2} = 0 \\
3x_{1} + x_{3} = 0
\end{cases}
\right\} = L\left(\left(1,-2,-3\right)\right) .\]
Así, tendríamos que las ecuaciones paramétricas para esta variedad serían
\[X = \left\{ \left(0,2,1\right) + \lambda\left(1,-2,-3\right) \; : \; \lambda \in \K\right\}  .\]
\end{eg}
En una referencia afín $\displaystyle \mathcal{R}_{A} $ podemos describir $\displaystyle X $ con \textbf{ecuaciones baricéntricas}. Es decir, existe $\displaystyle C \in \mathcal{M}_{\left(n + 1\right) \times \left(d + 1\right)} $ tal que
\[\begin{pmatrix} x_{0} \\ \vdots \\ x_{n} \end{pmatrix}\in X \iff \begin{pmatrix} x_{0} \\ \vdots \\ x_{n} \end{pmatrix} = C\begin{pmatrix} \mu_{0} \\ \vdots \\ \mu_{d} \end{pmatrix} , \; \sum^{d}_{i = 0}\mu_{i} = 1. \]
\begin{eg}
En el ejemplo anterior consideremos la referencia afín estándar
\[\mathcal{R}_{A}= \left\{ \left(0,0,0\right), \left(1,0,0\right), \left(0,1,0\right), \left(0,0,1\right)\right\}  .\]
Recordamos que 
\[X = \left\{ \left(0,2,1\right) + \lambda\left(1,-2,-3\right) \; : \; \lambda \in \K\right\}  .\]
Si tomamos $\displaystyle P_{0} = \left(0,2,1\right) $, $\displaystyle \overrightarrow{P_{0}P_{1}} = \left(1, -2,-3\right) $ y $\displaystyle P_{1} = \left(1,0,-2\right) $. De esta manera nos queda 
\[X = \left\{ P_{0} + \lambda\overrightarrow{P_{0}P_{1}} \; : \; \lambda \in \K\right\}  = \left\{ \left(1-\lambda \right)P_{0} + \lambda P_{1} \; : \; \lambda \in \K\right\}  .\]
Matricialmente nos queda
\[X = \left\{ \begin{pmatrix} x_{0} \\ x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}_{\mathcal{R}_{A}} \; : \; \begin{pmatrix} x_{0} \\ x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}_{\mathcal{R}_{A}} = \left(1-\lambda \right)\begin{pmatrix} -2 \\ 0 \\ 2 \\ 1 \end{pmatrix} + \lambda \begin{pmatrix} 2 \\ 1 \\ 0 \\ - 2 \end{pmatrix}\right\}  .\]
\end{eg}
\begin{eg}
Consideremos $\displaystyle \mathbb{A} = \R^{3} $ y las variedades
\[X_{1} = \left(0,0,1\right) + L\left(\left(2,3,0\right), \left(-1,-1,0\right)\right) .\]
\[X_{2} = \left\{ 2x_{1} + x_{2} = 3\right\}  .\]
Calculemos las ecuaciones implícitas y paramétricas de $\displaystyle X_{1} + X_{2} $ y $\displaystyle X_{1} \cap X_{2} $. \\
En primer lugar, calculemos las ecuaciones implícitas de $\displaystyle X_{1} $:
\[
	X_{1} = \left\{ \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} + \lambda\begin{pmatrix} 2 \\ 3 \\ 0 \end{pmatrix}+\mu\begin{pmatrix} -1 \\ - 1 \\ 0 \end{pmatrix} \; : \; \lambda, \mu \in \K\right\} 
.\]
Obtenemos el sistema 
\[
\begin{cases}
x_{1} = 2\lambda -\mu \\ 
x_{2} = 3\lambda - \mu \\ 
x_{3} = 1
\end{cases} \Rightarrow 
X_{1} = \left\{ x_{3} = 1\right\} 
.\]
Así, tenemos que $\displaystyle X_{1} \cap X_{2} $ viene dado por los puntos que satisfacen las ecuaciones implícitas de $\displaystyle X_{1} $ y $\displaystyle X_{2} $:
\[X_{1} \cap X_{2} =
\begin{cases}
x_{3} = 1 \\ 
2x_{1} + x_{2} = 3
\end{cases}
.\]
Para obtener las ecuaciones paramétricas necesitamos calcular $\displaystyle \overrightarrow{X_{1} \cap X_{2}} $:
\[
\begin{cases}
x_{3} = 0 \\ 
2x_{1} + x_{2} = 0 
\end{cases}
\Rightarrow \overrightarrow{X_{1} \cap X_{2}} = L\left(\left(-1,2,0\right)\right).\]
Como $\displaystyle \left(1,1,1\right) \in X_{1} \cap X_{2} $ tenemos que 
\[X_{1} \cap X_{2} = \left\{ \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix} =\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} + \lambda \begin{pmatrix} -1 \\ 2 \\ 0 \end{pmatrix} \; : \; \lambda \in \K\right\}  .\]
Por otro lado, tenemos que $\displaystyle \dim\left(X_{1} + X_{2}\right) = 3 $, por lo que $\displaystyle X_{1}+X_{2} = \mathbb{A} $. 
\end{eg}
\subsection{Variedades proyectivas}
\begin{definition}[Variedad proyectiva]
	Se dice que $\displaystyle X \subset \mathbb{P}\left(V\right) $ es una \textbf{variedad proyectiva} si existe $\displaystyle U \in \mathcal{L}\left(V\right) $ tal que $\displaystyle X = \mathbb{P}\left(U\right) $, es decir, $\displaystyle X = \left\{ [u] \; : \; u \in U / \left\{ 0\right\} \right\}  $. La dimensión de $\displaystyle X $ es $\displaystyle \dim X = \dim_{\K}U - 1 $. 
\end{definition}
\begin{observation}
En el mundo proyectivo tenemos que el conjunto $\displaystyle \emptyset $ es una variedad de dimensión $\displaystyle -1 $. Sin embargo, en el mundo afín el conjunto $\displaystyle \emptyset $ no es una variedad.
\end{observation}
\begin{prop}
Consideremos la aplicación
\[\pi : V/ \left\{ 0\right\} \to \mathbb{P}\left(V\right) : v \to [v] .\]
Para $\displaystyle X \subset \mathbb{P}\left(V\right) $ definimos 
\[\hat{X} : = \pi^{-1}\left(X\right) \cup \left\{ 0\right\} .\]
Si $\displaystyle X $ es una variedad tenemos que $\displaystyle \hat{X} $ es un subespacio vectorial de $\displaystyle V $ y $\displaystyle \mathbb{P}\left(\hat{X}\right) = X $.
\end{prop}
\begin{proof}
\begin{description}
	\item[(i)] Supongamos que $\displaystyle x \in \hat{X} $. Si $\displaystyle x = 0 $ claramente $\displaystyle x \in \pi^{-1}\left(X\right) \cup \left\{ 0\right\}  $. Si $\displaystyle x \neq 0 $, tenemos que $\displaystyle [x] \in X $, por lo que $\displaystyle x \in \pi^{-1}\left(X\right)\cup \left\{ 0\right\}  $. 
	\item[(ii)] Supongamos que $\displaystyle x \in \pi^{-1}\left(X\right) \cup \left\{ 0\right\}  $. Si $\displaystyle x = 0 $ claramente $\displaystyle x \in \hat{X} $. Si $\displaystyle x \neq 0 $, tenemos que $\displaystyle \pi\left(x\right) = [x] \in X $, por lo que debe ser que $\displaystyle x \in \hat{X} $. 
\end{description} 
\end{proof}
\begin{prop}
	Sean $\displaystyle \left\{ X_{i}\right\} _{i\in I} $ una familia de variedades de $\displaystyle \mathbb{P}\left(V\right) $. Entonces, $\displaystyle X = \bigcap_{i \in I}X_{i} $ es una variedad y $\displaystyle \hat{X} = \bigcap_{i \in I}\hat{X}_{i} $. 
\end{prop}
\begin{proof}
	Tenemos que $\displaystyle \bigcap_{i \in I}X_{i} $ es una variedad si y solo si $\displaystyle \pi^{-1}\left(\bigcap_{i \in I}X_{i}\right) \cup \left\{ 0\right\}  $ es un subespacio vectorial. Tenemos que 
	\[\pi^{-1}\left(\bigcap_{i \in I}X_{i}\right) \cup \left\{ 0\right\} = \bigcap_{i \in I}\pi^{-1}\left(X_{i}\right) \cup \left\{ 0\right\}  = \bigcap_{i \in I}\left(\pi^{-1}\left(X_{i}\right) \cup \left\{ 0\right\} \right) = \bigcap_{i \in I}\hat{X}_{i} .\]
	Esto último sabemos que es un subespacio vectorial, por lo que $\displaystyle X $ es una variedad.
\end{proof}
\begin{definition}[Variedad generada por un conjunto]
Sea $\displaystyle \emptyset \neq S \subset \mathbb{P}\left(V\right) $. La \textbf{variedad proyectiva generada por} $\displaystyle S $ es 
\[V\left(S\right) = \bigcap_{\substack{X \subset \mathbb{P}\left(V\right) \; \text{variedad} \\ S \subset X}}X .\]
Es decir, $\displaystyle V\left(S\right) $ es la menor variedad que contiene a $\displaystyle S $.
\end{definition}
\begin{prop}
	$\displaystyle \widehat{V\left(S\right)} = L[\pi^{-1}\left(S\right)] $.
\end{prop}
\begin{proof}
	Ponemos $\displaystyle U = L[\pi^{-1}\left(S\right)] $. En primer lugar tenemos que ver que $\displaystyle \widehat{V\left(S\right)} \supset U $. Como $\displaystyle S \subset V\left(S\right) $ tenemos que
	\[\pi^{-1}\left(S\right) \subset \pi^{-1}\left(V\left(S\right)\right) \cup \left\{ 0\right\}  = \widehat{V\left(S\right)} \Rightarrow L[\pi^{-1}\left(S\right)] \subset \widehat{V\left(S\right)} \Rightarrow U \subset \widehat{V\left(S\right)} .\]
Recíprocamente tenemos que 
$\displaystyle S \subset \pi\left(U\right) \Rightarrow V\left(S\right) \subset \pi\left(U\right) $ Como $\displaystyle \pi\left(U\right) $ es una variedad tenemos que $\displaystyle \widehat{V\left(S\right)} \subset \widehat{\pi\left(U\right)} = U $. 
\end{proof}
\begin{definition}
Si $\displaystyle X_{1} $ y $\displaystyle X_{2} $ son variedades de $\displaystyle \mathbb{P}\left(V\right) $, definimos
\[X_{1} + X_{2} : = V\left(X_{1} \cup X_{2}\right) .\]
Por la proposición anterior tenemos que $\displaystyle \widehat{X_{1} + X_{2}} = \widehat{V\left(X_{1} \cup X_{2}\right)} = L\left(\hat{X}_{1} \cup \hat{X}_{2}\right) = \hat{X}_{1} + \hat{X}_{2} $.
\end{definition}
\begin{theorem}[Teorema de incidencia]
Sea $\displaystyle \mathbb{P}\left(V\right) $ de dimensión finita y $\displaystyle X,Y $ variedades de $\displaystyle \mathbb{P}\left(V\right) $. Entonces, 
\[\dim\left(X+Y\right) = \dim\left(X\right) + \dim\left(Y\right) - \dim\left(X \cap Y\right) .\]
\end{theorem}
\begin{proof}
Aplicando la fórmula de Grassmann
\[
\begin{split}
	\dim\left(X + Y\right) = & \dim_{\K}\left(\widehat{X + Y}\right)-1 = \dim_{\K}\left(\hat{X} + \hat{ Y}\right)-1 
	=   \dim_{\K}\hat{X} + \dim_{\K}\hat{Y} - \dim_{\K}\left(\hat{X} \cap \hat{ Y}\right)-1 \\
	= & \left(\dim_{\K}\hat{X}-1\right) + \left(\dim_{\K}\hat{Y}-1\right)-\left(\dim_{\K}\left(\hat{X}\cap\hat{Y}\right)-1\right)\\
	= & \dim X + \dim Y - \left(\dim_{\K}\widehat{X \cap Y} - 1\right) 
	=  \dim X + \dim Y - \dim\left(X \cap Y\right).
\end{split}
\]
\end{proof}
\begin{definition}[Posición relativa de dos variedades proyectivas.]
Sean $\displaystyle X_{1}, X_{2} \subset \mathbb{P}\left(V\right) $ dos variedades proyectivas. 
\begin{enumerate}
\item Si $\displaystyle X_{1} \subset X_{2} $ o $\displaystyle X_{2} \subset X_{1} $, una de las variedades está incluida en la otra.
\item Si $\displaystyle X_{1} \cap X_{2} \neq \emptyset $, $\displaystyle X_{1} \not\subset X_{2} $ y $\displaystyle X_{2}\not\subset X_{1} $, las variedades se \textbf{cortan}.
\item Si $\displaystyle X_{1} \cap X_{2} = \emptyset $, las variedades \textbf{no se cortan.} 
\end{enumerate}
\end{definition}
\begin{colorary}
Si $\displaystyle \dim\mathbb{P}\left(V\right) = n \geq 2 $ y $\displaystyle X,Y \subset \mathbb{P}\left(V\right) $ son hiperplanos, entonces $\displaystyle X \cap Y \neq \emptyset $. En particular, en $\displaystyle \mathbb{P}^{2} $ todo par de rectas se interseca.
\end{colorary}
\begin{proof}
Por el teorema anterior, tenemos que
\[
\dim\left(X \cap Y\right) = \dim X + \dim Y - \dim\left(X + Y\right) .
\]
Como $\displaystyle X \subset X + Y \subset \mathbb{P}\left(V\right) $ tenemos que 
\[\dim X = n-1 \leq \dim\left(X+Y\right)\leq n = \dim \mathbb{P}\left(V\right) .\]
Así, tenemos que
\[\dim\left(X \cap Y\right) \geq \dim X + \dim Y - n \geq \left(n-1\right) + \left(n-1\right) - n \geq n-2 \geq 0.\]
\end{proof}
\begin{observation}
De forma más general, es cierto que si $\displaystyle X $ es un hiperplano de $\displaystyle \mathbb{P}\left(V\right) $ e $\displaystyle Y $ es una variedad con $\displaystyle \dim Y \geq 1 $, la intersección de $\displaystyle X $ e $\displaystyle Y $ no es vacía. 
\end{observation}
\begin{prop}
Sea $\displaystyle f : \mathbb{P}\left(V\right) \to \mathbb{P}\left(V'\right) $ una aplicación proyectiva de centro $\displaystyle Z = Z\left(f\right) $ y aplicación lineal asociada $\displaystyle \hat{f} $. 
\begin{enumerate}
\item Si $\displaystyle X \subset \mathbb{P}\left(V\right) $ es una variedad, entonces $\displaystyle f\left(X/Z\right) $ es una variedad y $\displaystyle \widehat{f\left(X/Z\right)} = \hat{f}\left(\hat{X}\right) $. 
\item Si $\displaystyle Y \subset \mathbb{P}\left(V'\right) $ es una variedad, entonces $\displaystyle f^{-1}\left(Y\right)\cup Z $ es una variedad y $\displaystyle \widehat{f^{-1}\left(Y\right)\cup Z} = \hat{f}^{-1}\left(\hat{Y}\right) $.
\end{enumerate}
\end{prop}
\begin{proof}
\begin{enumerate}
	\item Basta ver que $\displaystyle f\left(X/Z\right) = \mathbb{P}\left(\hat{f}\left(\hat{X}\right)\right) $. En primer lugar podemos ver que $\displaystyle \hat{f}\left(\hat{X}\right) / \left\{ 0\right\} = \hat{f}\left(\hat{X}/\Ker\left(\hat{f}\right)\right) $. Además, 
	\[
	\begin{split}
		f\left(X/Z\right) = & \left\{ \left[\hat{f}\left(u\right)\right]  \in \mathbb{P}\left(E'\right) \; : \; u \in \hat{X}/\Ker\left(\hat{f}\right)\right\} = \left\{ [w] \in \mathbb{P}\left(E'\right) \; : \; w \in \hat{f}\left(\hat{X}\right) / \left\{ 0\right\} \right\} \\
		= &  \mathbb{P}\left(\hat{f}\left(\hat{X}\right)\right) .
	\end{split}
	\]
\item Basta con demostrar que $\displaystyle f^{-1}\left(Y\right) \cup Z = \mathbb{P}\left(\hat{f}^{-1}\left(\hat{Y}\right)\right) $. Podemos ver que $\displaystyle \hat{f}^{-1}\left(\hat{Y}\right) = \hat{f}^{-1}\left(\hat{Y}/ \left\{ 0\right\} \right) \cup \Ker\left(\hat{f}\right) $. Así, tenemos que 
	\[
	\begin{split}
		f^{-1}\left(Y\right) \cup Z = & \left\{ [u] \in \mathbb{P}\left(V\right) \; : \; \left[\hat{f}\left(u\right)\right] \in Y \; \text{o} \; [u] \in Z\right\} = \left\{ [u] \in \mathbb{P}\left(V\right)\; : \; u \in \hat{f}^{-1}\left(\hat{Y}/ \left\{ 0\right\} \right)\cup \Ker\left(\hat{f}\right)\right\} \\
		= & \mathbb{P}\left(\hat{f}^{-1}\left(\hat{Y}\right)\right) .
	\end{split}
	\]
\end{enumerate}
\end{proof}
\begin{notation}
Por estética escribiremos 
\[f\left(X\right):=f\left(X/Z\right)\quad \text{y} \quad f^{-1}\left(Y\right):= f^{-1}\left(Y\right)\cup Z .\]
\end{notation}
\begin{eg}
	Consideremos $\displaystyle f : \mathbb{P}^{3} \to \mathbb{P}^{3} : [x_{0}:x_{1}:x_{2}:x_{3}] \to [x_{0}:x_{1}:0:0]$. Tenemos que $\displaystyle Z\left(f\right)= \left\{ [0:0:x_{2}:x_{3}]\right\} = \left\{ x_{0} = 0, x_{1} = 0\right\}  $, que es una recta. Si $\displaystyle X = \left\{ x_{0}-x_{2} = 0\right\}  $ es un plano de $\displaystyle \mathbb{P}^{3} $, calculamos $\displaystyle f\left(X\right) $.
	Para ello, calculamos $\displaystyle \hat{f} : \K^{4} \to \K^{4} : \left(x_{0}, x_{1}, x_{2}, x_{3}\right) \to \left(x_{0}, x_{1}, 0, 0\right)$. Tenemos que
	\[\hat{X}= \left\{ \left(x_{0}, x_{1}, x_{2}, x_{3}\right) \in \K^{4} \; : \; x_{0}-x_{2} = 0\right\} = L\left(\left(1,0,1,0\right), \left(0,1,0,0\right), \left(0,0,0,1\right)\right) .\]
Así, tenemos que 
\[
\begin{split}
	\hat{f}\left(\hat{X}\right) = & L\left(\hat{f}\left(1,0,1,0\right), \hat{f}\left(0,1,0,0\right), \hat{f}\left(0,0,0,1\right)\right) = L\left(\left(1,0,0,0\right), \left(0,1,0,0\right)\right) \\
	= &  \left\{ x_{2} = 0, x_{3} = 0\right\}  .
\end{split}
\]
Así, nos queda que $\displaystyle f\left(X\right) = \left\{ x_{2}= 0, x_{3}=0\right\}  $ que es una recta de $\displaystyle \mathbb{P}^{3} $. Por otro lado, dado $\displaystyle Y = \left\{ [1:0:0:0]\right\}  $, tenemos que para calcular $\displaystyle f^{-1}\left(Y\right) $:
\[
\begin{split}
	\hat{f}^{-1}\left(L\left(1,0,0,0\right)\right) = & L\left(\left(1,0,0,0\right),\Ker\hat{f}\right)=L\left(\left(1,0,0,0\right), \left(0,0,1,0\right), \left(0,0,0,1\right)\right)\\
	= & \left\{ x_{1} = 0\right\} .
\end{split}
\]
Así, tenemso que $\displaystyle f^{-1}\left(Y\right) = \left\{ x_{1} = 0\right\}  $ que es un plano de $\displaystyle \mathbb{P}^{3} $.
\end{eg}

