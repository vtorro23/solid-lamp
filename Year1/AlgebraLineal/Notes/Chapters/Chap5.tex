\chapter{Formas Bilineales Simétricas}
\begin{fdefinition}[Forma bilineal]
\normalfont Una \textbf{forma bilineal} definida en $\displaystyle V $ es una aplicación $\displaystyle \beta : V \times V \to \K $ que verifica que
\begin{description}
\item[(a)] $\displaystyle \forall \vec{x}, \vec{y}, \vec{z} \in V $, $\displaystyle \beta\left(\vec{x}+\vec{y}, \vec{z}\right) = \beta\left(\vec{x}, \vec{z}\right) + \beta\left(\vec{y}, \vec{z}\right) $.
\item[(b)] $\displaystyle \forall \vec{x}, \vec{y}, \vec{z} \in V $, $\displaystyle \beta\left(\vec{x}, \vec{y}+\vec{z}\right) = \beta\left(\vec{x}, \vec{y}\right) + \beta\left(\vec{x}, \vec{z}\right) $.
\item[(c)] $\displaystyle \forall \vec{x}, \vec{y} \in V, \forall a \in \K$, $\displaystyle \beta\left(a\vec{x}, \vec{y}\right) = \beta\left(\vec{x}, a\vec{y}\right) = a \beta\left(\vec{x}, \vec{y}\right) $.
\end{description}
\end{fdefinition}
\begin{observation}
\normalfont Resulta trivial que $\displaystyle \forall \vec{x}, \vec{y} \in V $, $\displaystyle \beta\left(\vec{0}, \vec{y}\right) = \beta\left(\vec{x}, \vec{0}\right) = 0 $. En efecto, 
\[ \beta\left(\vec{x}, \vec{0}\right) = \beta\left(\vec{x}, 0 \cdot \vec{y}\right) = 0 \cdot \beta\left(\vec{x}, \vec{y}\right) = 0 .\]
\end{observation}
\begin{ftheorem}[]
	\normalfont Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ una base de $\displaystyle V $, y sean $\displaystyle a_{ij} \in \K $ con $\displaystyle i,j = 1, \ldots, n $. Entonces, $\displaystyle \exists! \beta : V \times V \to \K $ tal que $\displaystyle \beta\left(\vec{u}_{i}, \vec{u}_{j}\right) = a_{ij} $.
\end{ftheorem}
\begin{proof}
\begin{description}
\item[Unicidad.] Supongamos que existe. Sean $\displaystyle \vec{x},\vec{y}\in V $ tales que 
	\[
	\begin{split}
	\vec{x} = x^{1}\vec{u}_{1} + \cdots +x^{n}\vec{u}_{n} \\
	\vec{y} = y^{1}\vec{u}_{1} + \cdots + y^{n}\vec{u}_{n}.
	\end{split}
	\]
Entonces, tenemos que 
\[
\begin{split}
	\beta\left(\vec{x}, \vec{y}\right) = & \beta\left(x^{1}\vec{u}_{1} + \cdots +x^{n}\vec{u}_{n}, \vec{y}\right) = x^{1}\beta\left(\vec{u}_{1}, \vec{y}\right) + \cdots + x^{n}\beta\left(\vec{u}_{n}, \vec{y}\right) \\
	= & \sum^{n}_{i,j = 1}x^{i}y^{j}\beta\left(\vec{u}_{i}, \vec{u}_{j}\right) = \sum^{n}_{i,j = 1}x^{i}y^{j}a_{ij}.
\end{split}
\]
Matricialmente, lo podemos expresar de la siguiente manera:
\[\beta\left(\vec{x}, \vec{y}\right) = \begin{pmatrix} x^{1} & \cdots & x^{n} \end{pmatrix} \begin{pmatrix} a_{11} & \cdots & a_{1n} \\
\vdots & & \vdots \\
a_{n1} & \cdots & a_{nn}\end{pmatrix} \begin{pmatrix} y^{1} \\ \vdots \\ y^{n} \end{pmatrix}.\]
\item[Existencia.] Defino $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = \sum^{n}_{i,j = 1}x^{i}y^{j}a_{ij} $. Vamos a ver que es bilineal. Tenemos que 
\[
\begin{split}
	\beta\left(\vec{x}+\vec{y}, \vec{z}\right) = & \sum^{n}_{i,j = 1}\left(x^{i}+z^{i}\right)y^{j}a_{ij} = \sum^{n}_{i,j = 1}\left( x^{i}y^{j} + z^{i}y^{j}\right)a_{ij} = \sum^{n}_{i,j=1}x^{i}y^{j}a_{ij} + \sum^{n}_{i,j = 1}z^{i}y^{j}a_{ij}\\
	= &  \beta\left(\vec{x}, \vec{z}\right) + \beta\left(\vec{y}, \vec{z}\right).
\end{split}
\]
Las propiedades \textbf{(b)} y \textbf{(c)} se deduce de forma análoga.  	
\end{description}
\end{proof}
Entonces, tenemos que la matriz de $\displaystyle \beta  $ en la base $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ será
\[ \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta \right) = \begin{pmatrix} a_{11} & \cdots & a_{1n} \\
\vdots & & \vdots \\
a_{n1} & \cdots & a_{nn}\end{pmatrix} = \begin{pmatrix} \beta\left(\vec{u}_{1}, \vec{u}_{1}\right) & \cdots & \beta\left(\vec{u}_{1}, \vec{u}_{n}\right)\\
\vdots & & \vdots \\
\beta\left(\vec{u}_{n}, \vec{u}_{1}\right) & \cdots & \beta\left(\vec{u}_{n}, \vec{u}_{n}\right)\end{pmatrix}.\]
\begin{observation}
\normalfont 
La correspondencia entre matrices y formas bilineales es biyectiva. Así, podemos definir como hicimos anteriormente la suma y el producto de matrices de forma bilineales. En efecto, se trata de un isomorfismo entre el espacio vectorial de las formas bilineales definidas en $\displaystyle V \times V$ y el espacio de las matrices $\displaystyle n \times n $ sobre $\displaystyle \K $.
\end{observation}
\begin{fdefinition}[]
\normalfont Una forma bilineal $\displaystyle \beta : V \times V \to \K $ es \textbf{simétrica} si $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = \beta\left(\vec{y}, \vec{x}\right) $, $\displaystyle \forall \vec{x}, \vec{y} \in V $.
\end{fdefinition}
\begin{observation}
	\normalfont Si $\displaystyle A \in \mathcal{M}_{n \times n} $ es simétrica ($\displaystyle A = A^{t} $), entonces $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = \begin{pmatrix} x^{1} & \cdots & x^{n} \end{pmatrix} A \begin{pmatrix} y^{1} \\ \vdots \\ y^{n} \end{pmatrix} $. Entonces tenemos que 
	\[\beta\left(\vec{x}, \vec{y}\right)^{t} = \left(\begin{pmatrix} x^{1} & \cdots & x^{n} \end{pmatrix} A \begin{pmatrix} y^{1} \\ \vdots \\ y^{n} \end{pmatrix}\right)^{t} = \begin{pmatrix} y^{1} & \cdots & y^{n} \end{pmatrix} \underbrace{A^{t}}_{A} \begin{pmatrix} x^{1} \\ \vdots \\ x^{n} \end{pmatrix} = \beta\left(\vec{y}, \vec{x}\right).\]
	Por tanto, $\displaystyle \beta  $ es simétrica.
\end{observation}
\begin{fdefinition}[Ortogonal]
\normalfont Sea $\displaystyle \beta  $ una forma bilineal simétrica. Diremos que los vectores $\displaystyle \vec{x}, \vec{y} \in V $ son \textbf{ortogonales} respecto de $\displaystyle \beta  $ si $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = 0 $. Si $\displaystyle \emptyset \neq A \subset V $, llamaremos \textbf{ortogonal} a $\displaystyle A $ respecto de $\displaystyle \beta  $ y escribiremos $\displaystyle A^{\perp }_{\beta } = \left\{ \vec{x} \in V \; : \; \beta\left(\vec{x}, \vec{y}\right) = 0, \; \forall \vec{y} \in A\right\}  $.
\end{fdefinition}
\begin{fprop}[]
\normalfont Si $\displaystyle A, B \subset V $:
\begin{description}
\item[(i)] Si $\displaystyle A \subset B $, $\displaystyle B_{\beta }^{\perp } \subset A^{\perp }_{\beta } $.
\item[(ii)] $\displaystyle A^{\perp }_{\beta } = \left(L\left(A\right)\right)^{\perp }_{\beta } $.
\end{description}
\end{fprop}
\begin{proof}
\begin{description}
\item[(i)] Si $\displaystyle \vec{x} \in B^{\perp }_{\beta} $, tenemos que $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = 0 $, $\displaystyle \forall \vec{y}\in B $. Por tanto, $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = 0 $, $\displaystyle \forall \vec{y} \in A $.
\item[(ii)] Tenemos que $\displaystyle A \subset L\left(A\right) $, por lo que $\displaystyle L\left(A\right)^{\perp } \subset A^{\perp } $. Ahora, sea $\displaystyle \vec{x} \in A^{\perp }_{\beta } $. Si $\displaystyle \vec{y} \in L\left(A\right) $, tenemos que existen $\displaystyle \vec{x}_{1}, \ldots, \vec{x}_{p} \in A $ tales que
	\[\vec{y} = a^{1}\vec{x}_{1} + \cdots + a^{p}\vec{x}_{p} .\]
Entonces, tenemos que 
\[
\begin{split}
	\beta\left(\vec{x}, \vec{y}\right) = & \beta\left(\vec{x}, a^{1}\vec{x}_{1} + \cdots + a^{p}\vec{x}_{p}\right)  \\
	= & a^{1}\beta\left(\vec{x}, \vec{x}_{1}\right) + \cdots + a^{p}\beta\left(\vec{x}, \vec{x}_{p}\right) = 0.
\end{split}
\]
Así, tenemos que $\displaystyle A^{\perp } \subset L\left(A\right)^{\perp }_{\beta } $.
\end{description}
\end{proof}
Sea $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{n}\right\}  $ otra base de $\displaystyle V $. Entonces, tenemos que 
	\[ \begin{pmatrix} \vec{v}_{1} & \cdots & \vec{v}_{n} \end{pmatrix} = \begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix}C, \; C \in \GL\left(n, \K\right) .\]
En efecto, si $\displaystyle \vec{x} = \sum^{n}_{i = 1}x^{i}\vec{u}_{i} = \sum^{n}_{i = 1}x'^{i}\vec{v}_{i} $. Así:
\[ \begin{pmatrix} x'^{1} \\ \vdots \\ x'^{n} \end{pmatrix} = C \begin{pmatrix} x^{1} \\ \vdots \\ x^{n} \end{pmatrix} .\]
Recordamos que, si $\displaystyle \beta  $ es simétrica, 
	\[
	\begin{split}
			\beta\left(\vec{x}, \vec{y}\right) =  \begin{pmatrix} x'^{1} & \cdots & x'^{n} \end{pmatrix} \mathcal{M}_{ \left\{ \vec{v}_{i}\right\} }\left(\beta \right)\begin{pmatrix} y'^{1} \\ \vdots \\ y'^{n} \end{pmatrix} 
				=   \begin{pmatrix} x^{1} & \cdots & x^{n} \end{pmatrix} \underbrace{C^{t}\mathcal{M}_{ \left\{ \vec{v}_{i}\right\} }\left(\beta \right) C}_{\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left(\beta\right)}} \begin{pmatrix}y^{1} \\ \vdots \\ y^{n} \end{pmatrix} .
	\end{split}
	\]
\begin{fdefinition}[]
\normalfont Dos matrices $\displaystyle A,B \in \mathcal{M}_{n \times n}\left(\K\right) $ son \textbf{congruentes} si $\displaystyle \exists C \in \GL\left(n, \K\right) $ tal que $\displaystyle B = C^{t}AC $.
\end{fdefinition}
\begin{observation}
	\normalfont Tenemos que $\displaystyle \beta  $ es simétrica si y solo si $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta \right) = \left( \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta \right)\right)^{t}   $.
\end{observation}
Si $\displaystyle A,B \in \mathcal{M}_{ n \times n }\left(\K\right) $ son congruentes, tenemos que 
\[ \det\left(A\right) =\det\left(C^{t}BC\right) = \det\left(C^{t}\right)\det\left(B\right)\det\left(C\right) = \det\left(C\right)^{2}\det\left(B\right) .\]
\begin{fdefinition}[Discriminante]
	\normalfont Llamaremos \textbf{discriminante} de $\displaystyle \beta  $ respecto de la base $\displaystyle \left\{ \vec{u}_{1}, \ldots , \vec{u}_{n}\right\}  $ al $\displaystyle \det\left(\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta\right)\right) $, donde $\displaystyle \beta  $ es una forma bilineal simétrica.
\end{fdefinition}
\begin{observation}
\normalfont Por lo visto en el cálculo anterior, al cambiar de base cambia el discriminante pero simplemente queda multiplicado por un cuadrado no nulo. Es decir, el hecho de ser una forma bilineal de discriminante nulo o no nulo no depende de la base considerada.
\end{observation}
\begin{fdefinition}[Radical]
\normalfont Llamaremos \textbf{radical} de $\displaystyle \beta  $ a $\displaystyle V^{\perp }_{\beta } $ ($\displaystyle \rad\left(\beta\right) $).
\end{fdefinition}
\begin{fdefinition}[Isótropo]
\normalfont Un vector $\displaystyle \vec{x} \in V $ es \textbf{isótropo} si $\displaystyle \beta\left(\vec{x}, \vec{x}\right) = 0 $, es decir, es ortogonal a sí mismo.
\end{fdefinition}
\begin{fdefinition}[]
	\normalfont Se dice que $\displaystyle \beta  $ es \textbf{no degenerada} si $\displaystyle \rad\left(\beta \right) = \left\{ \vec{0}\right\}  $.
\end{fdefinition}
\begin{observation}
\normalfont Si $\displaystyle \rad\left(\beta \right) = \left\{ \vec{0}\right\}  $ y $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = 0 $, $\displaystyle \forall \vec{y} \in V $, tenemos que $\displaystyle \vec{x} = \vec{0} $.
\end{observation}
\begin{observation}
\normalfont Si $\displaystyle \forall \vec{x} \in V $, $\displaystyle \beta\left(\vec{x}, \vec{x}\right)=0 $, tenemos que $\displaystyle \forall \vec{x}, \vec{y} \in V $, $\displaystyle \beta\left(\vec{x} + \vec{y}, \vec{x} + \vec{y}\right) =0$. Entonces se puede deducir que
\[ 0 = \beta\left(\vec{x} + \vec{y}, \vec{x} + \vec{y}\right) = \beta\left(\vec{x}, \vec{x}\right) + \beta\left(\vec{y} , \vec{y}\right) + 2 \beta\left(\vec{x}, \vec{y}\right) \iff \beta\left(\vec{x}, \vec{y}\right) = 0 .\]
Entonces tenemos que $\displaystyle \beta = 0 $.
\end{observation}
Sea $\displaystyle \beta : V \times V \to \K $ una forma bilineal simétrica.
\begin{ftheorem}[]
\normalfont $\displaystyle \beta  $ es no degenerada si y solo si el discrimintante de $\displaystyle \beta  $ respecto de una base es no nulo. Además, $\displaystyle \beta  $ induce una $\displaystyle \overline{\beta} : V/ \rad\left(\beta \right) \times V / \rad\left(\beta \right) \to \K $ bilineal simétrica no degenerada.
\end{ftheorem}
\begin{proof}
	Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ una base de $\displaystyle V $. Entonces, tenemos que $\displaystyle \rad\left(\beta \right) = \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\} ^{\perp }_{\beta } $. Tenemos que
	\[\vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} \in \rad\left(\beta \right) \iff \beta\left(\vec{x}, \vec{u}_{i}\right) = 0, \; i = 1, \ldots, n .\]
Tenemos que
\[
\begin{split}
0 =	\beta\left(\vec{x}, \vec{u}_{1}\right) = & x^{1}\beta \left(\vec{u}_{1}, \vec{u}_{1}\right) + \cdots + x^{n}\beta\left(\vec{u}_{n}, \vec{u}_{1}\right) \\
	\vdots \\
0 =	\beta\left(\vec{x}, \vec{u}_{n}\right) = & x^{1}\beta\left(\vec{u}_{1}, \vec{u}_{n}\right) + \cdots + x^{n}\beta\left(\vec{u}_{n}, \vec{u}_{n}\right).
\end{split}
\]
Nos encontramos ante un sistema de ecuaciones lineales homogéneo. Si $\displaystyle \beta  $ fuera no degenerada, tendríamos que el sistema obtenido no tiene solución no trivial, lo que solo se verifica si la matriz de $\displaystyle \beta  $ admite inversa. Es decir, si el discriminante respecto de esa base es no nulo. Recíprocamente, si el discriminante no fuera nulo, el sistema sólo tendría la solución trivial, por lo que $\displaystyle \beta $ sería no degenerado. \\ 
Supongamos que $\displaystyle \rad\left(\beta \right) $ es no nulo. Sea $\displaystyle \overline{\beta } : V/\rad\left(\beta \right) \times V/\rad\left(\beta\right) \to \K $, definida por $\displaystyle \overline{\beta }\left(\vec{x} + \rad\left(\beta \right), \vec{y} + \rad\left(\beta \right)\right) = \beta\left(\vec{x} , \vec{y}\right) $.
Vamos a ver que la aplicación está bien definida. Sean $\displaystyle \vec{x}', \vec{y}' $ tales que $\displaystyle \vec{x}' + \rad\left(\beta \right) = \vec{x} + \rad\left(\beta \right) $ y $\displaystyle \vec{y}' + \rad\left(\beta \right) = \vec{y} + \rad\left(\beta \right) $. Entonces, tenemos que $\displaystyle \vec{x}-\vec{x}', \vec{y}-\vec{y}' \in \rad\left(\beta\right) $.
Ahora, sean $\displaystyle \vec{u}, \vec{v} \in\rad\left(\beta \right) $ tales que $\displaystyle \vec{x} = \vec{x}'- \vec{u} $ y $\displaystyle \vec{y} = \vec{y}'-\vec{v} $. Entonces, tenemos que 
\[
	\beta\left(\vec{x}', \vec{y}'\right) =  \beta\left(\vec{x} + \vec{u}, \vec{y} + \vec{v}\right) = \beta\left(\vec{x}, \vec{y}\right) + \beta\left(\vec{x}, \vec{v}\right) + \beta\left(\vec{u}, \vec{y}\right) + \beta\left(\vec{u}, \vec{v}\right) = \beta\left(\vec{x}, \vec{y}\right) .
\]
El hecho de que $\displaystyle \overline{\beta } $ es bilineal es consecuencia de su definición a partir de la bilineabilidad de $\displaystyle \beta  $. Finalmente, queda ver que $\displaystyle \overline{\beta } $ es no degenerada. Si $\displaystyle \vec{x} + \rad\left(\beta \right) \in \rad\left(\overline{\beta } \right) $, tenemos que $\displaystyle \forall \vec{y}\in V $, 
\[ \beta\left(\vec{x} + \rad\left(\beta \right), \vec{y} + \rad\left(\beta \right)\right) =0 \Rightarrow \beta\left(\vec{x}, \vec{y}\right) = 0, \; \forall \vec{y} \in V .\]
Entonces, tenemos que $\displaystyle \vec{x} \in \rad\left(\beta \right) $, por lo que $\displaystyle \vec{x} + \rad\left(\beta \right) = \vec{0} + \rad\left(\beta\right) $.
\end{proof}
Supongamos que $\displaystyle \rad\left(\beta \right) \neq \left\{ \vec{0}\right\}  $. Sea $\displaystyle L \in \mathcal{L}\left(V\right) $ tal que $\displaystyle \rad\left(\beta \right)\oplus L = V $. Consideremos la aplicación lineal 
\[
\begin{split}
	p : L \to & V/\rad\left(\beta \right) \\
	\vec{x} \to & \vec{x} + \rad\left(\beta \right).
\end{split}
\]
Tenemos que si $\displaystyle \vec{x} \in \Ker\left(p\right) $, entonces $\displaystyle \vec{x} + \rad\left(\beta \right) = \vec{0} + \rad\left(\beta \right) $, por lo que $\displaystyle \vec{x} = \vec{x}- \vec{0} \in L \cap \rad\left(\beta \right) = \left\{ \vec{0}\right\}  $. Entonces, $\displaystyle p $ es un isomorfismo (es trivial que era sobreyectiva).
Tenemos que 
\[ \overline{\beta }\left(p\left(\vec{x}\right), p\left(\vec{y}\right)\right) = \beta\left(\vec{x}, \vec{y}\right) = \beta_{L}\left(\vec{x}, \vec{y}\right) ,\]
donde $\displaystyle \beta_{L} = \beta|_{L \times L} $. Es decir, $\displaystyle \beta_{L} $ es no degenerada. Si $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n-r}\right\}  $ es base de $\displaystyle L $ y $\displaystyle \left\{ \vec{u}_{n-r+1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle \rad\left(\beta \right) $. 
Entonces tenemos que la matriz de $\displaystyle \beta  $ respecto de esta base será:
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta \right) = \begin{pmatrix} \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta _{L}\right) & 0 \\
0 & 0\end{pmatrix} .\]
Sea $\displaystyle L \in \mathcal{L}\left(V\right) $ y sea $\displaystyle \beta_{L} = \beta|_{L \times L} $. Tenemos que 
\[ \rad\left(\beta_{L}\right) = \left\{ \vec{x} \in L \; : \; \beta_{L}\left(\vec{x}, \vec{y}\right) = 0, \; \forall \vec{y} \in L\right\} = L \cap L^{\perp }_{\beta} .\]
Esto será no degenerado cuando $\displaystyle L \cap L^{\perp }_{\beta }= L $.
\begin{observation}
\normalfont De esto podmos concluir que se pueden tener formas bilineales degeneradas que su restricción a un subespacio vectorial sea no degenerada, así como una forma bilineal no degenerada cuya restricción a un subespacio vectorial sea degenerada.
\end{observation}
\begin{eg}
\normalfont Consideremos la forma no degenerada
\[\beta \to \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} .\]
Tenemos que su restricción al suespacio $\displaystyle L\left( \left\{ \left(1,0\right)\right\} \right) $ es degenerada:
\[ \beta_{L \left( \left\{ \left(1,0\right)\right\} \right)} \to \begin{pmatrix} 0 \end{pmatrix} .\]
\end{eg}
Sea $\displaystyle \beta  $ una forma bilineal simétrica. Se define una aplicación
\[
\begin{split}
	\theta : V & \to V^{*} \\
	\vec{x} & \to \theta\left(\vec{x}\right) = \beta_{\vec{x}}.
\end{split}
\]
Donde, $\displaystyle \beta_{\vec{x}} : V \to \K  $ tal que $\displaystyle \vec{y} \to \beta_{\vec{x}}\left(\vec{y}\right) = \beta\left(\vec{x}, \vec{y}\right) $. Tenemos que $\displaystyle \beta_{\vec{x}} $ es lineal para $\displaystyle \forall \vec{x} \in V $. En efecto, si $\displaystyle \vec{y}, \vec{z} \in V $,
\[ \beta_{x}\left(\vec{y} + \vec{z}\right) = \beta\left(\vec{x}, \vec{y} + \vec{z}\right) = \beta\left(\vec{x}, \vec{y}\right) + \beta\left(\vec{x}, \vec{z}\right) = \beta_{\vec{x}}\left(\vec{y}\right) + \beta_{\vec{x}}\left(\vec{z}\right) .\]
Por tanto, tenemos que $\displaystyle \forall \vec{x} \in V $, $\displaystyle \beta_{\vec{x}} \in V^{*} $. Ahora vamos a ver que $\displaystyle \theta  $ es lineal. En efecto, $\displaystyle \forall \vec{z} \in V $  
\[\theta\left(\vec{x} + \vec{y}\right) = \beta_{\vec{x} + \vec{y}} .\]
\[ \beta_{\vec{x} + \vec{y}}\left(\vec{z}\right) = \beta\left(\vec{x} + \vec{y}, \vec{z}\right) = \beta_{\vec{x}}\left(\vec{z}\right) + \beta_{\vec{y}}\left(\vec{z}\right) = \theta\left(\vec{x}\right)(\vec{z}) + \theta\left(\vec{y}\right)\left(\vec{z}\right) .\]
Análogamente, si $\displaystyle a \in \K $ y $\displaystyle \vec{x} \in V $, tenemos que $\displaystyle \forall \vec{y} \in V $, $\displaystyle \theta\left(a\vec{x}\right) = \beta_{a\vec{x}} $, así
\[ \beta_{a\vec{x}}\left(\vec{y}\right) = \beta\left(a\vec{x}, \vec{y}\right) = a\beta\left(\vec{x}, \vec{y}\right) = a\beta_{\vec{x}}\left(\vec{y}\right) = a\theta\left(\vec{x}\right)\left(\vec{y}\right).\]
Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $ y $\displaystyle \left\{ \omega^{1}, \ldots, \omega^{n}\right\}  $ su dual. Vamos a calcular la matriz de 
\[
\begin{split}
	\theta : V _{ \left\{ \vec{u}_{i}\right\} } \to & V^{*}_{ \left\{ \omega^{i}\right\} } .
\end{split}
\]
Tenemos que $\displaystyle \beta_{\vec{u}_{i}} \in V^{*} $, $\displaystyle \forall i = 1, \ldots, n $. Si estudiamos la columna $\displaystyle i $ de $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \omega^{i}\right\} }\left(\theta\right) $ deducimos que 
\[ \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \omega^{i}\right\} }\left(\theta \right) = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta\right) .\]
En efecto, $\displaystyle \beta_{\vec{u}_{i}}\left(\vec{u}_{j}\right) = \beta\left(\vec{u}_{i}, \vec{u}_{j}\right) $.
Tenemos que 
\[ \Ker\left(\theta \right) = \left\{ \vec{x} \in V \; : \; \beta_{\vec{x}} = 0\right\} = \left\{ \vec{x} \in V \; : \; \beta_{\vec{x}}\left(\vec{y}\right) = 0, \forall \vec{y}\in V\right\} = \rad\left(\beta\right) .\]
\begin{fdefinition}[Rango]
\normalfont Llamaremos \textbf{rango} de $\displaystyle \beta  $ al rango de $\displaystyle \theta $. 
\end{fdefinition}
\begin{observation}
\normalfont Tenemos que $\displaystyle \ran\left(\beta \right) = n - \dim\left(\rad\left(\beta \right)\right) $.
\end{observation}
\begin{observation}
	\normalfont Tenemos que si $\displaystyle \beta  $ es no degenerada, la aplicación $\displaystyle \theta $ es un isomorfismo. En efecto, tendríamos que $\displaystyle \Ker\left(\theta \right) = \rad\left(\beta \right) = \left\{ \vec{0}\right\}  $, por lo que $\displaystyle \theta  $ es inyectiva. Dado que $\displaystyle \dim\left(V\right) = \dim\left(V^{*}\right) $, por ser $\displaystyle \theta  $ inyectiva, tenemos que es isomorfismo.
\end{observation}
\begin{ftheorem}[]
	\normalfont Sea $\displaystyle L \in \mathcal{L}\left(V\right) $, $\displaystyle \beta_{L} $ es no degenerada, entonces $\displaystyle V = L \oplus L^{\perp }_{\beta } $. En particular, si $\displaystyle \vec{x} \in V $ es no isótropo, $\displaystyle \left\{ \vec{x}\right\} ^{\perp }_{\beta } $ es un hiperplano vectorial.
\end{ftheorem}
\begin{proof}
	Dado que $\displaystyle \beta_{L} $ es no degenerada, tenemos que $\displaystyle \rad\left(\beta_{L}\right) = L\cap L^{\perp}_{\beta} = \left\{ \vec{0}\right\} $. \\
	Ahora, sea $\displaystyle \vec{x} \in V $. Tenemos que $\displaystyle \beta_{\vec{x}}|_{L} \in L^{*}$. Dado que $\displaystyle \beta_{L} $ es no degenerada, tenemos que existe $\displaystyle \vec{z} \in L $ tal que $\displaystyle \beta_{L}\left(\vec{z}, \vec{y}\right) = \beta_{\vec{x}}\left(\vec{y}\right) $, $\displaystyle \forall \vec{y} \in L $. Es decir, 
	\[ \forall \vec{y} \in L, \; \beta\left(\vec{z}, \vec{y}\right) = \beta\left(\vec{x}, \vec{y}\right) \Rightarrow \beta\left(\vec{z}-\vec{x}, \vec{y}\right) = 0 \Rightarrow \vec{z}-\vec{x} \in L^{\perp }_{\beta} .\]
	Así, tenemos que 
	\[ \vec{x} = \underbrace{\vec{z} }_{\in L} + \underbrace{\vec{x}-\vec{z}}_{\in L^{\perp}_{\beta}} .\]
Así, queda demostrado que $\displaystyle V = L \oplus L^{\perp }_{\beta } $. \\ 
Finalmente, si $\displaystyle \vec{x} \in V $ es no isótropo, tenemos que $\displaystyle \dim L\left( \left\{ \vec{x}\right\} \right) = 1 $ y, por consecuencia de lo visto anteriormente, $\displaystyle \dim\left(L\left( \left\{ \vec{x}\right\} \right)\right)^{\perp }_{\beta} = \dim V - 1 $.
\end{proof}
\begin{ftheorem}[]
\normalfont Sea $\displaystyle \beta  $ una forma bilineal no degenerada y sea $\displaystyle L \in \mathcal{L}\left(V\right) $. Entonces, $\displaystyle \dim\left(L^{\perp }_{\beta }\right) = \dim \left(V\right) - \dim \left(L\right) $ y $\displaystyle \left(L^{\perp }_{\beta }\right)^{\perp }_{\beta } = L $.
\end{ftheorem}
\begin{proof}
Por ser $\displaystyle \beta  $ no degenerada, tenemos que $\displaystyle \theta  $ es un isomorfismo. Sea $\displaystyle \vec{x} \in L^{\perp }_{\beta } $, entonces $\displaystyle \theta\left(\vec{x}\right) = \beta_{\vec{x}} = 0 $. Así, 
\[ \theta\left(L^{\perp }_{\beta }\right) \subset L^{\perp } = \left\{ \lambda \in V^{*} \; : \; \lambda\left(\vec{y}\right) = 0, \forall \vec{y} \in L\right\}  .\]
Si $\displaystyle \lambda \in L^{*} $, existe $\displaystyle \vec{z} \in V $ tal que $\displaystyle \lambda = \beta_{\vec{z}} $. Así, $\displaystyle \forall \vec{y}\in V $, tenemos que 
\[ \beta_{\vec{z}} = \beta\left(\vec{z}, \vec{y}\right) = \lambda\left(\vec{y}\right) = 0, \; \vec{z} \in L.\]
Así, tenemos que $\displaystyle \theta\left(L^{\perp }_{\beta }\right) = L^{\perp } $, por lo que 
\[\dim\left(L^{\perp }_{\beta }\right) = \dim\left(\theta\left(L^{\perp }_{\beta }\right)\right) = \dim\left(L^{\perp }\right) = \dim\left(V\right)-\dim\left(L\right) .\]
Si $\displaystyle \vec{x} \in L $, $\displaystyle \forall \vec{y} \in L^{\perp }_{\beta } $ tenemos que $\displaystyle \beta\left(\vec{x}, \vec{y}\right) = 0 $, por lo que $\displaystyle L \subset \left(L^{\perp }_{\beta }\right)^{\perp }_{\beta } $. Aplicando lo demostrado anteriormente, tenemos que
\[\dim\left(L^{\perp }_{\beta }\right)^{\perp }_{\beta } = \dim\left(V\right) - \dim\left(L^{\perp }_{\beta}\right) = \dim \left(L\right) .\]
Así, queda demostrado que $\displaystyle L = \left(L^{\perp }_{\beta }\right)^{\perp }_{\beta } $.
\end{proof}
\begin{fcolorary}[]
\normalfont $\displaystyle \beta_{L} $ es no degenerada si y solo si $\displaystyle \beta _{L^{\perp }_{\beta }} $ es no degenerada.
\end{fcolorary}
\begin{proof}
Tenemos que $\displaystyle \rad\left(\beta_{L}\right) = L\cap L^{\perp }_{\beta } $. De esta forma,
\[ \rad\left(\beta _{L^{\perp }_{\beta }}\right) = L^{\perp }_{\beta }\cap\left(L^{\perp }_{\beta }\right)^{\perp }_{\beta } = \rad\left(\beta_{L}\right) .\]
\end{proof}
\begin{fdefinition}[]
	\normalfont Una base $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ de $\displaystyle V $ es \textbf{ortogonal} para $\displaystyle \beta  $ si para $\displaystyle i \neq j $, $\displaystyle \beta\left(\vec{u}_{i}, \vec{u}_{j}\right) = 0 $.
\end{fdefinition}
\begin{ftheorem}[]
	\normalfont Si $\displaystyle \beta  $ es una forma bilineal simétrica, entonces existe $\displaystyle B =\left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $ ortogonal para $\displaystyle \beta  $. Además, $\displaystyle \rad\left(\beta \right) = L\left( \left\{ \vec{u}_{i} \in B \; : \; \beta\left(\vec{u}_{i}, \vec{u}_{i}\right)=0\right\} \right) $. \footnote{Es fundamental que estemos trabajando con un cuerpo de característica distinta de 2.} 
\end{ftheorem}
\begin{proof}
	\begin{description}
	\item[(i)] Si $\displaystyle n = 1 $, no hay nada que demostrar. Si $\displaystyle n = 2 $ y $\displaystyle \beta \neq 0 $ (no todos los vectores de la base son isótropos), cogemos $\displaystyle \vec{u}_{1} \in V $ tal que $\displaystyle \beta\left(\vec{u}_{1}, \vec{u}_{1}\right) \neq 0 $. Entonces tenemos que $\displaystyle \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta}$ es una recta tal que $\displaystyle L\left( \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta }\right)\oplus L\left( \left\{ \vec{u}_{i}\right\} \right) = V $.
	\item[(ii)] Supongamos que es cierto para $\displaystyle n - 1 $. Sea $\displaystyle \beta \neq 0 $ y $\displaystyle \vec{u}_{1} \in V $ tal que $\displaystyle \beta\left(\vec{u}_{1}, \vec{u}_{1}\right) \neq 0 $. Tenemos que $\displaystyle \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta } $ es un hiperplano vectorial de $\displaystyle V $. Por hipótesis de inducción, tenemos que existe $\displaystyle \left\{ \vec{u}_{2}, \ldots, \vec{u}_{n}\right\}  $ base ortogonal para $\displaystyle \beta_{ \left\{ \vec{u}_{1}\right\} ^{\perp }_\beta} $.
		Así, $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ es una base de $\displaystyle V $ ortogonal para $\displaystyle \beta  $. En efecto, para $\displaystyle i \geq 2 $,
		\[\beta\left(\vec{u}_{1}, \vec{u}_{i}\right) = 0 .\]
		Si $\displaystyle i \neq j$ ($\displaystyle i,j \geq 2 $), tenemos que $\displaystyle \beta\left(\vec{u}_{i}, \vec{u}_{j}\right) = \beta_{ \left\{ \vec{u}_{i}\right\} ^{\perp }_{\beta }} = 0 $.	
	\end{description}
	Ahora vamos a ver que $\displaystyle \rad\left(\beta \right)= L\left( \left\{ \vec{u}_{i} \in B \; : \; \beta\left(\vec{u}_{i}, \vec{u}_{i}\right)=0\right\} \right) $. Si $\displaystyle \vec{x} \in \rad\left(\beta \right) $, tenemos que si $\displaystyle \vec{u}_{i} \in B $ no es isótropo, $\displaystyle \beta\left(\vec{x}, \vec{u}_{i}\right) = x^{i}\beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0 $, por lo que $\displaystyle x^{i} = 0$.
	Así, tenemos que los únicos coeficientes que no se anulan son aquellos que corresponden a los vectores isótropos de la base de $\displaystyle B $. Así, $\displaystyle \vec{x} \in L\left( \left\{ \vec{u}_{i} \in B \; : \; \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0\right\} \right) $. Recíprocamente, si $\displaystyle \vec{x} \in L\left( \left\{ \vec{u}_{i} \in B \; : \; \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0\right\} \right) $ tenemos que si $\displaystyle \vec{y} \in V $ 
	\[\beta\left(\vec{x},\vec{y}\right) = x^{1}y^{1}\beta\left(\vec{u}_{1},\vec{u}_{1}\right) + \cdots + x^{n}y^{n}\beta\left(\vec{u}_{n}, \vec{u}_{n}\right) .\]
	Dado que $\displaystyle \vec{x} $ es una combinación lineal de los vectores isótropos de $\displaystyle B $, los coeficientes asociados a vectores no isótropos son nulos y la expresión que queda se anulará puesto que consistirá de $\displaystyle \beta  $ actuando sobre vectores isótropos, por lo que $\displaystyle \vec{x } \in \rad\left(\beta \right) $.
\end{proof}
\begin{observation}
	\normalfont Otra forma de enunciar este teorema es decir que toda matriz simétrica es congruente con una matriz diagonal. En efecto si $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ es una base ortogonal de $\displaystyle V $, la matriz de $\displaystyle \beta  $ en esta base será:
\[ \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} }\left(\beta \right) = \begin{pmatrix} \beta\left(\vec{u}_{1}, \vec{u}_{1}\right) & 0 & \cdots & 0\\
0 & \beta\left(\vec{u}_{2}, \vec{u}_{2}\right) & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \beta\left(\vec{u}_{n}, \vec{u}_{n}\right)\end{pmatrix} .\]
\end{observation}
\begin{observation}
\normalfont Esto solo funciona en un cuerpo de característica distinta de dos.
\end{observation}
\begin{eg}
\normalfont Aplicamos la demostración del teorema anterior para calcular bases ortogonales. Consideremos en $\displaystyle \R^{3} $ la forma bilineal $\displaystyle \beta $ cuya matriz sea
\[ A = \begin{pmatrix} 2 & 1 & 0 \\
1 & - 1 & 1 \\
0 & 1 & 0\end{pmatrix} .\]
Tenemos que $\displaystyle \det\left(A\right) = -2 $, por lo que $\displaystyle \beta  $ es no degenerada y ningún elemento de la base ortogonal va a ser isótropo. Así, podemos considererar $\displaystyle \vec{u}_{1} = \left(1,0,0\right) $ el primer vector de la base que buscamos. En efecto, tenemos que $\displaystyle \beta\left(\vec{u}_{1}, \vec{u}_{1}\right) = 2 $. A continuación, los otros dos vectores que buscamos formarán la base de $\displaystyle \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta } $. Sacamos las ecuaciones de este subespacio:
	\[ \begin{pmatrix} 1 & 0 & 0 \end{pmatrix} A \begin{pmatrix} x \\ y \\ z \end{pmatrix} = 0 .\]
	Así, nos sale que la ecuación de $\displaystyle \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta } $ será $\displaystyle 2x + y = 0 $. Por tanto, si $\displaystyle \vec{x} = \left(x,y,z\right)\in \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta } $, tenemos que
	\[ \vec{x} = \left(x,y,z\right) = \left(x,-2x,z\right) = x\left(1,-2,0\right) + z\left(0,0,1\right) .\]
	Así, tenemos que la base ortogonal que buscamos es $\displaystyle \left\{ \left(1,0,0\right), \left(1,-2,0\right),\left(0,0,1\right)\right\}  $.
\end{eg}
\begin{eg}
\normalfont Consideremos la forma bilineal simétrica dada por la matriz 
\[ A = \begin{pmatrix} 0 & 1 & 0 \\
1 & 0 & 1 \\
0 & 1 & 0\end{pmatrix} .\]
No podemos usar la base canónica como en el anterior, puesto que en este caso el determinante es 0. Buscamos una base de $\displaystyle \rad\left(\beta \right) $: 
	\[ \begin{pmatrix} x & y & z \end{pmatrix} A \begin{pmatrix} x \\ y \\ z \end{pmatrix} = 0 .\]
Es decir, 
\[ xy + y\left(x + z\right) + zy = 0 \iff y\left(x + z\right) = -y\left(x+z\right) \Rightarrow y\left(x + z\right) = 0 .\]
Tenemos que $\displaystyle y = 0 $ o $\displaystyle x + z = 0$. Entre los tres casos, podemos reunir estas posibles bases:
\[ \left\{ \left(1,0,0\right), \left(0,0,1\right)\right\}, \; \left\{ \left(1,0,-1\right)\right\} , \; \left\{ \left(1,0,-1\right),\left(0,1,0\right)\right\}  .\]
Cogemos $\displaystyle \vec{u}_{1} = \left(1,1,1\right) $, que sabemos que no pertenece a ninguno de los subespacios anteriores, por lo que no es isótropo. Ahora, encontramos una base de $\displaystyle \left\{ \vec{u}_{1}\right\} ^{\perp }_{\beta } $:
\[ \begin{pmatrix} 1 & 1 & 1 \end{pmatrix}A\begin{pmatrix} x \\ y \\ z \end{pmatrix} = 0 \Rightarrow x + 2y +z = 0 .\]
Con esta ecuación, tenemos que una base será $\displaystyle \left\{ \left(1,0,-1\right), \left(0,1,-2\right)\right\}  $. Así, la base ortogonal de $\displaystyle V $ que buscamos era $\displaystyle \left\{ \left(1,1,1\right), \left(1,0,-1\right), \left(0,1,-2\right)\right\}  $.
\end{eg}
\noindent\rule{15.5cm}{0.4pt}
Sea $\displaystyle \K = \C $ y sea $\displaystyle \beta : V \times V \to \C  $ bilineal simétrica. Tenemos que existe una base ortogonal $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{r}, \vec{u}_{r+1}, \ldots, \vec{u}_{n}\right\}  $ tal que 
\[ \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = a_{i} \neq 0, \; i \leq r .\]
\[ \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0, \; r + 1 \leq i \leq n .\]
Sea $\displaystyle b_{i} \neq 0 $ una raíz cuadrada de $\displaystyle a_{i} $ para $\displaystyle i = 1, \ldots, n $, definimos
\[ \left\{ \vec{v}_{1} = \frac{1}{b_{1}}\vec{u}_{1}, \ldots, \vec{v}_{r} = \frac{1}{b_{r}}\vec{u}_{r}, \vec{v}_{r+1} = \vec{u}_{r+1}, \ldots, \vec{v}_{n} = \vec{u}_{n} \right\}  .\]
Tenemos que 
\[
\begin{split}
&\beta\left(\vec{v}_{i}, \vec{v}_{j}\right) = 0, \; i \neq j \\
&\beta\left(\vec{v}_{i}, \vec{v}_{i}\right) = \beta\left(\lambda_{i}\vec{u}_{i},\lambda_{i}\vec{u}_{i} \right) = \frac{1}{b_{i}^{2}}a_{i} = 1, \; i = 1, \ldots, r.
\end{split}
\]
\begin{observation}
\normalfont Así, tenemos que existe $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{n}\right\}  $ base ortogonal tal que $\displaystyle \beta\left(\vec{v}_{i}, \vec{v}_{i}\right) = 1 $ si $\displaystyle i = 1, \ldots, r $ y $\displaystyle \beta\left(\vec{v}_{i}, \vec{v}_{i}\right)=0 $ si $\displaystyle i = r+1, \ldots, n $.
\end{observation}
Ahora, consideremos el caso $\displaystyle \K = \R $ y la forma $\displaystyle \beta : V \times V \to \R $ bilineal simétrica. Tenemos que existe $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ una base ortogonal tal que:
\[
\begin{split}
& \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = a_{i} > 0, \; i = 1, \ldots, p \\
& \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = b_{i} < 0, \; i = p + 1, \ldots, q \\
& \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0, \; i \geq p + q + 1.
\end{split}
\]
Sea $\displaystyle c_{i} $ una raíz cuadrada de $\displaystyle a_{i} $ para $\displaystyle i = 1, \ldots, p $ y sea $\displaystyle d _{i} $ una raíz cuadrada de $\displaystyle - b_{i} $ para $\displaystyle i = p + 1, \ldots, p + q $. Definimos
\[ \left\{ \vec{v}_{1} = \frac{1}{c_{1}}\vec{u}_{1}, \ldots, \vec{v}_{p + 1} = \frac{1}{d _{p+ 1}}\vec{u}_{ p + 1}, \ldots, \vec{v}_{n} = \vec{u}_{n}\right\}  .\]
Así, tenemos que 
\[ \beta\left(\vec{v}_{i}, \vec{v}_{j}\right) = \beta\left(\lambda_{i}\vec{u}_{i}, \lambda_{j}\vec{u}_{j}\right) = \lambda_{i}\lambda_{j}\beta\left(\vec{u}_{i}, \vec{u}_{j}\right) = 0, \; i \neq j .\]
Por tanto, se trata de una base ortogonal. Si $\displaystyle i = 1, \ldots, p $:
\[\beta\left(\vec{v}_{i},\vec{v}_{i}\right) = \beta\left(\frac{1}{c_{i}}\vec{u}_{i}, \frac{1}{c_{i}}\vec{u}_{i}\right) = \left(\frac{1}{c_{i}}\right)^{2}\beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 1 .\]
Similarmente, si $\displaystyle i = p + 1, \ldots, p + q $:
\[ \beta\left(\vec{v}_{i}, \vec{v}_{i}\right) = \left(\frac{1}{d _{i}}\right)^{2}\beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = -1 .\]
Finalmente, si $\displaystyle i = p + q + 1, \ldots, n $, tenemos que $\displaystyle \beta\left(\vec{v}_{i}, \vec{v}_{i}\right) = \beta\left(\vec{u}_{i}, \vec{u}_{i}\right) = 0 $.
\begin{observation}
	\normalfont Así, tenemos que existe $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{n}\right\}  $ base ortogonal tal que $\displaystyle \beta\left(\vec{v}_{i}, \vec{v}_{i}\right) \in \left\{ 0, 1, - 1\right\}  $.
\end{observation}
\begin{fdefinition}[]
\normalfont Sea $\displaystyle \beta  $ una forma bilineal simétrica real.
\begin{description}
\item[(a)] Se dice que $\displaystyle \beta  $ es \textbf{definida positiva} si $\displaystyle \beta\left(\vec{x}, \vec{x}\right) > 0 $, $\displaystyle \forall \vec{x} \in V $, y $\displaystyle \beta\left(\vec{x}, \vec{x}\right) = 0 \iff \vec{x} = \vec{0} $.
\item[(b)] Se dice que $\displaystyle \beta  $ es \textbf{definida negativa} si $\displaystyle \beta\left(\vec{x}, \vec{x}\right) < 0 $, $\displaystyle \forall \vec{x} \in V $, y $\displaystyle \beta\left(\vec{x}, \vec{x}\right) = 0 \iff \vec{x} = \vec{0} $.
\item[(c)] Se dice que $\displaystyle \beta  $ es \textbf{positiva} si $\displaystyle \beta\left(\vec{x}, \vec{x}\right) \geq 0 $, $\displaystyle \forall \vec{x} \in V $.
\item[(d)] Se dice que $\displaystyle \beta  $ es \textbf{negativa} si $\displaystyle \beta\left(\vec{x}, \vec{x}\right) \leq 0 $, $\displaystyle \forall \vec{x} \in V $.
\end{description}
\end{fdefinition}
Consideremos $\displaystyle L_{1} = L\left( \left\{ \vec{u}_{1}, \ldots, \vec{u}_{p}\right\} \right) $. Así, tenemos que $\displaystyle \forall \vec{x} \in L_{1} $,
\[ \vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{p}\vec{u}_{p} .\]
Así, tenemos que 
\[ \beta_{L_{1}}\left(\vec{x}, \vec{x}\right) = \beta\left(\vec{x}, \vec{x}\right) = \left(x^{1}\right)^{2}\beta\left(\vec{u}_{1},\vec{u}_{1}\right) + \cdots + \left(x^{p}\right)^{2}\beta\left(\vec{u}_{p}, \vec{u}_{p}\right) \geq 0 .\]
Además, tenemos que $\displaystyle \beta_{L_{1}}\left(\vec{x}, \vec{x}\right) = 0 $ si y solo si $\displaystyle \vec{x} = \vec{0} $. Es decir, $\displaystyle \beta_{L_{1}} $ es un subespacio definido positivo. 
\begin{observation}
\normalfont Si $\displaystyle L \in \mathcal{L}\left(V\right) $ tal que $\displaystyle L_{1} \subsetneq L $, entonces $\displaystyle \beta_{L} $ no es definida positiva y $\displaystyle L_{1} $ es un espacio definido positivo maximal. En efecto, tenemos que $\displaystyle \exists \vec{x} \in L $ con $\displaystyle \vec{x} \not\in L_{1} $. Entonces, tenemos que este vector se puede expresar de la forma
\[\vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{p + q}\vec{u}_{p + q} + 0 \vec{u}_{p + q +1} + \cdots + 0\vec{u}_{n} .\]
Entonces, tenemos que $\displaystyle \vec{x}-\left(x^{1}\vec{u}_{1} + \cdots + x^{p}\vec{u}_{p}\right) \in L $. Consideremos el vector
\[ \vec{y} = x^{p +1}\vec{u}_{p +1} + \cdots + x^{n}\vec{u}_{n} \in L .\]
Tenemos que 
\[\beta\left(\vec{y},\vec{y}\right) = \left(x^{p + 1}\right)^{2}b_{p + 1} + \cdots + \left(x^{p + q}\right)^{2}b_{p + q} \leq 0 .\]
\end{observation}
\begin{fdefinition}[]
\normalfont Si $\displaystyle L \in \mathcal{L}\left(V\right) $, se dice que $\displaystyle L $ es \textbf{definido positivo maximal} cuando no está contenido en otro subespacio definido positivo.
\end{fdefinition}
\begin{ftheorem}[]
\normalfont Todos los subespacios definidos positivos maximales tienen la misma dimensión.
\end{ftheorem}
\begin{proof}
	Sean $\displaystyle L_{1}, L_{2} \in \mathcal{L}\left(V\right) $ tales que $\displaystyle \beta_{L_{1}} $ y $\displaystyle \beta_{L_{2}} $ sean definidos positivos maximales. Tenemos que $\displaystyle L^{\perp }_{1,\beta} $ es negativo. En efecto, si $\displaystyle \vec{x} \in L_{1\beta }^{\perp} $ y $\displaystyle \beta\left(\vec{x}, \vec{x}\right) > 0 $ tenemos que $\displaystyle L_{1} \subset L_{1} + L\left( \left\{ \vec{x}\right\} \right) \in \mathcal{L}\left(V\right) $, lo cual es absurdo porque hemos dicho que $\displaystyle L_{1} $ es definido positivo maximal. También tenemos que $\displaystyle L_{1\beta }^{\perp } \cap L_{2} = \left\{ \vec{0}\right\}  $. Por tanto, debe ser que $\displaystyle F_{2} $ está contenido en alún suplementario de $\displaystyle L^{\perp }_{1\beta} $. Por tanto,
	\[ \dim L_{2} \leq \dim\left(V\right)-\dim\left(L^{\perp }_{1\beta }\right) = \dim\left(L_{1}\right)  .\]
	Análogamente, como $\displaystyle L_{1} \cap L^{\perp }_{2\beta } = \left\{ \vec{0}\right\}  $,
	\[ \dim\left(L_{1}\right) \leq \dim\left(V\right) - \dim\left(L^{\perp }_{2\beta }\right) = \dim\left(L_{2}\right) .\]
Por tanto, debe ser que $\displaystyle \dim\left(L_{1}\right) = \dim\left(L_{2}\right) $.
\end{proof}
\begin{fdefinition}[Índice y coíndice]
\normalfont Se llama \textbf{coíndice} de $\displaystyle \beta  $ al número de vectores $\displaystyle \vec{x} $ de una base ortogonal tales que $\displaystyle \beta\left(\vec{x}, \vec{x}\right) > 0 $. Se llama \textbf{índice} de $\displaystyle \beta  $ al número de vectores $\displaystyle \vec{x} $ de una base ortogonal tales que $\displaystyle \beta\left(\vec{x}, \vec{x}\right) < 0 $.
\end{fdefinition}
\begin{observation}
	\normalfont Si $\displaystyle \beta  $ es una forma bilineal simétrica de coíndice $\displaystyle p $ e índice $\displaystyle q $. Tenemos que existe $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{p}, \ldots, \vec{v}_{p +q}, \ldots, \vec{v}_{n}\right\}  $ tal que 
	\[\mathcal{M}_{ \left\{ \vec{v}_{i}\right\} }\left(\beta \right) = \begin{pmatrix} I_{p \times p} & 0 & 0 \\ 0 & - I_{q \times q} & 0\end{pmatrix} .\]
\end{observation}
Entonces, tenemos que 
\[
\begin{split}
	\beta\left(\vec{x}, \vec{y}\right)= & \beta\left(\sum^{n}_{i = 1}x^{i}\vec{v}_{i}, \sum^{n}_{i = 1}y^{i}\vec{v}_{i}\right) = \begin{pmatrix} x^{1} & \cdots & x^{n} \end{pmatrix} \mathcal{M}_{ \left\{ \vec{v}_{i}\right\} }\left(\beta \right) \begin{pmatrix} y^{1} \\ \vdots \\ y^{n} \end{pmatrix} \\
	= & x^{1}y^{1} + \cdots + x^{p}y^{p} - x^{p +1}y^{p +1} - \cdots - x^{p + q}y ^{p + q}.
\end{split}
\]
Una consecuencia de esto es que 
\[\beta\left(\vec{x}, \vec{x}\right) = \left(x^{1}\right)^{2} + \cdots + \left(x^{p}\right)^{2} - \left(x^{p + 1}\right)^{2} - \cdots - \left(x^{p + q}\right)^{2} .\]

