\chapter{Matrices}
\section{Matriz asociada a una aplicación lineal}
\begin{fdefinition}[Matriz]
\normalfont Una matriz $\displaystyle A $ con coeficientes $\displaystyle \K $ de $\displaystyle m \in \N $ filas y $\displaystyle n \in \N $ columnas es una tabla 
\[A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} .\]
con $\displaystyle a^{j}_{i} \in \K, \; \forall i = 1, \ldots, n, \; \forall j= 1, \ldots, m  $.
\end{fdefinition}

Sean $\displaystyle V $, $\displaystyle V' $ espacios vectoriales con $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ y $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ bases de $\displaystyle V $ y $\displaystyle V' $, respectivamente. Si $\displaystyle f \in \Hom\left(V, V'\right) $, entonces $\displaystyle f\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right) \in V' $ y existen $\displaystyle a^{j}_{i} \in \K $ tales que 
\[
\begin{split}
& f\left(\vec{u}_{1}\right) = a^{1}_{1}\vec{v}_{1} + \cdots + a^{m}_{1}\vec{v}_{m} \\
& f\left(\vec{u}_{2}\right) = a^{1}_{2}\vec{v}_{1} + \cdots + a^{m}_{2}\vec{v}_{m} \\
& \vdots \\
& f\left(\vec{u}_{n}\right) = a^{1}_{n}\vec{v}_{1} + \cdots + a^{m}_{n}\vec{v}_{m}.
\end{split}
\]
Entonces, a $\displaystyle f $  le podemos asignar la matriz $\displaystyle A $ tal que 
\[f \to A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix}.\]
\begin{ftheorem}[]
\normalfont 
Sea $\displaystyle \mathcal{M}_{m\times n}\left(\K\right) = \left\{ A \; : \; A \; \text{matriz} \; m\times n\right\}  $. Definimos 
\[
\begin{split}
	\mathcal{M}_{\{\vec{u}_{i}\}\{\vec{v}_{j}\}} : \Hom\left(V, V'\right) & \to \mathcal{M}_{m \times n}\left(\K\right) \\ 
 f & \to A.
\end{split}
\]
Tenemos que $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} } $ es biyectiva.
\end{ftheorem}

\begin{proof}
Tenemos que si $\displaystyle A \in \mathcal{M}_{m\times n}\left(\K\right) $, entonces existen unos únicos $\displaystyle \vec{w}_{1}, \ldots, \vec{w}_{n}\in V' $ tales que 
\[
\begin{split}
	\vec{w}_{1} = & a^{1}_{1}\vec{v}_{1} + \cdots + a^{n}_{1}\vec{v}_{m} \\
		      & \vdots \\
	\vec{w}_{n}= & a^{1}_{m}\vec{v}_{1} + \cdots + a^{n}_{m}\vec{v}_{m} .
\end{split}
\]
Para estos existe una única $\displaystyle f \in \Hom\left(V, V'\right) $ tal que $\displaystyle f\left(\vec{u}_{1}\right) = \vec{w}_{1} $, \ldots, $\displaystyle f\left(\vec{u}_{n}\right) = \vec{w}_{n} $.
\end{proof}

\begin{ftheorem}[]
	\normalfont $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} } $ es isomorfismo.
\end{ftheorem}

\begin{proof}
Sabemos que es biyectiva, ahora tenemos que ver que es una aplicación lineal. Para ello, definimos la suma de matrices de la siguiente forma: si $\displaystyle A, B \in \mathcal{M}_{m \times n}\left(\K\right) $,
\[ A + B = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(\mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\}}\left(A\right) + \mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\}}\left(B\right)\right).\]
Definimos el producto por escalares. Sea $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $ y $\displaystyle a \in \K $, 
\[a \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(a \cdot \mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(A\right)\right) .\]
La demostración se puede completar con la demostración del siguiente teorema.
\end{proof}
\begin{ftheorem}[]
\normalfont $\displaystyle \mathcal{M}_{m \times n}\left(\K\right) $ es un $\displaystyle \K $-espacio vectorial.
\end{ftheorem}

\begin{proof}
Tenemos las siguientes equivalencias
\[ A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} \iff f \in \Hom\left(V, V'\right) \quad \text{y} \quad B = \begin{pmatrix} 
	b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
	b^{m}_{1} & \cdots & b^{m}_{n}
\end{pmatrix} \iff g \in \Hom\left(V, V'\right).\]
Así, $\displaystyle A + B \in \mathcal{M}_{m \times n}\left(\K\right) \iff f + g \in \Hom\left(V, V'\right) $. Tenemos que 
\[
\begin{split}
	f\left(\vec{u}_{i}\right)  = & a^{1}_{i}\vec{v}_{i} + \cdots + a^{m}_{i}\vec{v}_{m}, \; \forall i = 1, \ldots, n \\
	g\left(\vec{u}_{i}\right)= & b^{1}_{i}\vec{v}_{i} + \cdots + b^{m}_{i}\vec{v}_{m}, \; \forall i = 1, \ldots, n .
\end{split}
\]
Entonces, tenemos que 
\[\left(f + g\right)\left(\vec{u}_{i}\right) = f\left(\vec{u}_{i}\right) + g\left(\vec{u}_{i}\right) = \left(a^{1}_{i}+b^{1}_{i}\right)\vec{v}_{1} + \cdots + \left(a^{m}_{i}+b^{m}_{i}\right)\vec{v}_{m}.\]
Es decir, 
\[A + B = \begin{pmatrix} a^{1}_{1} + b^{1}_{1} & \cdots & a^{1}_{n} + b^{1}_{n}\\
\vdots & & \vdots \\
a^{m}_{1}+b^{m}_{1} & \cdots & a^{m}_{n} + b^{m}_{n}\end{pmatrix} \in \mathcal{M}_{m \times n}\left(\K\right).\]
Hacemos lo mismo con el producto por escalares. Sea $\displaystyle A \in \mathcal{M}_{m\times n}\left(\K\right) $ y $\displaystyle a \in \K $, tenemos que $\displaystyle af \in \Hom\left(V, V'\right) \iff a \cdot A \in \mathcal{M}_{m\times n}\left(\K\right) $. Tenemos que 
\[af\left(\vec{u}_{i}\right) = a a^{1}_{i}\vec{v}_{1} + \cdots + a a^{m}_{i}\vec{v}_{m} .\]
Es decir, 
\[a \cdot A = \begin{pmatrix} a a^{1}_{1} & \cdots & a a^{1}_{n} \\
\vdots & & \vdots \\
a a ^{m}_{1} & \cdots & a a^{m}_{n}\end{pmatrix} .\]
Tenemos que existe la matriz 0, pues $\displaystyle 0 \in \Hom\left(V, V'\right) $, por lo que 
	\[0 = \begin{pmatrix} 0 & \cdots & 0 \\
	\vdots & & \vdots \\
0 & \cdots & 0\end{pmatrix} .\]
Similarmente, la matriz opuesta se puede definir así:
		\[-A = \left(-1\right) \cdot A = \begin{pmatrix} -a^{1}_{1} & \cdots & -a^{1}_{n} \\
		\vdots & & \vdots \\
	-a^{m}_{1} & \cdots & -a^{m}_{n}\end{pmatrix} .\]
\end{proof}

\section{Producto de matrices}

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $, $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ base de $\displaystyle V' $ y $\displaystyle \left\{ \vec{w}_{1}, \ldots, \vec{w}_{p}\right\}  $ base de $\displaystyle V'' $. Entonces, podemos encontrar aplicaciones lineales tales que 
\[ f \in \Hom\left(V, V'\right) \to A \in \mathcal{M}_{m\times n}\left(\K\right) \quad \text{y} \quad g \in \Hom\left(V', V''\right) \to B \in \mathcal{M}_{p\times m}\left(\K\right) .\]
Tenemos que $\displaystyle g \circ f \in \Hom\left(V, V''\right) $. Definimos el producto de matrices tal que
\[g \circ f \to B \cdot A \in \mathcal{M}_{p\times n}\left(\K\right) .\]
Es decir, 
\[B \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{w}_{k}\right\} }\left(g \circ f\right) .\]
Tenemos que la expresión de $\displaystyle A $ y $\displaystyle B $ será:
\[ A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} \quad \text{y} \quad B = \begin{pmatrix} 
	b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
	b^{m}_{1} & \cdots & b^{m}_{n}
\end{pmatrix}.\]
Entonces, $\displaystyle \forall i = 1, \ldots, m $ y $\displaystyle \forall j = 1, \ldots, p $, 
\[
\begin{split}
	f\left(\vec{u}_{i}\right) = & a^{1}_{i}\vec{v}_{1} + \cdots + a^{m}_{i}\vec{v}_{m}\\
	g\left(\vec{v}_{j}\right)= & b^{1}_{j}\vec{w}_{1} + \cdots + b^{p}_{j}\vec{w}_{p} .
\end{split}
\]
Tenemos que $\displaystyle g \circ f $ será:
\[
\begin{split}
	g\left(f\left(\vec{u}_{i}\right)\right) = & g\left(a^{1}_{i}\vec{v}_{1} + \cdots +a^{m}_{i}\vec{v}_{m}\right) \\
	= & a^{1}_{i}\left(b^{1}_{1}\vec{w}_{1} + \cdots + b^{p}_{1}\vec{w}_{p}\right) + \cdots + a^{m}_{i}\left(b^{1}_{p}\vec{w}_{1} + \cdots + b^{p}_{m}\vec{w}_{p}\right) \\
	= & \left(a^{1}_{i}b^{1}_{1} + \cdots + a^{m}_{i}b^{i}_{m}\right)\vec{w}_{1} + \cdots + \left(a^{1}_{i}b^{p}_{1} + \cdots + a^{m}_{i}b^{p}_{m}\right)\vec{w}_{p} .
\end{split}
\]

Así, tenemos que
\[ B \cdot A = \begin{pmatrix} 
	\sum^{m}_{j=1}a^{j}_{1}b^{1}_{j} & \cdots & \sum^{m}_{j=1}a^{j}_{n}b^{1}_{j} \\
	\vdots & & \vdots \\
	\sum^{m}_{j=1}a^{j}_{1}b^{p}_{j} & \cdots & \sum^{m}_{j = 1}a_{n}^{j}b^{p}_{j}
\end{pmatrix} .\]
Ahora vamos a ver como utilizamos la matriz de una transformación lineal para calcular $\displaystyle f\left(\vec{x}\right) $.\\ \\
Sea $\displaystyle f : V \to V' $ lineal. Si $\displaystyle \vec{x} \in V $, entonces existen únicos $\displaystyle x^{1}, \ldots, x^{n} \in \K $ tales que 
\[\vec{x} = x^{1} \vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} .\]
Entonces tenemos que $\displaystyle f\left(\vec{x}\right) \in V' $, por lo que $\displaystyle \exists! x'^{1}, \ldots, x'^{m} \in\K$ tales que
\[
\begin{split}
f\left(\vec{x}\right) = x'^{1}\vec{v}_{1} + \cdots + x'^{m}\vec{v}_{m}.
\end{split}
\]
Similarmente, 
\[
\begin{split}
	f\left(\vec{x}\right) = & f\left(x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n}\right) 
	=  x^{1}f\left(\vec{u}_{1}\right) + \cdots + x^{n}f\left(\vec{u}_{n}\right) \\
	= & x^{1}\left(a^{1}_{1}\vec{v}_{1} + \cdots + a^{m}_{1}\vec{v}_{m}\right) + \cdots + x^{n}\left(a^{1}_{n}\vec{v}_{1} + \cdots + a^{m}_{n}\vec{v}_{m}\right) \\
	= & \left(x^{1}a_{1}^{1} + \cdots + x^{1}a^{1}_{n}\right)\vec{v}_{1} + \cdots + \left(x^{1}a^{m}_{1} + \cdots + x^{n}a^{m}_{n}\right)\vec{v}_{m}.
\end{split}
\]
Así, tenemos que las coordenadas de $\displaystyle f\left(\vec{x}\right) $ serán:
\[
\begin{cases}
x'^{1} = x^{1}a^{1}_{1} + \cdots + x^{1}a^{1}_{n}\\
\vdots \\
x'^{m} = x^{1}a^{m}_{1} + \cdots + x^{n}a^{m}_{n}
\end{cases}
.\]
En forma matricial tenemos que
\[
	A \cdot \begin{pmatrix} x^{1} \\ x^{2} \\ \vdots \\ x^{n} \end{pmatrix} = \begin{pmatrix} x'^{1} \\ x'^{2} \\ \vdots \\ x'^{m} \end{pmatrix}
 .\]
 A continuación, vamos a estudiar la matriz de cambio de base. Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ y $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{n}\right\}  $ bases de $\displaystyle V $. Tenemos que 
 \[
 \begin{split}
	 \vec{v}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} \\
		       & \vdots \\
	 \vec{v}_{n} = & a^{1}_{n} \vec{u}_{1} + \cdots + a^{n}_{n}\vec{u}_{n}.
 \end{split}
 \]
Así, si $\displaystyle \vec{x} \in V $ existen unos únicos $\displaystyle x^{1}, \ldots, x^{n} \in \K $ tales que $\displaystyle \vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} $ y existen unos únicos $\displaystyle x'^{1}, \ldots, x'^{n} \in \K $ tales que $\displaystyle \vec{x} = x'^{1}\vec{v}_{1} + \cdots + x'^{n}\vec{v}_{n} $. Entonces, tenemos que
\[
\begin{split}
	\vec{x} = & x'^{1}\vec{v}_{1} + \cdots + x'^{n}\vec{v}_{n} \\
	= & x'^{1}\left(a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n}\right) + \cdots + x'^{n}\left(a^{1}_{n}\vec{u}_{1} + \cdots + a^{n}_{n}\vec{u}_{n}\right) \\
	= & \left(x'^{1}a^{1}_{1} + \cdots + x'^{n}a^{1}_{n}\right)\vec{u}_{1} + \cdots + \left(x'^{1}a^{n}_{1} + \cdots + x'^{n}a^{n}_{n}\right)\vec{u}_{n} .
\end{split}
\]
Así, tenemos que 
\[
\begin{cases}
x^{1} = x'^{1}a^{1}_{1} + \cdots + x'^{n}a^{1}_{n} \\
\vdots \\
x^{n} = x'^{1}a^{n}_{1} + \cdots + x'^{n}a^{n}_{n}
\end{cases}
\Rightarrow \underbrace{\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix}}_{\text{Matriz cambio de base}} \begin{pmatrix} x'^{1} \\ \vdots \\ x'^{n} \end{pmatrix} = \begin{pmatrix} x^{1} \\ \vdots \\ x^{n} \end{pmatrix}
.\]

\begin{eg}
	\normalfont $\displaystyle id _{V}: V_{ \left\{ \vec{v}_{i}\right\} } \to V _{ \left\{ \vec{u}_{i}\right\} } $. Queremos calcular la matriz $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{i}\right\} }\left(id _{V}\right) $. Tenemos que 
	\[id _{V}\left(\vec{v}_{i}\right) = \vec{v}_{i} = a^{1}_{i}\vec{u}_{1} + \cdots +a^{n}_{i}\vec{u}_{n} .\]
Por tanto, 
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{i}\right\} }\left(id _{V}\right) = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n}\\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} .\]
\end{eg}

\section{Permutaciones}
 \begin{fdefinition}[Permutación]
	 \normalfont Sea $\displaystyle X = \left\{ 1, 2, \ldots, n\right\}  $. Una \textbf{permutación} de $\displaystyle X $ es una aplicación biyectiva $\displaystyle \sigma: X \to X $. 
 \end{fdefinition}
 
 Normalmente se escriben de la siguiente forma:
 \[\sigma = \begin{bmatrix} 1 & 2 & \cdots & n\\
 \sigma\left(1\right) & \sigma\left(2\right) & \cdots & \sigma\left(n\right)\end{bmatrix}  .\]
 
 \begin{ftheorem}[]
	 \normalfont Sea $\displaystyle \mathcal{S}_{n} = \left\{ \sigma \; : \; \sigma \; \text{permutación de }\; X\right\}  $ y $\displaystyle \circ : \mathcal{S}_{n} \times \mathcal{S}_{n} \to \mathcal{S}_{n} $ sea la operación de composición entre permutaciones. Entonces, $\displaystyle \left(\mathcal{S}_{n}, \circ\right) $ es un grupo.
 \end{ftheorem}
 
 \begin{proof}
 Tenemos que la composición de funciones biyectivas es una operación cerrada. Además, tenemos que es asociativa, existe el elemento neutro (i.e. $\displaystyle id _{X} $) y el opuesto (i.e. $\displaystyle \sigma^{-1} $, el inverso, que también es biyección).
 \end{proof}

\begin{fdefinition}[]
\normalfont Sean $\displaystyle \left\{ a_{1}, \ldots, a_{r}\right\} \subset X $ distintos 2 a 2. El \textbf{ $\displaystyle r $-ciclo} $\displaystyle \left(a_{1}, \ldots, a_{r}\right) $ es la permutación $\displaystyle \left(a_{1}, \ldots, a_{r}\right) $ tal que
\[
\begin{split}
	a_{1} & \to a_{2} \\
	a_{2} & \to a_{3} \\
	      & \vdots \\
	a_{r} & \to a_{1}.
\end{split}
\]
\end{fdefinition}

\begin{fprop}[]
\normalfont Toda permutación es composición de $\displaystyle r $-ciclos.
\end{fprop}

\begin{proof}
Sea $\displaystyle \sigma \in \mathcal{S}_{n} $. Si $\displaystyle \forall a \in X $, $\displaystyle \sigma\left(a\right)=a $, entonces $\displaystyle \sigma = id _{X} $, que es un $\displaystyle 1 $-ciclo. Si $\displaystyle \exists a_{1}\in X $ tal que $\displaystyle \sigma\left(a_{1}\right) = a_{2}\neq a_{1} $, defino
\[
\begin{split}
	a_{3} & = \sigma^{2}\left(a_{1}\right) = \sigma\left(a_{2}\right) \in X \\
	      & \vdots \\
	a_{k} & = \sigma^{k-1}\left(a_{1}\right) = \sigma\left(a_{k-1}\right) \in X.
\end{split}
\]
Sea $\displaystyle r \in \N $ el primer natural tal que $\displaystyle a_{r} \in \left\{ a_{1}, \ldots, a_{r-1}\right\}  $. Entonces, $\displaystyle \sigma\left(a_{r}\right) = a_{1} $. Así, tenemos que $a_{r} = \sigma^{r-1}\left(a_{1}\right)$ y $\displaystyle \sigma\left(a_{r}\right) = \sigma^{h}\left(a_{1}\right) $, con $\displaystyle h < r $. De esta manera, $\displaystyle a^{r-h} = \sigma^{r-1-h}\left(a_{1}\right) = a_{1} $. Si $\displaystyle \forall b \in X - \left\{ a_{1}, \ldots, a_{r}\right\}  $ tenemos que $\displaystyle \sigma\left(b\right) = b $ ya hemos terminado. En cambio, si existe $\displaystyle b_{1} \in X - \left\{ a_{1}, \ldots, a_{r}\right\}  $ tal que $\displaystyle \sigma\left(b_{1}\right) = b_{2} \neq b_{1} $, hacemos el mismo procedimiento de antes:
\[
\begin{split}
	b_{3} & = \sigma\left(b_{2}\right) = \sigma^{2}\left(b_{1}\right) \\
				 & \vdots \\
	b_{k} & = \sigma\left(b_{k-1}\right) = \sigma^{k-1}\left(b_{1}\right).
\end{split}
\]
Sea $\displaystyle s $ el primer natural tal que $\displaystyle \sigma\left(b_{s}\right) \in \left\{ b_{1}, \ldots, b_{s-1}\right\}  $. Entonces, $\displaystyle \sigma\left(b_{s}\right)= b_{1} $. Así, tenemos que $\displaystyle \sigma = \left(b_{1}, \ldots, b_{s}\right) \circ \left(a_{1}, \ldots, a_{r}\right) $. Repetimos este proceso hasta que se agoten los elementos de $\displaystyle X $.
\end{proof}

\begin{eg}
\normalfont Consideremos 
\[
\begin{split}
	\sigma & = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10  \\  9 & 8 & 10 & 2 & 1 & 3 & 4 & 6 & 5 & 7 \end{bmatrix} = \begin{bmatrix} 1 & 9 & 5 & 2 & 8 & 6 & 3 & 10 & 7 & 4\\
9 & 5 & 1 & 8 & 6 & 3 & 10 & 7 & 4 & 2\end{bmatrix} \\
& = \left(1, 9, 5\right) \circ \left(2, 8, 6, 3, 10, 7, 4\right)
\end{split}
\]
\end{eg}
\begin{eg}
\normalfont 
\[
\begin{split}
	\sigma = & \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
		8 & 5 & 6 & 1 & 2 & 7 & 3 & 4 & 10 & 9\end{bmatrix} = \begin{bmatrix} 1 & 8 & 4 & 2 & 5 & 3 & 6 & 7 & 9 & 10 \\ 8 & 4 & 1 & 5 & 2 & 6 & 7 & 3 & 10 & 9\end{bmatrix} \\
		= & \left(1,8, 4\right) \circ \left(2, 5\right) \circ \left(3, 6, 7\right) \circ \left(9, 10\right).
\end{split}
\]
\end{eg}

\begin{fdefinition}[]
\normalfont Los $\displaystyle 2 $-ciclos los llamaremos \textbf{trasposiciones}.
\end{fdefinition}

\begin{fprop}[]
\normalfont Todo ciclo es producto de trasposiciones.
\end{fprop}
\begin{proof}
Tenemos que 
\[\left(a_{1}, \ldots, a_{m}\right) = \left(a_{1}, a_{2}\right)\left(a_{2}, a_{3}\right) \cdots \left(a_{m -1}, a_{m}\right) .\]
\end{proof}

Definimos 
\[
\begin{split}
	\nabla =  \prod_{1 \leq i < j \leq n} \left(\vec{x}_{i}-\vec{x}_{j}\right) = \left(\vec{x}_{1}-\vec{x}_{2}\right) &\left(\vec{x}_{1}-\vec{x}_{3}\right) \cdots \left(\vec{x}_{1}-\vec{x}_{n}\right) 
		  \left(\vec{x}_{2}-\vec{x}_{3}\right) \cdots \left(\vec{x}_{2}-\vec{x}_{n}\right) \cdots \left(\vec{x}_{n-1}-\vec{x}_{n}\right).
\end{split}
\]
De esta manera, si $\displaystyle \sigma \in \mathcal{S}_{n} $, definimos 
\[
\begin{split}
\nabla\sigma  = \prod_{1 \leq i < j \leq n}\vec{x}_{\sigma\left(i\right)}-\vec{x}_{\sigma\left(j\right)} .
\end{split}
\]
Una permutación de $\displaystyle \sigma  $ es un par $\displaystyle i < j $ tal que $\displaystyle \sigma\left(i\right) < \sigma\left(j\right) $. Una inversión de $\displaystyle \sigma  $ es un par $\displaystyle i < j $ tal que $\displaystyle \sigma\left(j\right) < \sigma\left(i\right) $. Entonces, el número de factores de $\displaystyle \nabla  $ y $\displaystyle \nabla \sigma 	 $ es el mismo. 
Así, tenemos que 
\[\nabla \sigma = \left(-1\right)^{\text{número de inversiones}} \nabla  .\]
El valor de $\displaystyle -1 $ elevado al número de inversiones de $\displaystyle \sigma  $ lo llamaremos índice o signo de $\displaystyle \sigma  $ y se escribirá $\displaystyle \ind\left(\sigma \right) $ o $\displaystyle \sig\left(\sigma \right) $ \footnote{También se corresponde con el número de trasposiciones en las que se puede componer $\displaystyle \sigma  $.} . Entonces, 
\[\nabla \sigma = \sig\left(\sigma \right) \nabla .\]
Sea $\displaystyle \tau \in \mathcal{S}_{n} $, entonces
\[\underbrace{\nabla \sigma \circ \tau}_{\sig\left(\sigma \circ \tau\right)\nabla } = \sig\left(\sigma \right)\nabla\tau = \sig\left(\sigma \right)\sig\left(\tau\right)\nabla .\]
Definimos,
\[
\begin{split}
	\sig : \mathcal{S}_{n} & \to \left\{ -1, 1\right\} \\
	 \sigma & \to \sig\left(\sigma\right).
\end{split}
\]
\begin{fprop}[]
\normalfont La trasposición $\displaystyle \left(1,2\right) $ tiene signo negativo.
\end{fprop}
\begin{proof}
Tenemos que 
\[\nabla\left(1,2\right) = \left(x_{2}-x_{1}\right) \left(x_{2}-x_{3}\right) \cdots \left(x_{2}-x_{n}\right) \left(x_{1}-x_{3}\right) \cdots \left(x_{1}-x_{n}\right) \cdots \left(x_{n-1} - x_{n}\right) .\]
Como podemos ver respecto a la primera definición de $\displaystyle \nabla  $, tenemos que todo es igual menos el primer factor, que es de signo opuesto. Así, $\displaystyle \sig\left(1,2\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont La trasposición $\displaystyle \left(1,a\right) $ es negativa para $\displaystyle a \geq 2 $. 
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \left(1, a\right) = \left(2 , a\right) \circ \left(1,2\right) \circ \left(2,a\right) $. 
\[
\begin{split}
& 1 \to 1 \to 2 \to a \\
& 2 \to a \to a \to 2 \\
& a \to 2 \to 1 \to 1.
\end{split}
\]
Entonces, 
\[\sig\left(1,a\right) = \sig\left(2,a\right)\left(-1\right)\left(\sig\left(2,a\right)\right) = \left(\sig\left(2,a\right)\right)^{2}\left(-1\right) = -1 .\]
\end{proof}

\begin{fprop}[]
\normalfont $\displaystyle \sig\left(a,b\right) = -1$.
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \left(a,b\right) = \left(1, b\right) \circ \left(1,a\right) \circ \left(1,b\right) $. Así, $\displaystyle a \to b $ y $\displaystyle b \to a $, por lo que
\[
\begin{split}
& 1 \to b \to b \to b \\
& a \to a \to 1 \to b \\
& b \to 1 \to a \to a.
\end{split}
\]
Entonces, $\displaystyle \sig\left(\left(a,b\right)\right) = \sig\left(1,b\right)^{2} \cdot \sig\left(1,a\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont $\displaystyle \sig\left(\left(a_{1}, \ldots, a_{r}\right)\right) = \left(-1\right)^{r-1} $.
\end{fprop}

\begin{proof}
Tenemos que 
\[\left(a_{1}, a_{r}\right) \circ \left(a_{1}a_{r-1}\right) \cdots \left(a_{1}a_{3}\right)\left(a_{1}a_{2}\right) .\]
De esta manera, 
\[
\begin{split}
& a_{1} \to a_{2} \to a_{2} \to \cdots \to a_{2} \to a_{2}\\
& a_{2} \to a_{1} \to a_{3} \to \cdots \to a_{3} \to a_{3} \\
& \vdots \\
& a_{r-1} \to a_{r-1} \to a_{r-1} \to \cdots \to a_{1} \to a_{r} \\
& a_{r} \to a_{r-1} \to a_{r} \to \cdots \to a_{r} \to a_{r}.
\end{split}
\]
\end{proof}

\begin{eg}
\normalfont Sea
\[
\begin{split}
	\sigma = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
	10 & 9 & 7 & 2 & 1 & 3 & 8 & 4 & 6 & 5\end{pmatrix} = \left(1, 10, 5\right) \circ \left(2, 9, 6, 3, 7, 8, 4\right) .
\end{split}
\]
Por tanto, 
\[\sig\left(\sigma \right) = \sig\left(1, 10, 5\right) \cdot \sig\left(2, 9, 6, 3, 7, 8, 4\right) = \left(-1\right)^{2} \cdot \left(-1\right)^{6} = 1 .\]
\end{eg}

\section{Estructura de álgebra}

Sean $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $, $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ y $\displaystyle \left\{ \vec{w}_{1}, \ldots, \vec{w}_{p}\right\}  $ bases de $\displaystyle V$, $\displaystyle V' $ y $\displaystyle V'' $ respectivamente. Si $\displaystyle f \in \Hom\left(V, V'\right) $, tenemos que 
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ v_{j}\right\} }\left(f\right) = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{m}_{1} & \cdots & a^{m}_{n}\end{pmatrix} = A ,\]
donde $\displaystyle f\left(\vec{u}_{i}\right) = a^{1}_{i} \vec{v}_{1} + \cdots + a^{m}_{i}\vec{v}_{m} $. Similarmente, si $\displaystyle g \in \Hom\left(V,V'\right) $, 
	\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(g\right) = \begin{pmatrix} b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
b^{m}_{1} & \cdots & b^{m}_{n}\end{pmatrix} = B .\]
Recordamos que 
		\[A + B = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(f+g\right) = \begin{pmatrix} a^{1}+b^{1} & \cdots & a^{1}_{n}+b^{1}_{n} \\
		\vdots & & \vdots \\
	a^{m}_{1}+b^{m}_{1} & \cdots & a^{m}_{n} + b^{m}_{n}\end{pmatrix} .\]
Similarmente, si $\displaystyle a \in \K $, 
			\[a \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(a \cdot f\right) = \begin{pmatrix} a a^{1}_{1} & \cdots & a a ^{1}_{n} \\
				\vdots & & \vdots \\
			a a^{m}_{1} & \cdots & a a ^{m}_{n}\end{pmatrix} .\]
\begin{fprop}[]
\normalfont El producto de matrices es distributivo por la izquierda y por la derecha.
\end{fprop}

	Si $\displaystyle f \in \Hom\left(V, V'\right) $ y $\displaystyle g \in \Hom\left(V', V''\right) $, tenemos que $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(f\right) = A \in \mathcal{M}_{ m \times n}\left(\K\right), \; \mathcal{M}_{ \left\{ \vec{v}_{j}\right\} \left\{ \vec{w}_{k}\right\}  }\left(g\right) = B \in \mathcal{M}_{p \times m}\left(\K\right)$. Definíamos el producto de matrices de la siguiente forma, 
			\[ B \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{w}_{k}\right\}}\left(g\circ f\right).\]
Si $\displaystyle f \in \Hom\left(V,V'\right) $, $\displaystyle g \in \Hom\left(V', V''\right) $ y $\displaystyle h \in \Hom\left(V'', V'''\right) $, tenemos que 
\[h \circ \left(g \circ f\right) = \left(h \circ g\right) \circ f .\]
Así, como $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, $\displaystyle B \in \mathcal{M}_{p \times m}\left(\K\right) $ y $\displaystyle C \in \mathcal{M}_{q \times p} $, tenemos que el producto de matrices es asociativo:
\[ C \cdot \left(B \cdot A\right) = \left(C \cdot B\right) \cdot A \in \mathcal{M}_{q \times n}\left(\K\right) .\]
Si $\displaystyle f,g \in \Hom\left(V, V'\right) $, $\displaystyle h \in \Hom\left(V', V''\right) $, tenemos que $\displaystyle \forall \vec{x} \in V $,
\[h \circ \left(f + g\right)\left(\vec{x}\right) = h \circ \left(f\left(\vec{x}\right) + g\left(\vec{x}\right)\right) = h\left(f\left(x\right)\right) + h\left(g\left(x\right)\right) = \left(h\circ f + h\circ g\right)\left(\vec{x}\right) .\]
Así, $\displaystyle h \circ \left(f+g\right) = h \circ f + h \circ g $. Entonces, hemos demostrado que el producto de matrices es distributivo por la izquierda. Entonces, $\displaystyle A, B \in \mathcal{M}_{m \times n} \left(\K\right)$, $\displaystyle C \in \mathcal{M}_{p\times m}\left(\K\right) $,
\[C \cdot \left(A + B \right) = C \cdot A + C \cdot B .\]
Sea $\displaystyle f \in \Hom\left(V,V'\right) $ y $\displaystyle g, h \in \Hom\left(V',V''\right) $. Tenemos que $\displaystyle \forall\vec{x} \in V $,
\[\left(\left(g+h\right)\circ f\right)\left(\vec{x}\right) = \left(g+h\right)\left(f\left(\vec{x}\right)\right) = g\left(f\left(\vec{x}\right)\right)+h\left(f\left(\vec{x}\right)\right) = g\circ f\left(\vec{x}\right) + h \circ f\left(\vec{x}\right)=\left(g \circ f + h \circ f\right)\left(\vec{x}\right) .\]
Así, concluimos que $\displaystyle \left(g+h\right)\circ f = g\circ f + h \circ f $. Por lo que el producto de matrices es distributivo por la derecha. Es decir, sean $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, $\displaystyle B,C \in \mathcal{M}_{p \times m}\left(\K\right) $, 
\[ \left(B + C\right) \cdot A = B \cdot A + C \cdot A.\]
\begin{fprop}[]
\normalfont El producto de matrices no es conmutativo.
\end{fprop}
 Si $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $ y $\displaystyle B \in \mathcal{M}_{p \times m}\left(\K\right) $. Entonces $\displaystyle B \cdot A \in \mathcal{M}_{p \times n}\left(\K\right) $, pero $\displaystyle A \cdot B $ no está definido. Si $\displaystyle A,B \in \mathcal{M}_{n \times n}\left(\K\right) $, tenemos que $\displaystyle A \cdot B, B \cdot A \in \mathcal{M}_{n \times n}\left(\K\right) $, pero no tienen por qué ser iguales. En efecto, sean 
$ \displaystyle A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} $ y $\displaystyle B = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ .Entonces
\[A \cdot B = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \neq B \cdot A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} .\]
\begin{fprop}[]
\normalfont Existe un elemento identidad en el producto de matrices
\end{fprop}
Tenemos que 
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{~/Desktop/Images/matriz_inversa1.png}
\caption{Matriz inversa respecto a $\displaystyle V $}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{~/Desktop/Images/matriz_inversa2.png}
\caption{Matriz inversa respecto a $\displaystyle V' $}
\label{ enter label$}
\end{figure}
Observando las imágenes, tenemos que 
\[A \cdot I_{n \times n} = A \quad \text{y} \quad I_{m \times m }A = A .\]
Entonces, si consideramos la aplicación
\[id _{V} : V_{ \left\{ \vec{u}_{i}\right\}} \to V_{ \left\{ \vec{u}_{i}\right\} } ,\]
tal que $\displaystyle \forall i = 1, \ldots, n $, $\displaystyle id _{V}\left(\vec{u}_{i}\right) = \vec{u}_{i} $, tenemos que su matriz será
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} }\left(id _{V}\right) = \begin{pmatrix} 1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \cdots & \vdots \\
0 & 0 & \cdots & 1\end{pmatrix} = I_{n \times n} .\]

Consideremos la función, 
\[
\begin{split}
	\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} } : \End\left(V\right) & \to \mathcal{M}_{n \times n}\left(\K\right) \\
	f & \to \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} }\left(f\right).
\end{split}
\]
\begin{fprop}[]
\normalfont Por lo demostrado anteriormente, tenemos que $\displaystyle \left(\End\left(V\right), +, \cdot _{\K}\right) $ y $\displaystyle \left( \mathcal{M}_{n \times n}, + , \cdot _{\K}\right) $ son $\displaystyle \K $-espacios vectoriales. Similarmente, $\displaystyle \left(\End\left(V\right), + ,\circ\right) $ y $\displaystyle \left(\mathcal{M}_{n \times n }\left(\K\right), +, \cdot\right) $  son anillos con unidad. 
\end{fprop}

\begin{fdefinition}[]
\normalfont Diremos que $\displaystyle \End\left(V\right) $ y $\displaystyle \mathcal{M}_{n \times n}\left(\K\right) $ son \textbf{álgebras} con unidad.
\end{fdefinition}

Tenemos que 
\[\Aut\left(V\right) = \left\{ f \in \End\left(V\right) \; : \; f \; \text{invertible}\right\} = \left\{ f \in \End\left(V\right) \; : \; \exists f^{-1}\right\}  .\]
Así, $\displaystyle \left(\Aut\left(V\right), \cdot\right) $ es un grupo. En general, la suma de automorfismos no es autormorfismo. Por ejemplo, $\displaystyle id _{V} - id _{V} = 0 \not\in\Aut\left(V\right) $. Definimos 
\[\GL_{n}\left(\K\right) = \left\{ A \in \mathcal{M}_{n \times n}\left(\K\right) \; : \; \exists A^{-1}\right\}  .\]
Para estudiar este conjunto, introduciremos la noción de determinante.

\section{Determinantes}
\begin{fdefinition}[]
	\normalfont Una $\displaystyle r $-\textbf{forma multilineal}  definida en $\displaystyle V $ es una aplicación $\displaystyle t : \underbrace{ V \times V \cdots \times V}_{r \; \text{veces}} \to \K $ tal que 
\begin{description}
\item[(a)] $\displaystyle \forall i = 1, \ldots, r, \; \forall \vec{x}_{1}, \ldots, \vec{x}_{r}, \vec{x'}_{i} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \vec{x'}_{i}, \ldots, \vec{x}_{r}\right) = t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) + t\left(\vec{x}_{1}, \ldots, \vec{x'}_{i}, \ldots, \vec{x}_{r}\right).\]
\item[(b)] $\displaystyle \forall i = 1, \ldots, r $, $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V, \; \forall a \in \K $,
	\[t\left(x_{1}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) = a t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) .\]
\end{description}
\end{fdefinition}

\begin{fdefinition}[]
\normalfont Una $\displaystyle r $-forma multilineal es \textbf{alternada} si $\displaystyle \forall \sigma \in \mathcal{S}_{r} $, 
\[t\left(\vec{x}_{\sigma\left(1\right)}, \ldots, \vec{x}_{\sigma\left(r\right)}\right) = \sig\left(\sigma \right)t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right), \; \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V .\]
\end{fdefinition}

\begin{fprop}[]
\normalfont Si $\displaystyle t $ es una $\displaystyle r $-forma multilineal, $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{0}, \ldots, \vec{x}_{r}\right) = 0 .\]
\end{fprop}
\begin{proof} Tenemos que $\displaystyle \vec{0} = 0 \cdot \vec{x}_{i}  $, así, 
\[t\left(\vec{x}_{1}, \ldots, \vec{0}, \ldots, \vec{x}_{r}\right) =0 \cdot t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) = 0.\]
\end{proof}

\begin{fprop}[]
\normalfont Sea $\displaystyle t $ una $\displaystyle r $-forma multilineal alternada. Entonces $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = -t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) .\]
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \sig\left(i,j\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont Si $\displaystyle \vec{x}_{i} = \vec{x}_{j} $ y $\displaystyle t $ es una forma multilineal alternada,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0, \; i \neq j .\]
\end{fprop}
\begin{proof}
Tenemos que 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = -t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) .\]
Esto implica que $\displaystyle t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0 $.
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) =0 .\]
\end{fprop}
\begin{proof}
Por la proposición anterior,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) = at\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) = a \cdot 0 = 0 .\]
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \sum^{r}_{j=1}a^{j}\vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0, \; j \neq i .\]
\end{fprop}
\begin{proof}
Tenemos que
\[\sum^{r}_{j=1}a^{j}t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0 .\]
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \sum^{r}_{j=1}a^{j}\vec{x}_{j}, \ldots, \vec{x}_{r}\right) = t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right), \; j \neq i.\]
\end{fprop}
	Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $, $\displaystyle t $ multilineal alternada
\[
\begin{split}
	\vec{x}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{1}\vec{u}_{j} \\
	\vdots & \\
	\vec{x}_{i} = & a^{1}_{i}\vec{u}_{1} + \cdots + a^{n}_{i}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{i}\vec{u}_{j} \\
	\vdots & \\
	\vec{x}_{r} = & a^{1}_{r}\vec{u}_{1} + \cdots + a^{n}_{r}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{r}\vec{u}_{j}.
\end{split}
\]
Así, 
\[
\begin{split}
	t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right) = & t\left(\sum^{n}_{j=1}a^{j}_{1}\vec{u}_{j}, \ldots, \sum^{n}_{j=1}a^{j}_{r}\vec{u}_{j}\right) \\
	= & \sum_{\left(j_{1}, \ldots, j_{r}\right)}a^{j_{1}}_{1}\cdots a^{j_{r}}_{r}t\left(\vec{u}_{j_{1}}, \ldots, \vec{u}_{j_{r}}\right) .
\end{split}
\]
Se trata de $\displaystyle r $-variaciones con repetición. Tenemos que si $\displaystyle j_{i} = j_{h} $, entonces el término $\displaystyle t\left(\vec{u}_{j_{1}}, \ldots, \vec{u}_{j_{r}}\right) $ se va a anular, por lo que realmente se trata de variaciones $\displaystyle n $-arias sin repetición.
Sea $\displaystyle \dim\left(V\right) = n $ y sea $\displaystyle t $ una $\displaystyle n $-forma multilineal alternada. Si $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{n}\right\} \subset V $, 
\[\begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} = \begin{pmatrix} \vec{x}_{1} & \cdots & \vec{x}_{n} \end{pmatrix}.\]

Son variaciones $\displaystyle n $-arias sin repetición de $\displaystyle \left(1, 2, \ldots, n\right) $.
\[
\begin{split}
	t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right) = &\sum_{\sigma \in \mathcal{S}_{n}} a^{\sigma\left(1\right)}_{1} a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma \left(n\right)}_{n}t\left(\vec{u}_{\sigma\left(1\right)}, \ldots, \vec{u}_{\sigma\left(n\right)}\right) \\
	= & \sum_{\sigma \in \mathcal{S}_{n}}a^{\sigma\left(1\right)}_{1} \cdots a^{\sigma\left(n\right)}_{n}\sig\left(\sigma \right)t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right).
\end{split}
\]
\begin{fdefinition}[Determinante]
\normalfont Si $\displaystyle A \in \mathcal{M}_{n \times n}\left(\K\right) $, el determinante de $\displaystyle A_{\sigma \in \mathcal{S}_{n}} $ es
\[\det A = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma \right)a^{\sigma\left(1\right)}_{1} \cdots a^{\sigma\left(n\right)}_{n} .\]
\end{fdefinition}

\begin{fdefinition}[Traspuesta]
\normalfont Si $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, llamaremos \textbf{traspuesta} de $\displaystyle A $ a la matriz $\displaystyle A^{t} \in \mathcal{M}_{n \times m}\left(\K\right) $ que tiene por filas las columnas de $\displaystyle A $.
\end{fdefinition}

\begin{ftheorem}[]
\normalfont 
\[\det A^{t} = \det A .\]
\end{ftheorem}

\begin{proof}
Si $\displaystyle A \in \mathcal{M}_{n \times n} $, entonces
\[\det A^{t} = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma \right)a^{1}_{\sigma\left(1\right)} \cdots a^{n}_{\sigma\left(n\right)} = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma^{-1}\right)a^{\sigma^{-1}\left(1\right)}_{1} \cdots a^{\sigma^{-1}\left(n\right)}_{n}=\sum_{\Pi\in\mathcal{S}_{n}} \sig\left(\Pi\right)a^{\Pi\left(1\right)}_{1} \cdots a^{\Pi\left(n\right)}_{n} = \det A.\]
\end{proof}

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $ tal que $\displaystyle t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right) = 1 $. Entonces, $\displaystyle t\left(\vec{x}_{1}, \ldots, \vec{x}_{n}\right) = \det A $ y 
\[\begin{pmatrix} \vec{x}_{1} & \cdots & \vec{x}_{n} \end{pmatrix} = \begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} A .\]

\begin{observation}
\normalfont 
\[ .\]
\[
\begin{split}
 \det\begin{pmatrix} a^{1}_{1} & \cdots& a^{1}_{i} + b^{1}_{i} & \cdots & a^{1}_{n} \\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{i}+b^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} = & t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \vec{y}_{i} , \ldots \vec{x}_{n}\right) \\
	 = & t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) + t\left(\vec{x}_{1}, \ldots, \vec{y}_{i}, \ldots, \vec{x}_{n}\right) \\
	 = & \det \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{i} & \cdots & a^{1}_{n}\\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a^{n}_{1} & \cdots & a^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} + \det \begin{pmatrix} a^{1}_{1} & \cdots & b^{1}_{i} & \cdots & a^{1}_{n}\\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a^{n}_{1} & \cdots & b^{n}_{i} & \cdots & a^{n}_{n} \end{pmatrix}.
\end{split}
\]
\end{observation}

\begin{observation}
\normalfont 
\[
\begin{split}
	\det\begin{pmatrix} a^{1}_{1} & \cdots & a a^{1}_{i} & \cdots & a^{1}_{n} \\
	\vdots & \vdots & \vdots & \vdots & \vdots\\
a^{n}_{1} & \cdots & a a ^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} = & t\left(\vec{x}_{1}, \ldots,a \vec{x}_{i}, \ldots, \vec{x}_{n}\right) \\
= & at\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) \\
= & a \det\begin{pmatrix} a^{1}_{1} & \cdots &  a^{1}_{i} & \cdots & a^{1}_{n} \\
	\vdots & \vdots & \vdots & \vdots & \vdots\\
a^{n}_{1} & \cdots &  a ^{n}_{1} & \cdots & a^{n}_{n} \end{pmatrix}.
\end{split}
\]
\end{observation}

\begin{observation}
\normalfont En el caso anteriro, si $\displaystyle a = 0 $, el determinante de la matriz es 0.
\end{observation}

\begin{observation}
\normalfont 
\[
\begin{split}
	\det\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{j} & \cdots & a^{1}_{i} & \cdots & a^{1}_{n}\\
	\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{j} & \cdots & a^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} = t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) = - t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{n}\right) .
\end{split}
\]
\end{observation}

Sea 
\[A = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n}\\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} \in \mathcal{M}_{n \times n}\left(\K\right) .\]
Tenemos que 
\[\det\left(A\right) = \sum_{\sigma \in \mathcal{S}_{n}} \sig\left(\sigma \right)a_{1}^{\sigma\left(1\right)}\cdots a^{\sigma\left(n\right)}_{n}.\]
Vamos a ver como calcular el determinante de una matriz aislando una única fila o columna. Seleccionamos el elemento $\displaystyle a_{1}^{1} $ y calculamos la suma de todos los sumandos donde interviene $\displaystyle a^{1}_{1} $:
\[ \sum_{\sigma \in \mathcal{S}_{n}, \; \sigma\left(1\right) = 1} \sig\left(\sigma \right)a^{1}_{1}a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma\left(n\right)}_{n} = \left(\sum_{\sigma \in \mathcal{S}_{n-1}}\sig\left(\sigma \right)a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma\left(n\right)}_{n}\right) \cdot a_{1}^{1} = a^{1}_{1} \det \begin{pmatrix} a^{2}_{2} & \cdots & a^{2}_{n}\\
\vdots & & \vdots \\
a^{n}_{2} & \cdots & a^{n}_{n}\end{pmatrix}.\]
Este mismo procedimiento se puede hacer para un elemento cualquiera $\displaystyle a^{i}_{j} $.
\begin{fdefinition}[Adjunto]
\normalfont  El menor complementario $\displaystyle \alpha^{i}_{j} $ es el determinante de la matriz $\displaystyle \left(n - 1\right) \times \left(n - 1\right) $ que se obtiene $\displaystyle A $ suprimiendo la fila $\displaystyle i $ y la columna $\displaystyle j $, y llamaremos \textbf{adjunto} a $\displaystyle A^{i}_{j} = \left(-1\right)^{i+j}\alpha^{i}_{j} $. 
\end{fdefinition}

Si cogemos la fija $\displaystyle j $, vamos a tener que 
\[\det\left(A\right) = a^{1}_{j}A^{1}_{j}+a^{2}_{j}A^{2}_{j}+\cdots+a^{n}_{j}A^{n}_{j} = a^{i}_{1}A^{i}_{1} + a^{i}_{2}A^{i}_{2} + \cdots + a^{i}_{n}A^{i}_{n} .\]
Si $\displaystyle i\neq k $ tenemos que
\[a^{1}_{i}A^{1}_{k} + a^{2}_{i}A^{2}_{k} + \cdots + a^{n}_{i}A^{n}_{k} = 0 .\]
Similarmente, si $\displaystyle j \neq k $,
\[a^{j}_{1}A^{k}_{1} + a^{j}_{2}A^{k}_{2} + \cdots + a^{j}_{n}A^{k}_{n} = 0 .\]
Tenemos que 
\[\begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} = \begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} I_{n\times n} .\]
Por tanto, tenemos que
\[1 = t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right) = \underbrace{t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right)}_{1}\det \left(I_{n \times n}\right) \Rightarrow \det\left(I_{ n \times n}\right) = 1 .\]

Recordamos que $\displaystyle \Aut\left(V\right) = \left\{ f \in \End\left(V\right) \; : \; \exists f^{-1}\right\}  $ y $\displaystyle \GL\left(n, \K\right) = \left\{ A \in \mathcal{M}_{n \times n}\left(\K\right) \; : \; \exists A^{-1}\right\}  $.
\begin{ftheorem}[]
\normalfont Sea $\displaystyle A \in \mathcal{M}_{ n \times n}\left(\K\right) $. Entonces, $\displaystyle A \in \GL\left(n, \K\right) \iff \det\left(A\right) \neq 0 $.
\end{ftheorem}

\begin{proof}
\begin{description}
\item[(i)] Si $\displaystyle A \in \GL\left(n, \K\right) $, $\displaystyle \exists A^{-1} $ tal que $\displaystyle A \cdot A^{-1} = A^{-1} \cdot A = I_{n \times n} $. Así,
	\[1 = \det\left(I_{n \times n}\right) = \det\left(A \cdot A^{-1}\right) = \det\left(A\right) \cdot \det\left(A^{-1}\right) .\]
Por tanto, tenemos que si $\displaystyle \det\left(A\right) \neq 0 $, $\displaystyle \det\left(A^{-1}\right) = \frac{1}{\det\left(A\right)} \neq 0 $. Estrictamente tenemos que tener que $\displaystyle \det\left(A\right), \det\left(A^{-1}\right) \neq 0 $.
\item[(ii)] Si $\displaystyle \det\left(A\right) \neq 0 $. Vamos a comprobar que $\displaystyle A^{-1} = \frac{1}{\det\left(A\right)}\Adj\left(A^{t}\right) $. Tenemos que 
\[ 	\begin{pmatrix} \frac{A^{1}_{1}}{ \left|A\right|} & \cdots & \frac{A^{n}_{1}}{ \left|A\right|} \\
	\vdots & & \vdots \\
	\frac{A^{1}_{n}}{ \left|A\right|} & \cdots & \frac{A^{n}_{n}}{ \left|A\right|}\end{pmatrix} \cdot \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} =\]
	\[
\begin{split}
		\frac{1}{\det\left(A\right)}\begin{pmatrix} A^{1}_{1}a^{1}_{1} + \cdots + a^{n}_{1}A^{n}_{1} & \cdots & A^{1}_{n}a^{1}_{n} + \cdots + A^{n}_{1}a^{n}_{n}\\
\vdots & & \vdots \\
\cdots & A^{1}_{j}a^{1}_{j} + \cdots + A^{n}_{j}a^{n}_{j} & \cdots \\
\vdots & & \vdots\end{pmatrix}
.
\end{split}
\]
Tenemos que $\displaystyle \forall i,j = 1, \ldots, n $ el coeficiente de la fila $\displaystyle j $ y la columna $\displaystyle i $ de la matriz producto. Tenemos que
\[\frac{\sum^{n}_{k=1}A^{i}_{k}a^{j}_{k}}{ \left|A\right|} = 
\begin{cases}
\frac{\det\left(A\right)}{\det\left(A\right)} = 1, \; \text{si} \; j =i \\
0, \; \text{si} \; j \neq i
\end{cases}
.\]
Es decir, el producto de las dos matrices anteriores es la identidad.
\end{description}
\end{proof}

\section{Sistemas de Cramer}

\begin{fdefinition}[Sistemas de Cramer]
\normalfont Un \textbf{sistema de Cramer} es un sistema de ecuaciones lineales
\[
\left(S\right) 
\begin{cases}
a^{1}_{1}x^{1} + \cdots + a^{1}_{n}x^{n} = b^{1} \\
\vdots \\
a^{n}_{1}x^{1}+\cdots + a^{n}_{n}x^{n} = b^{n}
\end{cases}
\text{ donde $\displaystyle a^{j}_{i} \in \K $, $\displaystyle b^{j} \in \K $, $\displaystyle \forall i, j = 1, \ldots, n $.}
\]
Además, 
\[\det \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n}\\
\vdots & & \vdots \\
a^{1}_{n} & \cdots & a^{n}_{n}\end{pmatrix} \neq 0 .\]
\end{fdefinition}

\begin{ftheorem}[]
\normalfont Todo sistema de cramer tiene solución única.
\end{ftheorem}

\begin{proof}
Primero vemos la \textbf{unicidad}. Si $\displaystyle \left(x^{1}_{0}, \ldots, x^{n}_{0}\right) $ es solución de $\displaystyle \left(S\right) $,
\[
\left(S\right) 
\begin{cases}
a^{1}_{1}x^{1}_{0} + \cdots + a^{1}_{n}x^{n}_{0} = b^{1} \\
\vdots \\
a^{n}_{1}x^{1}_{0}+\cdots + a^{n}_{n}x^{n}_{0} = b^{n}
\end{cases}
\text{ donde $\displaystyle a^{j}_{i} \in \K $, $\displaystyle b^{j} \in \K $, $\displaystyle \forall i, j = 1, \ldots, n $.}
\]
Entonces, 
\[\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix}\begin{pmatrix} x^{1}_{0} \\ \vdots \\ x^{n}_{0}\end{pmatrix} = \begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} .\]
Como $\displaystyle \det\left(A\right)\neq 0 $, tenemos que $\displaystyle \exists!A^{-1} $. Entonces,
\[A^{-1}A\begin{pmatrix} x^{1}_{0} \\ \vdots \\ x^{n}_{0} \end{pmatrix} = A^{-1}\begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} \Rightarrow \begin{pmatrix} x^{1}_{0} \\ \vdots \\ x^{n}_{0} \end{pmatrix} = A^{-1}\begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} .\]
Ahora vamos a ver la \textbf{existencia}. Vamos a ver que el producto $\displaystyle A^{-1}\begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} $ es solución de $\displaystyle S $.
\[
\begin{split}
	A^{-1}\begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} = & \frac{1}{\det\left(A\right)}\begin{pmatrix} A^{1}_{1} & \cdots & A^{n}_{1} \\
\vdots & & \vdots \\
A^{1}_{n} & \cdots & A^{n}_{n}\end{pmatrix}\begin{pmatrix} b^{1} \\ \vdots \\ b^{n} \end{pmatrix} = \frac{1}{\det\left(A\right)}\begin{pmatrix} A^{1}_{1}b^{1} + \cdots + A^{n}_{1}b^{n} \\
\vdots \\
A^{1}_{n}b^{1} + \cdots + A^{n}_{n}b^{n}\end{pmatrix}\\
= & \begin{pmatrix} \frac{ \det\begin{pmatrix} b^{1} & a^{1}_{2} & \cdots & a^{1}_{n}\\
\vdots & \vdots & \vdots & \vdots \\
b^{n} & a^{n}_{2} & \cdots & a^{n}_{n}\end{pmatrix}}{\det\left(A\right)} \\ \vdots \\
		\frac{ \det\begin{pmatrix} a^{1}_{1} & \cdots& a^{1}_{i-1} & b^{1} & a^{1}_{i+1} & \cdots & a^{1}_{n}\\
\vdots & \vdots & \vdots & \vdots \\
 a^{n}_{1} & \cdots& a^{n}_{i-1} & b^{n} & a^{n}_{i+1} & \cdots & a^{n}_{n}\end{pmatrix}}{\det\left(A\right)} \\ \vdots \\ \frac{ \det\begin{pmatrix} a^{1}_{1} & a^{1}_{2} & \cdots & b^{1}\\
\vdots & \vdots & \vdots & \vdots \\
a^{n}_{1} & a^{n}_{2} & \cdots & b^{n}\end{pmatrix}}{\det\left(A\right)}\end{pmatrix} \begin{pmatrix} \frac{A^{1}_{1}}{ \left|A\right|} & \cdots & \frac{A^{n}_{1}}{ \left|A\right|} \\
\vdots & & \vdots \\
\frac{A^{1}_{n}}{ \left|A\right|} & \cdots & \frac{A^{n}_{n}}{ \left|A\right|}\end{pmatrix}.
\end{split}
\]
\end{proof}

\begin{fdefinition}[Menor]
	\normalfont Un \textbf{menor} de orden $\displaystyle r $ de $\displaystyle A $, $\displaystyle r \leq \min \left\{ m,n\right\}  $, es el determinante de una matriz $\displaystyle r \times r $ que se obtiene de $\displaystyle A $ suprimiendo $\displaystyle \left(m - r\right) $ filas y $\displaystyle \left(n - r\right) $ columnas. 
\end{fdefinition}

\begin{fdefinition}[Rango]
\normalfont Decimos que el \textbf{rango} de $\displaystyle A $ es $\displaystyle r $ si existe un menor de orden $\displaystyle r $ de $\displaystyle A $ no nulo y todos los de orden $\displaystyle r+1 $ son nulos.
\end{fdefinition}
Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de un $\displaystyle \K $-espacio vectorial $\displaystyle V $ y sean $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{m}\right\} \subset V $, tales que
\[
\begin{split}
	\vec{x}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} \\
	\vdots & \\
	\vec{x}_{m} = & a^{1}_{m}\vec{u}_{1} + \cdots + a^{n}_{m}\vec{u}_{n}.
\end{split}
\]
Entonces, en función de estos valores definimos,
\[A = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{m}\\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{m}\end{pmatrix} \in \mathcal{M}_{n \times m}\left(\K\right) .\]
\begin{ftheorem}[]
\normalfont 
\[\ran\left(A\right) = \dim\left(L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{m}\right\} \right)\right) .\]
\end{ftheorem}
\begin{proof}
	Sea $\displaystyle r = \ran\left(A\right) $ y $\displaystyle \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r}\\
	\vdots & & \vdots \\
a^{r}_{1} & \cdots & a^{r}_{r}\end{vmatrix} \neq 0 $.
\begin{description}
	\item[(1)] Vamos a ver que $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{r}\right\}  $ son linealmente independientes. \\
	Sean $\displaystyle a^{1}, \ldots, a^{r} \in \K $. Si 
	\[a^{1}\vec{x}_{1} + \cdots + a^{r}\vec{x}_{r} = \vec{0} .\]
	Entonces, 
	\[
	\begin{split}
		& a^{1}\left(a^{1}_{1}\vec{u}_{1} + \cdots + a^{r}_{1}\vec{u}_{r} + \cdots +a^{n}_{1}\vec{u}_{n}\right) + \cdots + a^{r}\left(a^{1}_{r}\vec{u}_{1} + \cdots+a^{r}_{r}\vec{u}_{r}+\cdots + a^{n}_{r}\vec{u}_{n}\right)  \\
		= & \left(a^{1}a^{1}_{1} + \cdots + a^{r}a^{1}_{r}\right)\vec{u}_{1} + \cdots + \left(a^{1}a^{r}_{1} + \cdots + a^{r}a^{r}_{r}\right)\vec{u}_{r} + \cdots + \left(a^{1}a^{n}_{1}+ \cdots + a^{r}a^{n}_{r}\right)\vec{u}_{n} = \vec{0}.
	\end{split}
	\]
	Así, tenemos que 
	\[
	\begin{split}
		a^{1}a^{1}_{1} + \cdots + a^{r}a^{1}_{r} & = 0 \\
							 & \vdots \\
		a^{1}a^{r}_{1} + \cdots + a^{r}a^{r}_{r} & = 0 \\
							 & \vdots \\
		a^{1}a^{n}_{1}+ \cdots + a^{r}a^{n}_{r} & = 0.
	\end{split}
	\]
	Entonces, $\displaystyle \left(a^{1}, \ldots, a^{r}\right) $ es solución del sistema 
	\[
	\begin{cases}
	a^{1}_{1}x^{1} + \cdots + a^{1}_{r}x^{r} = 0\\
	\vdots \\
	a^{r}_{1}x^{1} + \cdots + a^{r}_{r}x^{r} = 0
	\end{cases}
	.\]
Como es un sistema de Cramer homogéneo, tenemos que $\displaystyle a^{1} = \cdots = a^{r} = 0 $.	
	\item[(2)] Vamos a ver que $\displaystyle \forall i = 1, \ldots, m-r $, $\displaystyle \vec{x}_{r+i} $ es linealmente dependiente de $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{r}\right\}  $. \\
		Sea $\displaystyle i = 1, \ldots, m - r $, queremos ver que $\displaystyle \vec{x}_{r+i} \in L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{r}\right\} \right) $. Queremos llegar al siguiente sistema, respecto a las coordenadas de $\displaystyle \vec{x}_{r+i} $, que es un sistema de Cramer:
	\[ \left(S\right) =
	\begin{cases}
	a^{1}_{1}x^{1} + \cdots + a^{1}_{r}x^{r} = x^{1}_{r+i}\\
	\vdots \\
	a^{r}_{1}x^{1} + \cdots + a^{r}_{r}x^{r} = x^{r}_{r+i}
	\end{cases}
	.\]
Al ser un sistema de Cramer, tenemos que $\displaystyle \exists!\left(a^{1}, \ldots, a^{r}\right) $ solución de $\displaystyle S $. Tenemos que
\[
\begin{split}
	0 = & \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} &  a^{1}_{r+i} \\
\vdots & \vdots & \vdots & \vdots \\
a^{r}_{1} & \cdots & a^{r}_{r}&  a^{r}_{r+i} \\
a^{r+j}_{1} & \cdots & a^{r+j}_{r}&  a^{r+j}_{r+i} \end{vmatrix} \\
		= & \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} &  a^{1}_{r+i} \\
		\vdots & \vdots & \vdots & \vdots  \\
		a^{r}_{1} & \cdots & a^{r}_{r}&  a^{r}_{r+i} \\
0 & \cdots & 0 & a^{r+j}_{r+i} - a^{1}a^{1}_{r+i}-\cdots-a^{r}a^{r}_{r+i}\end{vmatrix} \\
			= & \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} \\ \vdots & & \vdots \\
			a^{r}_{1} & \cdots & a^{r}_{r}\end{vmatrix} \left(a^{r+j}_{r+i}-a^{1}a^{1}_{r+i}-\cdots -a^{r}_{r+i}\right)
\end{split}
.\]
Entonces, 
\[a^{r+j}_{r+i} = a^{1}a^{1}_{r+i} + \cdots + a^{r}a^{r}_{r+i} .\]
\end{description}
\end{proof}

Dado el sistema 
\[
\left(S\right) 
\begin{cases}
a^{1}_{1}x^{1} + \cdots + a^{1}_{n}x^{n} = b^{1} \\
\vdots \\
a^{m}_{1}x^{1}+\cdots + a^{m}_{n}x^{n} = b^{m}
\end{cases}
\]
Definimos las matrices
\[A = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{m}_{1} & \cdots & a^{m}_{n}\end{pmatrix} \in \mathcal{M}_{m \times n}\left(\K\right), \quad B = \begin{pmatrix} a^{1}_{1} & a^{1}_{2} & \cdots &  a^{1}_{n} & b^{1} \\ \vdots & \cdots & \cdots & \cdots & \vdots \\ a^{m}_{1} & a^{m}_{2} & \cdots & a^{m}_{n}& b^{m}\end{pmatrix}\in\mathcal{M}_{m \times \left(n+1\right)}\left(\K\right) .\]

Tenemos que $\displaystyle \ran\left(A\right) \leq \ran\left(B\right) \leq \ran\left(A\right) + 1 $.
\begin{ftheorem}[Teorema de Rouché-Fröbenius]
\normalfont $\displaystyle \left(S\right) $ tiene solución si y solo si $\displaystyle \ran\left(A\right) = \ran\left(B\right) $.
\end{ftheorem}
\begin{proof}
\begin{description}
\item[(i)] Si el sistema $\displaystyle \left(S\right) $ tiene solución, existe $\displaystyle \left(x^{1}_{0}, \ldots, x^{n}_{0}\right) \in \K^{n} $ tal que 
	\[
	\begin{cases}
	a^{1}_{1}x^{1}_{0} + \cdots + a^{1}_{n}x^{n}_{0} = b^{1} \\
	\vdots \\
	a^{m}_{1}x^{1}_{0}+\cdots + a^{m}_{n}x^{n}_{0} = b^{n}
	\end{cases}
	.\]
Como la última columna de la matriz $\displaystyle B $ depende linealmente de $\displaystyle A $, tenemos que $\displaystyle \ran\left(A\right) = \ran\left(B\right) $.
\item[(ii)] Supongamos que $\displaystyle \ran\left(A\right) = \ran\left(B\right) = r $ y que $\displaystyle \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} \\
	\vdots & & \vdots \\
a^{r}_{1} & \cdots & a^{r}_{r}\end{vmatrix} \neq 0 $. Consideramos el sistema $\displaystyle \left(P\right) $ el sistema en el que intervienen las primeras $\displaystyle r $ ecuaciones:
\[\left(P\right)
\begin{cases}
a^{1}_{1}x^{1} + \cdots + a^{1}_{r}x^{r} + \cdots + a^{1}_{n}x^{n} = b^{1} \\ 
\vdots \\
a^{r}_{1}x^{1} + \cdots + a^{r}_{r}x^{r} + \cdots + a^{r}_{n}x^{n} = b^{r}
\end{cases}
.\]
Tenemos que toda solución de $\displaystyle \left(S\right) $ es solución de $\displaystyle \left(P\right) $. Vamos a ver que toda solución de $\displaystyle \left(P\right) $ es solución de $\displaystyle \left(S\right) $. Sea $\displaystyle \left(x^{1}_{0}, \ldots, x^{r}_{0}, \ldots, x^{n}_{0}\right) $ solución de $\displaystyle \left(P\right) $. Entonces,
\[\begin{split}
	a^{1}_{1} x^{1}_{0} + \cdots + a^{1}_{r}x^{r}_{0} + \cdots + a^{1}_{n}x^{n}_{0} = b^{1} \\
\vdots \\
a^{r}_{1}x^{1}_{0} + \cdots + a^{r}_{r}x^{r}_{0} + \cdots + a^{r}_{n}x^{n}_{0} = b^{r}.
\end{split}\]
Si $\displaystyle i > 1, \ldots, m - r $, queremos ver que
\[ a^{r+i}_{1}x^{1}_{0} + \cdots + a^{r+i}_{r}x^{r}_{0} + \cdots + a^{r}_{n}x^{n}_{0} = b^{r+i}.\]
Tenemos que existen $\displaystyle l_{1}, \ldots, l_{r}\in\K $ tales que 
\[
\begin{split}
	l_{1}a^{1}_{1} + \cdots + l_{r}a^{r}_{1} & = a_{1}^{r+i} \\
	\vdots & \\
	l_{1}a^{1}_{r} + \cdots + l_{r}a^{r}_{r} & = a^{r+i}_{r} \\
	\vdots & \\
	l_{1}a^{1}_{n} + \cdots + l_{r}a^{r}_{n} & = a^{r+i}_{n}\\
	l_{1}b^{1} + \cdots + l_{r}b^{r} & = b^{r+i}
\end{split}
.\]
Entonces, tenemos que
\[
\begin{split}
	a^{r+i}1x^{1}_{0} + \cdots + a^{r+i}_{r} x^{r}_{0} + \cdots + a^{r+i}_{n}x_{0}^{n} = & \left(l_{1} a^{1}_{1} + \cdots +l_{r}a^{r}_{1}\right)x^{1}_{0} + \cdots + \left(l_{1}a^{1}_{n} + \cdots +l_{r}a^{r}_{n}\right)x^{n}_{0} \\
	= & l_{1}\left(a^{1}_{1}x^{1}_{0} + \cdots + a^{1}_{n}x^{n}_{0}\right) + \cdots + l_{r}\left(a^{r}_{1}x^{1}_{0} + \cdots + a^{r}_{n}x^{n}_{0}\right)\\
	= & l_{1}b^{1} + \cdots + l_{r}b^{r} = b^{r+i} .
\end{split}
\]
Entonces, tenemos que el sistema $\displaystyle \left(P'\right) $ 
\[\left(P'\right)
\begin{cases}
	a^{1}_{1}x^{1} + \cdots + a^{1}_{r}x^{r} = b^{1}-a^{1}_{r+1}x^{r+1}-\cdots -a^{1}_{n}x^{n} \\
	\vdots \\
	a^{r}_{1}x^{1} + \cdots + a^{r}_{r} x^{r} = b^{r}-a^{r}_{r+1}x^{r+1}-\cdots-a^{r}_{n}x^{n}
\end{cases}
.\]
Claramente, el sistema $\displaystyle \left(P'\right) $ es equivalente al sistema $\displaystyle \left(P\right) $. Si fijo $\displaystyle \left(x^{r+1}_{0}, \ldots, x^{n}_{0}\right) \in \K^{n-r} $, tenemos que el sistema 
\[\left(P_{1}\right)
\begin{cases}
	a^{1}_{1}x^{1} + \cdots + a^{1}_{r}x^{r} = b^{1}-a^{1}_{r+1}x^{r+1}_{0}-\cdots -a^{1}_{n}x^{n}_{0} \\
	\vdots \\
	a^{r}_{1}x^{1} + \cdots + a^{r}_{r} x^{r} = b^{r}-a^{r}_{r+1}x^{r+1}_{0}-\cdots-a^{r}_{n}x^{n}_{0}
\end{cases}
.\]
Al tratarse de un sistema de Cramer, existe una única solución $\displaystyle \left(x^{1}_{0}, \ldots, x^{r}_{0}\right) $ de $\displaystyle \left(P_{1}\right) $. Entonces, $\displaystyle \left(x^{1}_{0}, \ldots, x^{r}_{0}, x^{r+1}_{0}, \ldots, x^{n}_{0}\right) $ es solución de $\displaystyle \left(P\right) $. Si $\displaystyle \left(y^{r+1}_{0}, \ldots, y^{n}_{0}\right)\in\K^{n-r} $. Consideremos el sistema $\displaystyle \left(P_{2}\right) $,
\[\left(P_{2}\right)
\begin{cases}
a^{1}_{1}x^{1}+\cdots + a^{1}_{r}x^{r} = b^{1}-a^{1}_{r+1}y^{r+1}_{0}-\cdots-a^{1}_{n}y^{n}_{0}\\
\vdots \\
a^{r}_{1}x^{1} + \cdots + a^{r}_{r}x^{r} = b^{r}-a^{r}_{r+1}y^{r+1}_{0} - \cdots - a^{r}_{n}y^{n}_{0}
\end{cases}
.\]
Como es sistema de Cramer, $\displaystyle \exists!\left(y^{1}_{0}, \ldots, y^{r}_{0}\right) $ solución de $\displaystyle \left(P_{2}\right) $. Por tanto, $\displaystyle \left(y^{1}_{0}, \ldots, y^{r}_{0}, y^{r+1}_{0}, \ldots, y^{n}_{0}\right) $ es solución de $\displaystyle \left(P\right) $.
\end{description}
\end{proof}

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ una base de $\displaystyle V $. Sea $\displaystyle L \in \mathcal{L}\left(V\right) $ tal que $\displaystyle \dim\left(L\right)= r $ y sea $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{r}\right\}  $ una base de $\displaystyle L $. Entonces, tenemos que
\[
\begin{split}
	\vec{v}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} \\
	\vdots & \\
	\vec{v}_{r} = & a^{1}_{r}\vec{u}_{1} + \cdots + a^{n}_{r}\vec{u}_{n}.
\end{split}
\]

Entonces, si $\displaystyle \vec{x} \in V $ con $\displaystyle \vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} $. Tenemos que $\displaystyle \vec{x} \in L $ si y solo si existen $\displaystyle \lambda^{1}, \ldots, \lambda^{r} \in \K $ tales que $\displaystyle \vec{x} = \lambda^{1}\vec{v}_{1}+\cdots+\lambda^{r}\vec{v}_{r} $ (ecuaciones paramétricas). Esto es, si y solo si
\[
\begin{split}
	x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} = & \lambda^{1}\left(a^{1}_{1}\vec{u}_{1}+\cdots +a^{n}_{1}\vec{u}_{n}\right) + \cdots + \lambda^{r}\left(a^{1}_{r}\vec{u}_{1}+\cdots+a^{n}_{r}\vec{u}_{n}\right) \\
	= & \left(\lambda^{1}a^{1}_{1}+\cdots + \lambda^{r}a^{1}_{r}\right)\vec{u}_{1} + \cdots + \left(\lambda^{1}a^{n}_{1}+\cdots + \lambda^{r}a^{n}_{r}\right)\vec{u}_{n} .
\end{split}
\]
Entonces, 
\[
\begin{split}
	x^{1} = & \lambda^{1}a^{1}_{1} + \cdots + \lambda^{r}a^{1}_{r} \\
	\vdots & \\
	x^{n} = & \lambda^{1}a^{n}_{1} + \cdots + \lambda^{r}a^{n}_{r}.
\end{split}
\]
Es decir, si y solo si el sistema anterior tiene solución. Es decir, 
\[r = \ran\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} = \ran\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} & x^{1} \\
\vdots & \cdots & \vdots & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n} & x^{n}\end{pmatrix} .\]
Así obtenemos las ecuaciones implícitas de $\displaystyle L $. Supongamos que 
	\[ \begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} \\
	\vdots & & \vdots \\
a^{r}_{1} & \cdots & a^{r}_{r}\end{vmatrix}= \nabla \neq 0 .\]
Entonces, tenemos que los menores de orden $\displaystyle r+1 $ de la matriz de coeficientes que contienen al menor anterior son nulos.
		\[\begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} & x^{1} \\
		\vdots & \cdots & \vdots & \vdots \\
	a^{r}_{1} & \cdots & a^{r}_{r} & x^{r} \\
a^{r+1}_{1} & \cdots & a^{r+1}_{r} & x^{r+1}\end{vmatrix}  = 0 .\]
Similarmente, $\displaystyle \forall i = 1, \ldots, n - r $,
	\[\begin{vmatrix} a^{1}_{1} & \cdots & a^{1}_{r} & x^{1} \\
		\vdots & \cdots & \vdots & \vdots \\
	a^{r}_{1} & \cdots & a^{r}_{r} & x^{r} \\
a^{r+i}_{1} & \cdots & a^{r+i}_{r} & x^{r+i}\end{vmatrix}  = 0 .\]
Si desarrollamos este determinante por adjuntos:
\[
\begin{split}
x^{1}b^{1}_{1} + x^{2}b^{1}_{2} + \cdots + x^{r}b^{1}_{r} x^{r+1} \nabla = 0.
\end{split}
\]
Tenemos que $\displaystyle \forall i = 1, \ldots, n -r $, 
\[x^{1}b^{1}_{1} + \cdots + x^{r}b^{1}_{r} + x^{r+i}\nabla = 0 .\]
Tenemos $\displaystyle n - r $ ecuaciones donde la matriz de coeficientes será
\[\ran \begin{pmatrix} b^{1}_{1} & \cdots & b^{1}_{r} & \nabla & 0 & \cdots & 0 \\
b^{2}_{1} & \cdots & b^{2}_{r} & 0 & \nabla & \cdots & 0 \\
\vdots & \cdots & \vdots & \vdots & \vdots & \cdots & \vdots \\
b^{n-r}_{1} & \cdots & b^{n-r}_{r} & 0 & \cdots & 0 & \nabla\end{pmatrix} = n - r .\]
Con esto hemos demostrado que todo subespacio vectorial es el conjunto de las soluciones de un sistema homogéneo. Ahora consideremos el recíproco. Consideremos el sistema homogéneo $\displaystyle \left(H\right) $:
\[\left(H\right)
\begin{cases}
c^{1}_{1}x^{1} + \cdots + c^{1}_{n}x^{n} = 0 \\
\vdots \\
c^{n-r}_{1}x^{1} + \cdots + c^{n-r}_{n}x^{n} = 0
\end{cases}
.\]
Sea $\displaystyle L = \left\{ x^{1}_{0}\vec{u}_{1} + \cdots + x^{n}_{0}\vec{u}_{n} \; : \; \left(x^{1}_{0}, \ldots, x^{n}_{0}\right) \; \text{solución de} \; \left(H\right)\right\} \in \mathcal{L}\left(V\right) $ tales que $\displaystyle \dim\left(L\right)=r $. Supongamos que 
\[\ran\begin{pmatrix} c^{1}_{1} & \cdots & c^{1}_{n} \\ \vdots & & \vdots \\ c^{n-r}_{1} & \cdots & c^{n-r}_{n} \end{pmatrix} = n-r .\]
En la introducción está demostrado que $\displaystyle L $ es un subespacio vectorial. Supongamos que
\[ \begin{vmatrix} c^{1}_{1} & \cdots & c^{1}_{n-r} \\
	\vdots & & \vdots \\
c^{n-r}_{1} & \cdots & c^{n-r}_{n-r}\end{vmatrix} \neq 0 .\]
Entonces
\[
\begin{cases}
c^{1}_{1}x^{1} + \cdots + c^{1}_{n-r}x^{n-r} = -c^{1}_{\left(n-r\right)+1}x^{\left(n-r\right)+1} - \cdots - c^{1}_{n}x^{n} \\
\vdots \\
c^{n-r}_{1}x^{1}+\cdots + c^{n-r}_{n-r}x^{n-r} = -c^{n-r}_{\left(n-r\right)+1}x^{\left(n-r\right)+1} - \cdots c^{n-r}_{n}x^{n}
\end{cases}
.\]
Supongamos que las siguientes tuplas son soluciones de $\displaystyle \left(H\right) $:
\[
\begin{cases}
	\left(d^{1}_{1}, \ldots, d^{n-r}_{1}, 1, 0 \ldots, 0\right) \\
	\left(d^{1}_{2}, \ldots, d^{n-r}_{2}, 0, 1, \ldots, 0\right) \\
	\vdots \\
	\left(d^{1}_{r}, \ldots, d^{n-r}_{r},0, \ldots,0, 1\right)
\end{cases}
.\]

Sean 
\[
\begin{split}
	\vec{v}_{1} = & d^{1}_{1}\vec{u}_{1} + \cdots + d^{n-r}_{1}\vec{u}_{n-r} + \vec{u}_{n-r+1} \in L \\
	\vdots & \\
	\vec{v}_{r} = & d^{1}_{r}\vec{u}_{1} + \cdots + d^{n-r}_{r}\vec{u}_{n-r} + \vec{u}_{n-r+1} \in L
\end{split}
\]
Entonces, 
\[
\begin{split}
	\vec{x}_{0} = & x^{1}_{0}\vec{u}_{1} + \cdots + x^{n-r}_{0}\vec{u}_{n-r}+\cdots + x^{n}_{0}\vec{u}_{n} \in L \\
	\vec{z}_{0} = & \vec{x}_{0}-x^{n-r+1}_{0}\vec{x}_{1}-x_{0}^{n-r+2}\vec{v}_{2} - \cdots - x^{n}_{0}\vec{v}_{r} = z^{1}_{0}\vec{u}_{1} + \cdots + z^{n-r}_{0}\vec{u}_{n-r} + 0 \cdot \vec{u}_{n-r+1} + \cdots + 0 \cdot \vec{u}_{n} \in L.
\end{split}
\]
Entonces tenemos que $\displaystyle \left(z^{1}_{0}, \ldots, z^{n-r}_{0}, 0, \ldots, 0\right) $ son solución de $\displaystyle \left(H\right) $. Entonces, $\displaystyle \left(z^{1}_{0}, \ldots, z^{n-r}_{0}\right) $ es solución del sistema de Cramer. 
\[
\begin{cases}
c^{1}_{1}x^{1} + \cdots + c^{1}_{n-r}x^{n-r} = 0 \\
\vdots \\
c^{n-r}_{1}x^{1} + \cdots + c^{n-r}_{n-r}x^{n-r} = 0
\end{cases}
.\]
Así, $\displaystyle \vec{z}_{0} = \vec{0} $ y 
\[\vec{x}_{0} = x_{0}^{n-r+1}\vec{v}_{1} + x^{n}_{0}\vec{v}_{r} \Rightarrow L = L\left( \left\{ \vec{v}_{1}, \ldots, \vec{v}_{r}\right\} \right) .\]

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $ y $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{r}\right\} \subset V $, donde 
\[A = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{r} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{r}\end{pmatrix} \in \mathcal{M}_{n \times r}\left(\K\right).\]
Tenemos que $\displaystyle \dim\left(L \left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{r}\right\} \right)\right) = \ran\left(A\right)$. 
\begin{fprop}[]
\normalfont 
\[L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right\} \right) = L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right\} \right) .\]
\end{fprop}

\begin{fprop}[]
	\normalfont Sea $\displaystyle a \in \K/ \left\{ 0\right\}  $, 
	\[ L\left( \left\{ \vec{x}_{1}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right\} \right).= L\left( \left\{ \vec{x}_{1},\ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right\} \right)\]
\end{fprop}

\begin{fprop}[]
\normalfont Si $\displaystyle a \in \K $, 
\[L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{j}+a\vec{x}_{i} \ldots, \vec{x}_{r}\right\} \right) = L\left( \left\{ \vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right\} \right) .\]
\end{fprop}

Si $\displaystyle a^{1}_{1} \neq 0 $, a la segunda fila le resto la primera multiplicada por $\displaystyle \frac{a^{2}_{1}}{a^{1}_{1}} $. Para $\displaystyle i = 2, \ldots, n $ a la fila $\displaystyle i $  le resto la primera multiplicada por $\displaystyle \frac{a^{i}_{1}}{a^{1}_{1}} $. 
\[\begin{pmatrix} a^{1}_{1} & a^{1}_{2} & \cdots & a^{1}_{r} \\
0 & a^{2}_{2} & \cdots & a'^{2}_{r} \\
\vdots & \vdots & \cdots & \vdots \\
0 & a'^{r}_{2} & \cdots & a'^{r}_{r}\end{pmatrix} .\]
Si $\displaystyle a^{1}_{1} = 0 $ y $\displaystyle \exists a^{i}_{1} \neq 0 $, permutamos las filas 1 e $\displaystyle i $. Supongo por hipótesis de inducción que después de $\displaystyle i $ etapas. 
	\[\begin{pmatrix} b^{1}_{1} & * & \cdots & * \\
	0 & b^{2}_{1} & \cdots & * \\
0 & \cdots & b^{i}_{1} & * \\
0 & \cdots & 0 & D\end{pmatrix} ,\]
donde
		\[D = \begin{pmatrix} d^{1}_{1} & \cdots & d^{1}_{t} \\
		\vdots & & \vdots \\
	d^{s}_{1} & \cdots & d^{t}_{r}\end{pmatrix} .\]
El método anterior consiste en una forma de calcular el rango de una matriz usando Gauss. La forma de calcular la inversa de una matriz $\displaystyle A \in \GL\left(n, \K\right) $ consiste en utilizar combinaciones lineales hasta obtener la identidad. Si hacemos las mismas trasformaciones con $\displaystyle I_{n \times n} $, obtenemos $\displaystyle A^{-1} $.
