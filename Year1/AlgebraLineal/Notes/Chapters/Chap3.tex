\chapter{Matrices}
\section{Matriz asociada a una aplicación lineal}
\begin{fdefinition}[Matriz]
\normalfont Una matriz $\displaystyle A $ con coeficientes $\displaystyle \K $ de $\displaystyle m \in \N $ filas y $\displaystyle n \in \N $ columnas es una tabla 
\[A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} .\]
con $\displaystyle a^{j}_{i} \in \K, \; \forall i = 1, \ldots, n, \; \forall j= 1, \ldots, m  $.
\end{fdefinition}

Sean $\displaystyle V $, $\displaystyle V' $ espacios vectoriales con $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ y $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ bases de $\displaystyle V $ y $\displaystyle V' $, respectivamente. Si $\displaystyle f \in \Hom\left(V, V'\right) $, entonces $\displaystyle f\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right) \in V' $ y existen $\displaystyle a^{j}_{i} \in \K $ tales que 
\[
\begin{split}
& f\left(\vec{u}_{1}\right) = a^{1}_{1}\vec{v}_{1} + \cdots + a^{m}_{1}\vec{v}_{m} \\
& f\left(\vec{u}_{2}\right) = a^{1}_{2}\vec{v}_{1} + \cdots + a^{m}_{2}\vec{v}_{m} \\
& \vdots \\
& f\left(\vec{u}_{n}\right) = a^{1}_{n}\vec{v}_{1} + \cdots + a^{m}_{n}\vec{v}_{m}.
\end{split}
\]
Entonces, a $\displaystyle f $  le podemos asignar la matriz $\displaystyle A $ tal que 
\[f \to A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix}.\]
\begin{ftheorem}[]
\normalfont 
Sea $\displaystyle \mathcal{M}_{m\times n}\left(\K\right) = \left\{ A \; : \; A \; \text{matriz} \; m\times n\right\}  $. Definimos 
\[
\begin{split}
	\mathcal{M}_{\{\vec{u}_{i}\}\{\vec{v}_{j}\}} : \Hom\left(V, V'\right) & \to \mathcal{M}_{m \times n}\left(\K\right) \\ 
 f & \to A.
\end{split}
\]
Tenemos que $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} } $ es biyectiva.
\end{ftheorem}

\begin{proof}
Tenemos que si $\displaystyle A \in \mathcal{M}_{m\times n}\left(\K\right) $, entonces existen unos únicos $\displaystyle \vec{w}_{1}, \ldots, \vec{w}_{n}\in V' $ tales que 
\[
\begin{split}
	\vec{w}_{1} = & a^{1}_{1}\vec{v}_{1} + \cdots + a^{n}_{1}\vec{v}_{m} \\
		      & \vdots \\
	\vec{w}_{n}= & a^{1}_{m}\vec{v}_{1} + \cdots + a^{n}_{m}\vec{v}_{m} .
\end{split}
\]
Para estos existe una única $\displaystyle f \in \Hom\left(V, V'\right) $ tal que $\displaystyle f\left(\vec{u}_{1}\right) = \vec{w}_{1} $, \ldots, $\displaystyle f\left(\vec{u}_{n}\right) = \vec{w}_{n} $.
\end{proof}

\begin{ftheorem}[]
	\normalfont $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} } $ es isomorfismo.
\end{ftheorem}

\begin{proof}
Sabemos que es biyectiva, ahora tenemos que ver que es una aplicación lineal. Para ello, definimos la suma de matrices de la siguiente forma: si $\displaystyle A, B \in \mathcal{M}_{m \times n}\left(\K\right) $,
\[ A + B = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(\mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\}}\left(A\right) + \mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\}}\left(B\right)\right).\]
Definimos el producto por escalares. Sea $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $ y $\displaystyle a \in \K $, 
\[a \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(a \cdot \mathcal{M}^{-1}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(A\right)\right) .\]
La demostración se puede completar con la demostración del siguiente teorema.
\end{proof}
\begin{ftheorem}[]
\normalfont $\displaystyle \mathcal{M}_{m \times n}\left(\K\right) $ es un $\displaystyle \K $-espacio vectorial.
\end{ftheorem}

\begin{proof}
Tenemos las siguientes equivalencias
\[ A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} \iff f \in \Hom\left(V, V'\right) \quad \text{y} \quad B = \begin{pmatrix} 
	b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
	b^{m}_{1} & \cdots & b^{m}_{n}
\end{pmatrix} \iff g \in \Hom\left(V, V'\right).\]
Así, $\displaystyle A + B \in \mathcal{M}_{m \times n}\left(\K\right) \iff f + g \in \Hom\left(V, V'\right) $. Tenemos que 
\[
\begin{split}
	f\left(\vec{u}_{i}\right)  = & a^{1}_{i}\vec{v}_{i} + \cdots + a^{m}_{i}\vec{v}_{m}, \; \forall i = 1, \ldots, n \\
	g\left(\vec{u}_{i}\right)= & b^{1}_{i}\vec{v}_{i} + \cdots + b^{m}_{i}\vec{v}_{m}, \; \forall i = 1, \ldots, n .
\end{split}
\]
Entonces, tenemos que 
\[\left(f + g\right)\left(\vec{u}_{i}\right) = f\left(\vec{u}_{i}\right) + g\left(\vec{u}_{i}\right) = \left(a^{1}_{i}+b^{1}_{i}\right)\vec{v}_{1} + \cdots + \left(a^{m}_{i}+b^{m}_{i}\right)\vec{v}_{m}.\]
Es decir, 
\[A + B = \begin{pmatrix} a^{1}_{1} + b^{1}_{1} & \cdots & a^{1}_{n} + b^{1}_{n}\\
\vdots & & \vdots \\
a^{m}_{1}+b^{m}_{1} & \cdots & a^{m}_{n} + b^{m}_{n}\end{pmatrix} \in \mathcal{M}_{m \times n}\left(\K\right).\]
Hacemos lo mismo con el producto por escalares. Sea $\displaystyle A \in \mathcal{M}_{m\times n}\left(\K\right) $ y $\displaystyle a \in \K $, tenemos que $\displaystyle af \in \Hom\left(V, V'\right) \iff a \cdot A \in \mathcal{M}_{m\times n}\left(\K\right) $. Tenemos que 
\[af\left(\vec{u}_{i}\right) = a a^{1}_{i}\vec{v}_{1} + \cdots + a a^{m}_{i}\vec{v}_{m} .\]
Es decir, 
\[a \cdot A = \begin{pmatrix} a a^{1}_{1} & \cdots & a a^{1}_{n} \\
\vdots & & \vdots \\
a a ^{m}_{1} & \cdots & a a^{m}_{n}\end{pmatrix} .\]
Tenemos que existe la matriz 0, pues $\displaystyle 0 \in \Hom\left(V, V'\right) $, por lo que 
	\[0 = \begin{pmatrix} 0 & \cdots & 0 \\
	\vdots & & \vdots \\
0 & \cdots & 0\end{pmatrix} .\]
Similarmente, la matriz opuesta se puede definir así:
		\[-A = \left(-1\right) \cdot A = \begin{pmatrix} -a^{1}_{1} & \cdots & -a^{1}_{n} \\
		\vdots & & \vdots \\
	-a^{m}_{1} & \cdots & -a^{m}_{n}\end{pmatrix} .\]
\end{proof}

\section{Producto de matrices}

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $, $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ base de $\displaystyle V' $ y $\displaystyle \left\{ \vec{w}_{1}, \ldots, \vec{w}_{p}\right\}  $ base de $\displaystyle V'' $. Entonces, podemos encontrar aplicaciones lineales tales que 
\[ f \in \Hom\left(V, V'\right) \to A \in \mathcal{M}_{m\times n}\left(\K\right) \quad \text{y} \quad g \in \Hom\left(V', V''\right) \to B \in \mathcal{M}_{p\times m}\left(\K\right) .\]
Tenemos que $\displaystyle g \circ f \in \Hom\left(V, V''\right) $. Definimos el producto de matrices tal que
\[g \circ f \to B \cdot A \in \mathcal{M}_{p\times n}\left(\K\right) .\]
Es decir, 
\[B \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{w}_{k}\right\} }\left(g \circ f\right) .\]
Tenemos que la expresión de $\displaystyle A $ y $\displaystyle B $ será:
\[ A = \begin{pmatrix} 
	a^{1}_{1} & \cdots & a^{1}_{n} \\
	\vdots & & \vdots \\
	a^{m}_{1} & \cdots & a^{m}_{n}
\end{pmatrix} \quad \text{y} \quad B = \begin{pmatrix} 
	b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
	b^{m}_{1} & \cdots & b^{m}_{n}
\end{pmatrix}.\]
Entonces, $\displaystyle \forall i = 1, \ldots, m $ y $\displaystyle \forall j = 1, \ldots, p $, 
\[
\begin{split}
	f\left(\vec{u}_{i}\right) = & a^{1}_{i}\vec{v}_{1} + \cdots + a^{m}_{i}\vec{v}_{m}\\
	g\left(\vec{v}_{j}\right)= & b^{1}_{j}\vec{w}_{1} + \cdots + b^{p}_{j}\vec{w}_{p} .
\end{split}
\]
Tenemos que $\displaystyle g \circ f $ será:
\[
\begin{split}
	g\left(f\left(\vec{u}_{i}\right)\right) = & g\left(a^{1}_{i}\vec{v}_{1} + \cdots +a^{m}_{i}\vec{v}_{m}\right) \\
	= & a^{1}_{i}\left(b^{1}_{1}\vec{w}_{1} + \cdots + b^{p}_{1}\vec{w}_{p}\right) + \cdots + a^{m}_{i}\left(b^{1}_{p}\vec{w}_{1} + \cdots + b^{p}_{m}\vec{w}_{p}\right) \\
	= & \left(a^{1}_{i}b^{1}_{1} + \cdots + a^{m}_{i}b^{i}_{m}\right)\vec{w}_{1} + \cdots + \left(a^{1}_{i}b^{p}_{1} + \cdots + a^{m}_{i}b^{p}_{m}\right)\vec{w}_{p} .
\end{split}
\]

Así, tenemos que
\[ B \cdot A = \begin{pmatrix} 
	\sum^{m}_{j=1}a^{j}_{1}b^{1}_{j} & \cdots & \sum^{m}_{j=1}a^{j}_{n}b^{1}_{j} \\
	\vdots & & \vdots \\
	\sum^{m}_{j=1}a^{j}_{1}b^{p}_{j} & \cdots & \sum^{m}_{j = 1}a_{n}^{j}b^{p}_{j}
\end{pmatrix} .\]
Ahora vamos a ver como utilizamos la matriz de una transformación lineal para calcular $\displaystyle f\left(\vec{x}\right) $.\\ \\
Sea $\displaystyle f : V \to V' $ lineal. Si $\displaystyle \vec{x} \in V $, entonces existen únicos $\displaystyle x^{1}, \ldots, x^{n} \in \K $ tales que 
\[\vec{x} = x^{1} \vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} .\]
Entonces tenemos que $\displaystyle f\left(\vec{x}\right) \in V' $, por lo que $\displaystyle \exists! x'^{1}, \ldots, x'^{m} \in\K$ tales que
\[
\begin{split}
f\left(\vec{x}\right) = x'^{1}\vec{v}_{1} + \cdots + x'^{m}\vec{v}_{m}.
\end{split}
\]
Similarmente, 
\[
\begin{split}
	f\left(\vec{x}\right) = & f\left(x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n}\right) 
	=  x^{1}f\left(\vec{u}_{1}\right) + \cdots + x^{n}f\left(\vec{u}_{n}\right) \\
	= & x^{1}\left(a^{1}_{1}\vec{v}_{1} + \cdots + a^{m}_{1}\vec{v}_{m}\right) + \cdots + x^{n}\left(a^{1}_{n}\vec{v}_{1} + \cdots + a^{m}_{n}\vec{v}_{m}\right) \\
	= & \left(x^{1}a_{1}^{1} + \cdots + x^{1}a^{1}_{n}\right)\vec{v}_{1} + \cdots + \left(x^{1}a^{m}_{1} + \cdots + x^{n}a^{m}_{n}\right)\vec{v}_{m}.
\end{split}
\]
Así, tenemos que las coordenadas de $\displaystyle f\left(\vec{x}\right) $ serán:
\[
\begin{cases}
x'^{1} = x^{1}a^{1}_{1} + \cdots + x^{1}a^{1}_{n}\\
\vdots \\
x'^{m} = x^{1}a^{m}_{1} + \cdots + x^{n}a^{m}_{n}
\end{cases}
.\]
En forma matricial tenemos que
\[
	A \cdot \begin{pmatrix} x^{1} \\ x^{2} \\ \vdots \\ x^{n} \end{pmatrix} = \begin{pmatrix} x'^{1} \\ x'^{2} \\ \vdots \\ x'^{m} \end{pmatrix}
 .\]
 A continuación, vamos a estudiar la matriz de cambio de base. Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ y $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{n}\right\}  $ bases de $\displaystyle V $. Tenemos que 
 \[
 \begin{split}
	 \vec{v}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} \\
		       & \vdots \\
	 \vec{v}_{n} = & a^{1}_{n} \vec{u}_{1} + \cdots + a^{n}_{n}\vec{u}_{n}.
 \end{split}
 \]
Así, si $\displaystyle \vec{x} \in V $ existen unos únicos $\displaystyle x^{1}, \ldots, x^{n} \in \K $ tales que $\displaystyle \vec{x} = x^{1}\vec{u}_{1} + \cdots + x^{n}\vec{u}_{n} $ y existen unos únicos $\displaystyle x'^{1}, \ldots, x'^{n} \in \K $ tales que $\displaystyle \vec{x} = x'^{1}\vec{v}_{1} + \cdots + x'^{n}\vec{v}_{n} $. Entonces, tenemos que
\[
\begin{split}
	\vec{x} = & x'^{1}\vec{v}_{1} + \cdots + x'^{n}\vec{v}_{n} \\
	= & x'^{1}\left(a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n}\right) + \cdots + x'^{n}\left(a^{1}_{n}\vec{u}_{1} + \cdots + a^{n}_{n}\vec{u}_{n}\right) \\
	= & \left(x'^{1}a^{1}_{1} + \cdots + x'^{n}a^{1}_{n}\right)\vec{u}_{1} + \cdots + \left(x'^{1}a^{n}_{1} + \cdots + x'^{n}a^{n}_{n}\right)\vec{u}_{n} .
\end{split}
\]
Así, tenemos que 
\[
\begin{cases}
x^{1} = x'^{1}a^{1}_{1} + \cdots + x'^{n}a^{1}_{n} \\
\vdots \\
x^{n} = x'^{1}a^{n}_{1} + \cdots + x'^{n}a^{n}_{n}
\end{cases}
\Rightarrow \underbrace{\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix}}_{\text{Matriz cambio de base}} \begin{pmatrix} x'^{1} \\ \vdots \\ x'^{n} \end{pmatrix} = \begin{pmatrix} x^{1} \\ \vdots \\ x^{n} \end{pmatrix}
.\]

\begin{eg}
	\normalfont $\displaystyle id _{V}: V_{ \left\{ \vec{v}_{i}\right\} } \to V _{ \left\{ \vec{u}_{i}\right\} } $. Queremos calcular la matriz $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{i}\right\} }\left(id _{V}\right) $. Tenemos que 
	\[id _{V}\left(\vec{v}_{i}\right) = \vec{v}_{i} = a^{1}_{i}\vec{u}_{1} + \cdots +a^{n}_{i}\vec{u}_{n} .\]
Por tanto, 
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{i}\right\} }\left(id _{V}\right) = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n}\\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} .\]
\end{eg}

\section{Permutaciones}
 \begin{fdefinition}[Permutación]
	 \normalfont Sea $\displaystyle X = \left\{ 1, 2, \ldots, n\right\}  $. Una \textbf{permutación} de $\displaystyle X $ es una aplicación biyectiva $\displaystyle \sigma: X \to X $. 
 \end{fdefinition}
 
 Normalmente se escriben de la siguiente forma:
 \[\sigma = \begin{bmatrix} 1 & 2 & \cdots & n\\
 \sigma\left(1\right) & \sigma\left(2\right) & \cdots & \sigma\left(n\right)\end{bmatrix}  .\]
 
 \begin{ftheorem}[]
	 \normalfont Sea $\displaystyle \mathcal{S}_{n} = \left\{ \sigma \; : \; \sigma \; \text{permutación de }\; X\right\}  $ y $\displaystyle \circ : \mathcal{S}_{n} \times \mathcal{S}_{n} \to \mathcal{S}_{n} $ sea la operación de composición entre permutaciones. Entonces, $\displaystyle \left(\mathcal{S}_{n}, \circ\right) $ es un grupo.
 \end{ftheorem}
 
 \begin{proof}
 Tenemos que la composición de funciones biyectivas es una operación cerrada. Además, tenemos que es asociativa, existe el elemento neutro (i.e. $\displaystyle id _{X} $) y el opuesto (i.e. $\displaystyle \sigma^{-1} $, el inverso, que también es biyección).
 \end{proof}

\begin{fdefinition}[]
\normalfont Sean $\displaystyle \left\{ a_{1}, \ldots, a_{r}\right\} \subset X $ distintos 2 a 2. El \textbf{ $\displaystyle r $-ciclo} $\displaystyle \left(a_{1}, \ldots, a_{r}\right) $ es la permutación $\displaystyle \left(a_{1}, \ldots, a_{r}\right) $ tal que
\[
\begin{split}
	a_{1} & \to a_{2} \\
	a_{2} & \to a_{3} \\
	      & \vdots \\
	a_{r} & \to a_{1}.
\end{split}
\]
\end{fdefinition}

\begin{fprop}[]
\normalfont Toda permutación es composición de $\displaystyle r $-ciclos.
\end{fprop}

\begin{proof}
Sea $\displaystyle \sigma \in \mathcal{S}_{n} $. Si $\displaystyle \forall a \in X $, $\displaystyle \sigma\left(a\right)=a $, entonces $\displaystyle \sigma = id _{X} $, que es un $\displaystyle 1 $-ciclo. Si $\displaystyle \exists a_{1}\in X $ tal que $\displaystyle \sigma\left(a_{1}\right) = a_{2}\neq a_{1} $, defino
\[
\begin{split}
	a_{3} & = \sigma^{2}\left(a_{1}\right) = \sigma\left(a_{2}\right) \in X \\
	      & \vdots \\
	a_{k} & = \sigma^{k-1}\left(a_{1}\right) = \sigma\left(a_{k-1}\right) \in X.
\end{split}
\]
Sea $\displaystyle r \in \N $ el primer natural tal que $\displaystyle a_{r} \in \left\{ a_{1}, \ldots, a_{r-1}\right\}  $. Entonces, $\displaystyle \sigma\left(a_{r}\right) = a_{1} $. Así, tenemos que $a_{r} = \sigma^{r-1}\left(a_{1}\right)$ y $\displaystyle \sigma\left(a_{r}\right) = \sigma^{h}\left(a_{1}\right) $, con $\displaystyle h < r $. De esta manera, $\displaystyle a^{r-h} = \sigma^{r-1-h}\left(a_{1}\right) = a_{1} $. Si $\displaystyle \forall b \in X - \left\{ a_{1}, \ldots, a_{r}\right\}  $ tenemos que $\displaystyle \sigma\left(b\right) = b $ ya hemos terminado. En cambio, si existe $\displaystyle b_{1} \in X - \left\{ a_{1}, \ldots, a_{r}\right\}  $ tal que $\displaystyle \sigma\left(b_{1}\right) = b_{2} \neq b_{1} $, hacemos el mismo procedimiento de antes:
\[
\begin{split}
	b_{3} & = \sigma\left(b_{2}\right) = \sigma^{2}\left(b_{1}\right) \\
				 & \vdots \\
	b_{k} & = \sigma\left(b_{k-1}\right) = \sigma^{k-1}\left(b_{1}\right).
\end{split}
\]
Sea $\displaystyle s $ el primer natural tal que $\displaystyle \sigma\left(b_{s}\right) \in \left\{ b_{1}, \ldots, b_{s-1}\right\}  $. Entonces, $\displaystyle \sigma\left(b_{s}\right)= b_{1} $. Así, tenemos que $\displaystyle \sigma = \left(b_{1}, \ldots, b_{s}\right) \circ \left(a_{1}, \ldots, a_{r}\right) $. Repetimos este proceso hasta que se agoten los elementos de $\displaystyle X $.
\end{proof}

\begin{eg}
\normalfont Consideremos 
\[
\begin{split}
	\sigma & = \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10  \\  9 & 8 & 10 & 2 & 1 & 3 & 4 & 6 & 5 & 7 \end{bmatrix} = \begin{bmatrix} 1 & 9 & 5 & 2 & 8 & 6 & 3 & 10 & 7 & 4\\
9 & 5 & 1 & 8 & 6 & 3 & 10 & 7 & 4 & 2\end{bmatrix} \\
& = \left(1, 9, 5\right) \circ \left(2, 8, 6, 3, 10, 7, 4\right)
\end{split}
\]
\end{eg}
\begin{eg}
\normalfont 
\[
\begin{split}
	\sigma = & \begin{bmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
		8 & 5 & 6 & 1 & 2 & 7 & 3 & 4 & 10 & 9\end{bmatrix} = \begin{bmatrix} 1 & 8 & 4 & 2 & 5 & 3 & 6 & 7 & 9 & 10 \\ 8 & 4 & 1 & 5 & 2 & 6 & 7 & 3 & 10 & 9\end{bmatrix} \\
		= & \left(1,8, 4\right) \circ \left(2, 5\right) \circ \left(3, 6, 7\right) \circ \left(9, 10\right).
\end{split}
\]
\end{eg}

\begin{fdefinition}[]
\normalfont Los $\displaystyle 2 $-ciclos los llamaremos \textbf{trasposiciones}.
\end{fdefinition}

\begin{fprop}[]
\normalfont Todo ciclo es producto de trasposiciones.
\end{fprop}
\begin{proof}
Tenemos que 
\[\left(a_{1}, \ldots, a_{m}\right) = \left(a_{1}, a_{2}\right)\left(a_{2}, a_{3}\right) \cdots \left(a_{m -1}, a_{m}\right) .\]
\end{proof}

Definimos 
\[
\begin{split}
	\nabla =  \prod_{1 \leq i < j \leq n} \left(\vec{x}_{i}-\vec{x}_{j}\right) = \left(\vec{x}_{1}-\vec{x}_{2}\right) &\left(\vec{x}_{1}-\vec{x}_{3}\right) \cdots \left(\vec{x}_{1}-\vec{x}_{n}\right) 
		  \left(\vec{x}_{2}-\vec{x}_{3}\right) \cdots \left(\vec{x}_{2}-\vec{x}_{n}\right) \cdots \left(\vec{x}_{n-1}-\vec{x}_{n}\right).
\end{split}
\]
De esta manera, si $\displaystyle \sigma \in \mathcal{S}_{n} $, definimos 
\[
\begin{split}
\nabla\sigma  = \prod_{1 \leq i < j \leq n}\vec{x}_{\sigma\left(i\right)}-\vec{x}_{\sigma\left(j\right)} .
\end{split}
\]
Una permutación de $\displaystyle \sigma  $ es un par $\displaystyle i < j $ tal que $\displaystyle \sigma\left(i\right) < \sigma\left(j\right) $. Una inversión de $\displaystyle \sigma  $ es un par $\displaystyle i < j $ tal que $\displaystyle \sigma\left(j\right) < \sigma\left(i\right) $. Entonces, el número de factores de $\displaystyle \nabla  $ y $\displaystyle \nabla \sigma 	 $ es el mismo. 
Así, tenemos que 
\[\nabla \sigma = \left(-1\right)^{\text{número de inversiones}} \nabla  .\]
El valor de $\displaystyle -1 $ elevado al número de inversiones de $\displaystyle \sigma  $ lo llamaremos índice o signo de $\displaystyle \sigma  $ y se escribirá $\displaystyle \ind\left(\sigma \right) $ o $\displaystyle \sig\left(\sigma \right) $ \footnote{También se corresponde con el número de trasposiciones en las que se puede componer $\displaystyle \sigma  $.} . Entonces, 
\[\nabla \sigma = \sig\left(\sigma \right) \nabla .\]
Sea $\displaystyle \tau \in \mathcal{S}_{n} $, entonces
\[\underbrace{\nabla \sigma \circ \tau}_{\sig\left(\sigma \circ \tau\right)\nabla } = \sig\left(\sigma \right)\nabla\tau = \sig\left(\sigma \right)\sig\left(\tau\right)\nabla .\]
Definimos,
\[
\begin{split}
	\sig : \mathcal{S}_{n} & \to \left\{ -1, 1\right\} \\
	 \sigma & \to \sig\left(\sigma\right).
\end{split}
\]
\begin{fprop}[]
\normalfont La trasposición $\displaystyle \left(1,2\right) $ tiene signo negativo.
\end{fprop}
\begin{proof}
Tenemos que 
\[\nabla\left(1,2\right) = \left(x_{2}-x_{1}\right) \left(x_{2}-x_{3}\right) \cdots \left(x_{2}-x_{n}\right) \left(x_{1}-x_{3}\right) \cdots \left(x_{1}-x_{n}\right) \cdots \left(x_{n-1} - x_{n}\right) .\]
Como podemos ver respecto a la primera definición de $\displaystyle \nabla  $, tenemos que todo es igual menos el primer factor, que es de signo opuesto. Así, $\displaystyle \sig\left(1,2\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont La trasposición $\displaystyle \left(1,a\right) $ es negativa para $\displaystyle a \geq 2 $. 
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \left(1, a\right) = \left(2 , a\right) \circ \left(1,2\right) \circ \left(2,a\right) $. 
\[
\begin{split}
& 1 \to 1 \to 2 \to a \\
& 2 \to a \to a \to 2 \\
& a \to 2 \to 1 \to 1.
\end{split}
\]
Entonces, 
\[\sig\left(1,a\right) = \sig\left(2,a\right)\left(-1\right)\left(\sig\left(2,a\right)\right) = \left(\sig\left(2,a\right)\right)^{2}\left(-1\right) = -1 .\]
\end{proof}

\begin{fprop}[]
\normalfont $\displaystyle \sig\left(a,b\right) = -1$.
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \left(a,b\right) = \left(1, b\right) \circ \left(1,a\right) \circ \left(1,b\right) $. Así, $\displaystyle a \to b $ y $\displaystyle b \to a $, por lo que
\[
\begin{split}
& 1 \to b \to b \to b \\
& a \to a \to 1 \to b \\
& b \to 1 \to a \to a.
\end{split}
\]
Entonces, $\displaystyle \sig\left(\left(a,b\right)\right) = \sig\left(1,b\right)^{2} \cdot \sig\left(1,a\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont $\displaystyle \sig\left(\left(a_{1}, \ldots, a_{r}\right)\right) = \left(-1\right)^{r-1} $.
\end{fprop}

\begin{proof}
Tenemos que 
\[\left(a_{1}, a_{r}\right) \circ \left(a_{1}a_{r-1}\right) \cdots \left(a_{1}a_{3}\right)\left(a_{1}a_{2}\right) .\]
De esta manera, 
\[
\begin{split}
& a_{1} \to a_{2} \to a_{2} \to \cdots \to a_{2} \to a_{2}\\
& a_{2} \to a_{1} \to a_{3} \to \cdots \to a_{3} \to a_{3} \\
& \vdots \\
& a_{r-1} \to a_{r-1} \to a_{r-1} \to \cdots \to a_{1} \to a_{r} \\
& a_{r} \to a_{r-1} \to a_{r} \to \cdots \to a_{r} \to a_{r}.
\end{split}
\]
\end{proof}

\begin{eg}
\normalfont Sea
\[
\begin{split}
	\sigma = \begin{pmatrix} 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
	10 & 9 & 7 & 2 & 1 & 3 & 8 & 4 & 6 & 5\end{pmatrix} = \left(1, 10, 5\right) \circ \left(2, 9, 6, 3, 7, 8, 4\right) .
\end{split}
\]
Por tanto, 
\[\sig\left(\sigma \right) = \sig\left(1, 10, 5\right) \cdot \sig\left(2, 9, 6, 3, 7, 8, 4\right) = \left(-1\right)^{2} \cdot \left(-1\right)^{6} = 1 .\]
\end{eg}

\section{Estructura de álgebra}

Sean $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $, $\displaystyle \left\{ \vec{v}_{1}, \ldots, \vec{v}_{m}\right\}  $ y $\displaystyle \left\{ \vec{w}_{1}, \ldots, \vec{w}_{p}\right\}  $ bases de $\displaystyle V$, $\displaystyle V' $ y $\displaystyle V'' $ respectivamente. Si $\displaystyle f \in \Hom\left(V, V'\right) $, tenemos que 
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ v_{j}\right\} }\left(f\right) = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{m}_{1} & \cdots & a^{m}_{n}\end{pmatrix} = A ,\]
donde $\displaystyle f\left(\vec{u}_{i}\right) = a^{1}_{i} \vec{v}_{1} + \cdots + a^{m}_{i}\vec{v}_{m} $. Similarmente, si $\displaystyle g \in \Hom\left(V,V'\right) $, 
	\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(g\right) = \begin{pmatrix} b^{1}_{1} & \cdots & b^{1}_{n} \\
	\vdots & & \vdots \\
b^{m}_{1} & \cdots & b^{m}_{n}\end{pmatrix} = B .\]
Recordamos que 
		\[A + B = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(f+g\right) = \begin{pmatrix} a^{1}+b^{1} & \cdots & a^{1}_{n}+b^{1}_{n} \\
		\vdots & & \vdots \\
	a^{m}_{1}+b^{m}_{1} & \cdots & a^{m}_{n} + b^{m}_{n}\end{pmatrix} .\]
Similarmente, si $\displaystyle a \in \K $, 
			\[a \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(a \cdot f\right) = \begin{pmatrix} a a^{1}_{1} & \cdots & a a ^{1}_{n} \\
				\vdots & & \vdots \\
			a a^{m}_{1} & \cdots & a a ^{m}_{n}\end{pmatrix} .\]
\begin{fprop}[]
\normalfont El producto de matrices es distributivo por la izquierda y por la derecha.
\end{fprop}

	Si $\displaystyle f \in \Hom\left(V, V'\right) $ y $\displaystyle g \in \Hom\left(V', V''\right) $, tenemos que $\displaystyle \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{v}_{j}\right\} }\left(f\right) = A \in \mathcal{M}_{ m \times n}\left(\K\right), \; \mathcal{M}_{ \left\{ \vec{v}_{j}\right\} \left\{ \vec{w}_{k}\right\}  }\left(g\right) = B \in \mathcal{M}_{p \times m}\left(\K\right)$. Definíamos el producto de matrices de la siguiente forma, 
			\[ B \cdot A = \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{w}_{k}\right\}}\left(g\circ f\right).\]
Si $\displaystyle f \in \Hom\left(V,V'\right) $, $\displaystyle g \in \Hom\left(V', V''\right) $ y $\displaystyle h \in \Hom\left(V'', V'''\right) $, tenemos que 
\[h \circ \left(g \circ f\right) = \left(h \circ g\right) \circ f .\]
Así, como $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, $\displaystyle B \in \mathcal{M}_{p \times m}\left(\K\right) $ y $\displaystyle C \in \mathcal{M}_{q \times p} $, tenemos que el producto de matrices es asociativo:
\[ C \cdot \left(B \cdot A\right) = \left(C \cdot B\right) \cdot A \in \mathcal{M}_{q \times n}\left(\K\right) .\]
Si $\displaystyle f,g \in \Hom\left(V, V'\right) $, $\displaystyle h \in \Hom\left(V', V''\right) $, tenemos que $\displaystyle \forall \vec{x} \in V $,
\[h \circ \left(f + g\right)\left(\vec{x}\right) = h \circ \left(f\left(\vec{x}\right) + g\left(\vec{x}\right)\right) = h\left(f\left(x\right)\right) + h\left(g\left(x\right)\right) = \left(h\circ f + h\circ g\right)\left(\vec{x}\right) .\]
Así, $\displaystyle h \circ \left(f+g\right) = h \circ f + h \circ g $. Entonces, hemos demostrado que el producto de matrices es distributivo por la izquierda. Entonces, $\displaystyle A, B \in \mathcal{M}_{m \times n} \left(\K\right)$, $\displaystyle C \in \mathcal{M}_{p\times m}\left(\K\right) $,
\[C \cdot \left(A + B \right) = C \cdot A + C \cdot B .\]
Sea $\displaystyle f \in \Hom\left(V,V'\right) $ y $\displaystyle g, h \in \Hom\left(V',V''\right) $. Tenemos que $\displaystyle \forall\vec{x} \in V $,
\[\left(\left(g+h\right)\circ f\right)\left(\vec{x}\right) = \left(g+h\right)\left(f\left(\vec{x}\right)\right) = g\left(f\left(\vec{x}\right)\right)+h\left(f\left(\vec{x}\right)\right) = g\circ f\left(\vec{x}\right) + h \circ f\left(\vec{x}\right)=\left(g \circ f + h \circ f\right)\left(\vec{x}\right) .\]
Así, concluimos que $\displaystyle \left(g+h\right)\circ f = g\circ f + h \circ f $. Por lo que el producto de matrices es distributivo por la derecha. Es decir, sean $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, $\displaystyle B,C \in \mathcal{M}_{p \times m}\left(\K\right) $, 
\[ \left(B + C\right) \cdot A = B \cdot A + C \cdot A.\]
\begin{fprop}[]
\normalfont El producto de matrices no es conmutativo.
\end{fprop}
 Si $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $ y $\displaystyle B \in \mathcal{M}_{p \times m}\left(\K\right) $. Entonces $\displaystyle B \cdot A \in \mathcal{M}_{p \times n}\left(\K\right) $, pero $\displaystyle A \cdot B $ no está definido. Si $\displaystyle A,B \in \mathcal{M}_{n \times n}\left(\K\right) $, tenemos que $\displaystyle A \cdot B, B \cdot A \in \mathcal{M}_{n \times n}\left(\K\right) $, pero no tienen por qué ser iguales. En efecto, sean 
$ \displaystyle A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} $ y $\displaystyle B = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ .Entonces
\[A \cdot B = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \neq B \cdot A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} .\]
\begin{fprop}[]
\normalfont Existe un elemento identidad en el producto de matrices
\end{fprop}
Tenemos que 
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{~/Desktop/Images/matriz_inversa1.png}
\caption{Matriz inversa respecto a $\displaystyle V $}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{~/Desktop/Images/matriz_inversa2.png}
\caption{Matriz inversa respecto a $\displaystyle V' $}
\label{ enter label$}
\end{figure}
Observando las imágenes, tenemos que 
\[A \cdot I_{n \times n} = A \quad \text{y} \quad I_{m \times m }A = A .\]
Entonces, si consideramos la aplicación
\[id _{V} : V_{ \left\{ \vec{u}_{i}\right\}} \to V_{ \left\{ \vec{u}_{i}\right\} } ,\]
tal que $\displaystyle \forall i = 1, \ldots, n $, $\displaystyle id _{V}\left(\vec{u}_{i}\right) = \vec{u}_{i} $, tenemos que su matriz será
\[\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} }\left(id _{V}\right) = \begin{pmatrix} 1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \cdots & \vdots \\
0 & 0 & \cdots & 1\end{pmatrix} = I_{n \times n} .\]

Consideremos la función, 
\[
\begin{split}
	\mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} } : \End\left(V\right) & \to \mathcal{M}_{n \times n}\left(\K\right) \\
	f & \to \mathcal{M}_{ \left\{ \vec{u}_{i}\right\} \left\{ \vec{u}_{i}\right\} }\left(f\right).
\end{split}
\]
\begin{fprop}[]
\normalfont Por lo demostrado anteriormente, tenemos que $\displaystyle \left(\End\left(V\right), +, \cdot _{\K}\right) $ y $\displaystyle \left( \mathcal{M}_{n \times n}, + , \cdot _{\K}\right) $ son $\displaystyle \K $-espacios vectoriales. Similarmente, $\displaystyle \left(\End\left(V\right), + ,\circ\right) $ y $\displaystyle \left(\mathcal{M}_{n \times n }\left(\K\right), +, \cdot\right) $  son anillos con unidad. 
\end{fprop}

\begin{fdefinition}[]
\normalfont Diremos que $\displaystyle \End\left(V\right) $ y $\displaystyle \mathcal{M}_{n \times n}\left(\K\right) $ son \textbf{álgebras} con unidad.
\end{fdefinition}

Tenemos que 
\[\Aut\left(V\right) = \left\{ f \in \End\left(V\right) \; : \; f \; \text{invertible}\right\} = \left\{ f \in \End\left(V\right) \; : \; \exists f^{-1}\right\}  .\]
Así, $\displaystyle \left(\Aut\left(V\right), \cdot\right) $ es un grupo. En general, la suma de automorfismos no es autormorfismo. Por ejemplo, $\displaystyle id _{V} - id _{V} = 0 \not\in\Aut\left(V\right) $. Definimos 
\[\GL_{n}\left(\K\right) = \left\{ A \in \mathcal{M}_{n \times n}\left(\K\right) \; : \; \exists A^{-1}\right\}  .\]
Para estudiar este conjunto, introduciremos la noción de determinante.

\section{Determinantes}
\begin{fdefinition}[]
	\normalfont Una $\displaystyle r $-\textbf{forma multilineal}  definida en $\displaystyle V $ es una aplicación $\displaystyle t : \underbrace{ V \times V \cdots \times V}_{r \; \text{veces}} \to \K $ tal que 
\begin{description}
\item[(a)] $\displaystyle \forall i = 1, \ldots, r, \; \forall \vec{x}_{1}, \ldots, \vec{x}_{r}, \vec{x'}_{i} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \vec{x'}_{i}, \ldots, \vec{x}_{r}\right) = t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) + t\left(\vec{x}_{1}, \ldots, \vec{x'}_{i}, \ldots, \vec{x}_{r}\right).\]
\item[(b)] $\displaystyle \forall i = 1, \ldots, r $, $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V, \; \forall a \in \K $,
	\[t\left(x_{1}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) = a t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) .\]
\end{description}
\end{fdefinition}

\begin{fdefinition}[]
\normalfont Una $\displaystyle r $-forma multilineal es alternada si $\displaystyle \forall \sigma \in \mathcal{S}_{r} $, 
\[t\left(\vec{x}_{\sigma\left(1\right)}, \ldots, \vec{x}_{\sigma\left(r\right)}\right) = \sig\left(\sigma \right)t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right), \; \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V .\]
\end{fdefinition}

\begin{fprop}[]
\normalfont Si $\displaystyle t $ es una $\displaystyle r $-forma multilineal, $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{0}, \ldots, \vec{x}_{r}\right) = 0 .\]
\end{fprop}
\begin{proof} Tenemos que $\displaystyle \vec{0} = 0 \cdot \vec{x}_{i}  $, así, 
\[t\left(\vec{x}_{1}, \ldots, \vec{0}, \ldots, \vec{x}_{r}\right) =0 \cdot t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) = 0.\]
\end{proof}

\begin{fprop}[]
\normalfont Sea $\displaystyle t $ una $\displaystyle r $-forma multilineal alternada. Entonces $\displaystyle \forall \vec{x}_{1}, \ldots, \vec{x}_{r} \in V $,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = -t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) .\]
\end{fprop}

\begin{proof}
Tenemos que $\displaystyle \sig\left(i,j\right) = -1 $.
\end{proof}

\begin{fprop}[]
\normalfont Si $\displaystyle \vec{x}_{i} = \vec{x}_{j} $ y $\displaystyle t $ es una forma multilineal alternada,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0, \; i \neq j .\]
\end{fprop}
\begin{proof}
Tenemos que 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = -t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) .\]
Esto implica que $\displaystyle t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0 $.
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) =0 .\]
\end{fprop}
\begin{proof}
Por la proposición anterior,
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, a\vec{x}_{i}, \ldots, \vec{x}_{r}\right) = at\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right) = a \cdot 0 = 0 .\]
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \sum^{r}_{j=1}a^{j}\vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0, \; j \neq i .\]
\end{fprop}
\begin{proof}
Tenemos que
\[\sum^{r}_{j=1}a^{j}t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{r}\right) = 0 .\]
\end{proof}

\begin{fprop}[]
\normalfont 
\[t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \sum^{r}_{j=1}a^{j}\vec{x}_{j}, \ldots, \vec{x}_{r}\right) = t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{r}\right), \; j \neq i.\]
\end{fprop}
	Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $, $\displaystyle t $ multilineal alternada
\[
\begin{split}
	\vec{x}_{1} = & a^{1}_{1}\vec{u}_{1} + \cdots + a^{n}_{1}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{1}\vec{u}_{j} \\
	\vdots & \\
	\vec{x}_{i} = & a^{1}_{i}\vec{u}_{1} + \cdots + a^{n}_{i}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{i}\vec{u}_{j} \\
	\vdots & \\
	\vec{x}_{r} = & a^{1}_{r}\vec{u}_{1} + \cdots + a^{n}_{r}\vec{u}_{n} = \sum^{n}_{j=1}a^{j}_{r}\vec{u}_{j}.
\end{split}
\]
Así, 
\[
\begin{split}
	t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right) = & t\left(\sum^{n}_{j=1}a^{j}_{1}\vec{u}_{j}, \ldots, \sum^{n}_{j=1}a^{j}_{r}\vec{u}_{j}\right) \\
	= & \sum_{\left(j_{1}, \ldots, j_{r}\right)}a^{j_{1}}_{1}\cdots a^{j_{r}}_{r}t\left(\vec{u}_{j_{1}}, \ldots, \vec{u}_{j_{r}}\right) .
\end{split}
\]
Se trata de $\displaystyle r $-variaciones con repetición. Tenemos que si $\displaystyle j_{i} = j_{h} $, entonces el término $\displaystyle t\left(\vec{u}_{j_{1}}, \ldots, \vec{u}_{j_{r}}\right) $ se va a anular, por lo que realmente se trata de variaciones $\displaystyle n $-arias sin repetición.
Sea $\displaystyle \dim\left(V\right) = n $ y sea $\displaystyle t $ una $\displaystyle n $-forma multilineal alternada. Si $\displaystyle \left\{ \vec{x}_{1}, \ldots, \vec{x}_{n}\right\} \subset V $, 
\[\begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n} \\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} = \begin{pmatrix} \vec{x}_{1} & \cdots & \vec{x}_{n} \end{pmatrix}.\]

Son variaciones $\displaystyle n $-arias sin repetición de $\displaystyle \left(1, 2, \ldots, n\right) $.
\[
\begin{split}
	t\left(\vec{x}_{1}, \ldots, \vec{x}_{r}\right) = &\sum_{\sigma \in \mathcal{S}_{n}} a^{\sigma\left(1\right)}_{1} a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma \left(n\right)}_{n}t\left(\vec{u}_{\sigma\left(1\right)}, \ldots, \vec{u}_{\sigma\left(n\right)}\right) \\
	= & \sum_{\sigma \in \mathcal{S}_{n}}a^{\sigma\left(1\right)}_{1} \cdots a^{\sigma\left(n\right)}_{n}\sig\left(\sigma \right)t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right).
\end{split}
\]
\begin{fdefinition}[Determinante]
\normalfont Si $\displaystyle A \in \mathcal{M}_{n \times n}\left(\K\right) $, el determinante de $\displaystyle A_{\sigma \in \mathcal{S}_{n}} $ es
\[\det A = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma \right)a^{\sigma\left(1\right)}_{1} \cdots a^{\sigma\left(n\right)}_{n} .\]
\end{fdefinition}

\begin{fdefinition}[Traspuesta]
\normalfont Si $\displaystyle A \in \mathcal{M}_{m \times n}\left(\K\right) $, llamaremos \textbf{traspuesta} de $\displaystyle A $ a la matriz $\displaystyle A^{t} \in \mathcal{M}_{n \times m}\left(\K\right) $ que tiene por filas las columnas de $\displaystyle A $.
\end{fdefinition}

\begin{ftheorem}[]
\normalfont 
\[\det A^{t} = \det A .\]
\end{ftheorem}

\begin{proof}
Si $\displaystyle A \in \mathcal{M}_{n \times n} $, entonces
\[\det A^{t} = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma \right)a^{1}_{\sigma\left(1\right)} \cdots a^{n}_{\sigma\left(n\right)} = \sum_{\sigma \in \mathcal{S}_{n}}\sig\left(\sigma^{-1}\right)a^{\sigma^{-1}\left(1\right)}_{1} \cdots a^{\sigma^{-1}\left(n\right)}_{n}=\sum_{\Pi\in\mathcal{S}_{n}} \sig\left(\Pi\right)a^{\Pi\left(1\right)}_{1} \cdots a^{\Pi\left(n\right)}_{n} = \det A.\]
\end{proof}

Sea $\displaystyle \left\{ \vec{u}_{1}, \ldots, \vec{u}_{n}\right\}  $ base de $\displaystyle V $ tal que $\displaystyle t\left(\vec{u}_{1}, \ldots, \vec{u}_{n}\right) = 1 $. Entonces, $\displaystyle t\left(\vec{x}_{1}, \ldots, \vec{x}_{n}\right) = \det A $ y 
\[\begin{pmatrix} \vec{x}_{1} & \cdots & \vec{x}_{n} \end{pmatrix} = \begin{pmatrix} \vec{u}_{1} & \cdots & \vec{u}_{n} \end{pmatrix} A .\]

\begin{observation}
\normalfont 
\[ .\]
\[
\begin{split}
 \det\begin{pmatrix} a^{1}_{1} & \cdots& a^{1}_{i} + b^{1}_{i} & \cdots & a^{1}_{n} \\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{i}+b^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} = & t\left(\vec{x}_{1}, \ldots, \vec{x}_{i} + \vec{y}_{i} , \ldots \vec{x}_{n}\right) \\
	 = & t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) + t\left(\vec{x}_{1}, \ldots, \vec{y}_{i}, \ldots, \vec{x}_{n}\right) \\
	 = & \det \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{i} & \cdots & a^{1}_{n}\\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a^{n}_{1} & \cdots & a^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} + \det \begin{pmatrix} a^{1}_{1} & \cdots & b^{1}_{i} & \cdots & a^{1}_{n}\\
	 \vdots & \vdots & \vdots & \vdots & \vdots \\
 a^{n}_{1} & \cdots & b^{n}_{i} & \cdots & a^{n}_{n} \end{pmatrix}.
\end{split}
\]
\end{observation}

\begin{observation}
\normalfont 
\[
\begin{split}
	\det\begin{pmatrix} a^{1}_{1} & \cdots & a a^{1}_{i} & \cdots & a^{1}_{n} \\
	\vdots & \vdots & \vdots & \vdots & \vdots\\
a^{n}_{1} & \cdots & a a ^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} = & t\left(\vec{x}_{1}, \ldots,a \vec{x}_{i}, \ldots, \vec{x}_{n}\right) \\
= & at\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) \\
= & a \det\begin{pmatrix} a^{1}_{1} & \cdots &  a^{1}_{i} & \cdots & a^{1}_{n} \\
	\vdots & \vdots & \vdots & \vdots & \vdots\\
a^{n}_{1} & \cdots &  a ^{n}_{1} & \cdots & a^{n}_{n} \end{pmatrix}.
\end{split}
\]
\end{observation}

\begin{observation}
\normalfont En el caso anteriro, si $\displaystyle a = 0 $, el determinante de la matriz es 0.
\end{observation}

\begin{observation}
\normalfont 
\[
\begin{split}
	\det\begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{j} & \cdots & a^{1}_{i} & \cdots & a^{1}_{n}\\
	\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{j} & \cdots & a^{n}_{i} & \cdots & a^{n}_{n}\end{pmatrix} = t\left(\vec{x}_{1}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{n}\right) = - t\left(\vec{x}_{1}, \ldots, \vec{x}_{i}, \ldots, \vec{x}_{j}, \ldots, \vec{x}_{n}\right) .
\end{split}
\]
\end{observation}

Sea 
\[A = \begin{pmatrix} a^{1}_{1} & \cdots & a^{1}_{n}\\
\vdots & & \vdots \\
a^{n}_{1} & \cdots & a^{n}_{n}\end{pmatrix} \in \mathcal{M}_{n \times n}\left(\K\right) .\]
Tenemos que 
\[\det\left(A\right) = \sum_{\sigma \in \mathcal{S}_{n}} \sig\left(\sigma \right)a_{1}^{\sigma\left(1\right)}\cdots a^{\sigma\left(n\right)}_{n}.\]
Vamos a ver como calcular el determinante de una matriz aislando una única fila o columna. Seleccionamos el elemento $\displaystyle a_{1}^{1} $ y calculamos la suma de todos los sumandos donde interviene $\displaystyle a^{1}_{1} $:
\[ \sum_{\sigma \in \mathcal{S}_{n}, \; \sigma\left(1\right) = 1} \sig\left(\sigma \right)a^{1}_{1}a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma\left(n\right)}_{n} = \left(\sum_{\sigma \in \mathcal{S}_{n-1}}\sig\left(\sigma \right)a^{\sigma\left(2\right)}_{2} \cdots a^{\sigma\left(n\right)}_{n}\right) \cdot a_{1}^{1} = a^{1}_{1} \det \begin{pmatrix} a^{2}_{2} & \cdots & a^{2}_{n}\\
\vdots & & \vdots \\
a^{n}_{2} & \cdots & a^{n}_{n}\end{pmatrix}.\]
Este mismo procedimiento se puede hacer para un elemento cualquiera $\displaystyle a^{i}_{j} $.
\begin{fdefinition}[Adjunto]
\normalfont  El menor complementario $\displaystyle \alpha^{i}_{j} $ es el determinante de la matriz $\displaystyle \left(n - 1\right) \times \left(n - 1\right) $ que se obtiene $\displaystyle A $ suprimiendo la fila $\displaystyle i $ y la columna $\displaystyle j $, y llamaremos \textbf{adjunto} a $\displaystyle A^{i}_{j} = \left(-1\right)^{i+j}\alpha^{i}_{j} $. 
\end{fdefinition}

Si cogemos la fija $\displaystyle j $, vamos a tener que 
\[\det\left(A\right) = a^{1}_{j}A^{1}_{j}+a^{2}_{j}A^{2}_{j}+\cdots+a^{n}_{j}A^{n}_{j} .\]
Si $\displaystyle i\neq j $ tenemos que
\[a^{1}_{j}A^{1}_{i} + a^{2}_{j}A^{2}_{i} + \cdots + a^{n}_{j}+A^{n}_{i} = 0 .\]

